{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd95780b",
   "metadata": {},
   "source": [
    "# Demo #5: Re-ranking with Cross-Encoders\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo demonstrates the **two-pass retrieval architecture** using cross-encoder re-ranking, a powerful technique that significantly improves retrieval precision in RAG systems.\n",
    "\n",
    "### The Problem with Single-Pass Retrieval\n",
    "\n",
    "Traditional RAG systems use **bi-encoders** (like sentence-transformers or OpenAI embeddings) for retrieval:\n",
    "- Query and documents are encoded **independently**\n",
    "- Similarity is computed via simple operations (cosine similarity, dot product)\n",
    "- **Fast** but **less accurate** - no interaction between query and document during encoding\n",
    "\n",
    "### The Solution: Two-Pass Retrieval with Cross-Encoders\n",
    "\n",
    "Cross-encoders evaluate query-document pairs **jointly**:\n",
    "1. **Pass 1 (Recall)**: Use fast bi-encoder to retrieve top-K candidates (e.g., top-20)\n",
    "2. **Pass 2 (Precision)**: Use accurate cross-encoder to re-rank candidates to top-N (e.g., top-3)\n",
    "\n",
    "**Why this works:**\n",
    "- Cross-encoders encode query+document **together**, capturing interaction signals\n",
    "- Much more accurate than bi-encoders but also much slower (O(n) vs O(1))\n",
    "- Two-pass approach gets **best of both worlds**: speed from bi-encoders, accuracy from cross-encoders\n",
    "\n",
    "### Core Concepts Demonstrated\n",
    "- Two-pass retrieval architecture (recall → precision)\n",
    "- Bi-encoder vs. Cross-encoder comparison\n",
    "- Post-retrieval optimization\n",
    "- Precision improvement over recall-focused retrieval\n",
    "\n",
    "### References\n",
    "- Retrieval-Augmented Generation (RAG) from basics to advanced (Reference 15)\n",
    "- Advanced RAG Techniques: What They Are & How to Use Them - FalkorDB (Reference 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda2c59c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e055523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "# For re-ranking - we'll implement a custom cross-encoder using sentence-transformers\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from typing import List, Optional\n",
    "\n",
    "# For cross-encoder model\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    HAS_SENTENCE_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ sentence-transformers not installed. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\", \"-q\"])\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    HAS_SENTENCE_TRANSFORMERS = True\n",
    "\n",
    "# Visualization\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec18664",
   "metadata": {},
   "source": [
    "## 2. Azure OpenAI Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2620c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Azure OpenAI configured successfully\n",
      "  LLM Deployment: gpt-4\n",
      "  Embedding Deployment (Bi-Encoder): text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "# Azure OpenAI configuration from environment variables\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-ada-002\")\n",
    "\n",
    "# Validate configuration\n",
    "if not all([AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT]):\n",
    "    raise ValueError(\n",
    "        \"Missing Azure OpenAI configuration. Please set:\\n\"\n",
    "        \"- AZURE_OPENAI_API_KEY\\n\"\n",
    "        \"- AZURE_OPENAI_ENDPOINT\\n\"\n",
    "        \"- AZURE_OPENAI_DEPLOYMENT (optional, default: gpt-4)\\n\"\n",
    "        \"- AZURE_OPENAI_EMBEDDING_DEPLOYMENT (optional, default: text-embedding-ada-002)\"\n",
    "    )\n",
    "\n",
    "# Initialize Azure OpenAI LLM\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI Embeddings (Bi-Encoder)\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=AZURE_OPENAI_EMBEDDING_DEPLOYMENT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    ")\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"✓ Azure OpenAI configured successfully\")\n",
    "print(f\"  LLM Deployment: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "print(f\"  Embedding Deployment (Bi-Encoder): {AZURE_OPENAI_EMBEDDING_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f6d56",
   "metadata": {},
   "source": [
    "## 3. Custom Cross-Encoder Re-Ranker\n",
    "\n",
    "Implement a custom cross-encoder post-processor for LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd61fe87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0844ddf083bb4e5eada619c1c30f7d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-16T12:57:00.529371Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x7efd32b42520>), traceback: Some(<traceback object at 0x7efb7370be00>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-16T12:57:00.568624Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x7efd32b42520>), traceback: Some(<traceback object at 0x7efb7370bf40>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class CrossEncoderReranker(BaseNodePostprocessor):\n",
    "    \"\"\"Cross-encoder reranker using sentence-transformers.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "        top_n: int = 3,\n",
    "    ):\n",
    "        \"\"\"Initialize cross-encoder reranker.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for cross-encoder\n",
    "            top_n: Number of top results to return after reranking\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = CrossEncoder(model_name)\n",
    "        self.top_n = top_n\n",
    "        self._model_name = model_name\n",
    "        print(f\"✓ Cross-Encoder loaded: {model_name}\")\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_bundle: Optional[QueryBundle] = None,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"Rerank nodes using cross-encoder.\"\"\"\n",
    "        if query_bundle is None:\n",
    "            return nodes\n",
    "        \n",
    "        query_str = query_bundle.query_str\n",
    "        \n",
    "        # Prepare query-document pairs for cross-encoder\n",
    "        pairs = [[query_str, node.node.get_content()] for node in nodes]\n",
    "        \n",
    "        # Get cross-encoder scores\n",
    "        scores = self.model.predict(pairs)\n",
    "        \n",
    "        # Update node scores with cross-encoder scores\n",
    "        for node, score in zip(nodes, scores):\n",
    "            node.score = float(score)\n",
    "        \n",
    "        # Sort by cross-encoder score and return top_n\n",
    "        nodes_sorted = sorted(nodes, key=lambda x: x.score, reverse=True)\n",
    "        return nodes_sorted[:self.top_n]\n",
    "\n",
    "# Initialize cross-encoder reranker\n",
    "cross_encoder_reranker = CrossEncoderReranker(\n",
    "    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",  # Fast, accurate cross-encoder\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "print(\"✓ Cross-Encoder Re-ranker ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78efa8",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "Load technical documents with varying relevance patterns to demonstrate re-ranking benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf51f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = Path(\"./data/tech_docs\")\n",
    "\n",
    "# Load documents\n",
    "print(\"Loading documents...\")\n",
    "documents = SimpleDirectoryReader(str(data_dir)).load_data()\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    file_name = Path(doc.metadata.get('file_name', 'unknown')).name\n",
    "    print(f\"  {i}. {file_name} ({len(doc.text)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5f9c4",
   "metadata": {},
   "source": [
    "## 5. Build Vector Index (Bi-Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e190d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index using bi-encoder (Azure OpenAI embeddings)\n",
    "print(\"Building vector index with bi-encoder embeddings...\")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "print(\"✓ Vector index built successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233f956",
   "metadata": {},
   "source": [
    "## 6. Single-Pass Retrieval (Baseline)\n",
    "\n",
    "Create a baseline query engine using only bi-encoder retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca282f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Retrieve top-10 with bi-encoder only\n",
    "baseline_retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,  # Retrieve top-10 for baseline\n",
    ")\n",
    "\n",
    "baseline_query_engine = RetrieverQueryEngine(\n",
    "    retriever=baseline_retriever,\n",
    ")\n",
    "\n",
    "print(\"✓ Baseline query engine ready (bi-encoder only, top-10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9015d",
   "metadata": {},
   "source": [
    "## 7. Two-Pass Retrieval with Cross-Encoder Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd49b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-pass: Retrieve top-10 with bi-encoder, then rerank to top-3 with cross-encoder\n",
    "rerank_retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,  # Pass 1: Cast wide net with bi-encoder\n",
    ")\n",
    "\n",
    "rerank_query_engine = RetrieverQueryEngine(\n",
    "    retriever=rerank_retriever,\n",
    "    node_postprocessors=[cross_encoder_reranker],  # Pass 2: Rerank with cross-encoder\n",
    ")\n",
    "\n",
    "print(\"✓ Two-pass query engine ready\")\n",
    "print(\"  Pass 1: Bi-encoder retrieves top-10\")\n",
    "print(\"  Pass 2: Cross-encoder reranks to top-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ac489",
   "metadata": {},
   "source": [
    "## 8. Comparative Evaluation\n",
    "\n",
    "Test both systems with queries that benefit from cross-encoder re-ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd082a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "    \"How does the attention mechanism work in transformers?\",\n",
    "    \"What are the key differences between REST APIs and GraphQL?\",\n",
    "    \"Explain how BERT uses masked language modeling for training.\",\n",
    "]\n",
    "\n",
    "print(f\"Testing with {len(test_queries)} queries...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdacb84",
   "metadata": {},
   "source": [
    "### Query 1: Attention Mechanism in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6921077",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = test_queries[0]\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Baseline: Bi-encoder only (top-10)\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE: BI-ENCODER ONLY (Top-10)\")\n",
    "print(\"=\"*80)\n",
    "baseline_response = baseline_query_engine.query(query)\n",
    "\n",
    "print(f\"\\nRetrieved Nodes: {len(baseline_response.source_nodes)}\\n\")\n",
    "baseline_results = []\n",
    "for i, node in enumerate(baseline_response.source_nodes, 1):\n",
    "    file_name = Path(node.node.metadata.get('file_name', 'unknown')).name\n",
    "    baseline_results.append({\n",
    "        'Rank': i,\n",
    "        'Score': f\"{node.score:.4f}\",\n",
    "        'Source': file_name,\n",
    "        'Text': node.node.text[:150] + \"...\"\n",
    "    })\n",
    "    print(f\"Rank {i} | Score: {node.score:.4f} | Source: {file_name}\")\n",
    "    print(f\"  {node.node.text[:150]}...\\n\")\n",
    "\n",
    "# Two-pass: Bi-encoder + Cross-encoder reranking (top-3)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TWO-PASS: BI-ENCODER → CROSS-ENCODER RE-RANKING (Top-3)\")\n",
    "print(\"=\"*80)\n",
    "rerank_response = rerank_query_engine.query(query)\n",
    "\n",
    "print(f\"\\nRe-ranked Nodes: {len(rerank_response.source_nodes)}\\n\")\n",
    "rerank_results = []\n",
    "for i, node in enumerate(rerank_response.source_nodes, 1):\n",
    "    file_name = Path(node.node.metadata.get('file_name', 'unknown')).name\n",
    "    rerank_results.append({\n",
    "        'Rank': i,\n",
    "        'Cross-Encoder Score': f\"{node.score:.4f}\",\n",
    "        'Source': file_name,\n",
    "        'Text': node.node.text[:150] + \"...\"\n",
    "    })\n",
    "    print(f\"Rank {i} | Cross-Encoder Score: {node.score:.4f} | Source: {file_name}\")\n",
    "    print(f\"  {node.node.text[:150]}...\\n\")\n",
    "\n",
    "# Compare answers\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANSWER COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBaseline Answer (Bi-encoder only):\")\n",
    "print(baseline_response.response)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\\nRe-ranked Answer (With cross-encoder):\")\n",
    "print(rerank_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122397fa",
   "metadata": {},
   "source": [
    "### Query 2: REST APIs vs GraphQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = test_queries[1]\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Baseline\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE: BI-ENCODER ONLY\")\n",
    "print(\"=\"*80)\n",
    "baseline_response = baseline_query_engine.query(query)\n",
    "print(f\"\\nTop 3 from baseline (out of {len(baseline_response.source_nodes)}):\")\n",
    "for i, node in enumerate(baseline_response.source_nodes[:3], 1):\n",
    "    file_name = Path(node.node.metadata.get('file_name', 'unknown')).name\n",
    "    print(f\"  {i}. Score: {node.score:.4f} | {file_name}\")\n",
    "\n",
    "# Two-pass\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TWO-PASS: WITH CROSS-ENCODER RE-RANKING\")\n",
    "print(\"=\"*80)\n",
    "rerank_response = rerank_query_engine.query(query)\n",
    "print(f\"\\nRe-ranked top 3:\")\n",
    "for i, node in enumerate(rerank_response.source_nodes, 1):\n",
    "    file_name = Path(node.node.metadata.get('file_name', 'unknown')).name\n",
    "    print(f\"  {i}. Cross-Encoder Score: {node.score:.4f} | {file_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANSWER COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBaseline Answer:\")\n",
    "print(baseline_response.response)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\\nRe-ranked Answer:\")\n",
    "print(rerank_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea058b8",
   "metadata": {},
   "source": [
    "### Query 3: BERT Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee39574",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = test_queries[2]\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Baseline\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE: BI-ENCODER ONLY\")\n",
    "print(\"=\"*80)\n",
    "baseline_response = baseline_query_engine.query(query)\n",
    "print(f\"\\nTop 3 from baseline:\")\n",
    "for i, node in enumerate(baseline_response.source_nodes[:3], 1):\n",
    "    file_name = Path(node.node.metadata.get('file_name', 'unknown')).name\n",
    "    print(f\"  {i}. Score: {node.score:.4f} | {file_name}\")\n",
    "\n",
    "# Two-pass\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TWO-PASS: WITH CROSS-ENCODER RE-RANKING\")\n",
    "print(\"=\"*80)\n",
    "rerank_response = rerank_query_engine.query(query)\n",
    "print(f\"\\nRe-ranked top 3:\")\n",
    "for i, node in enumerate(rerank_response.source_nodes, 1):\n",
    "    file_name = Path(node.node.metadata.get('file_name', 'unknown')).name\n",
    "    print(f\"  {i}. Cross-Encoder Score: {node.score:.4f} | {file_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANSWER COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBaseline Answer:\")\n",
    "print(baseline_response.response)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\\nRe-ranked Answer:\")\n",
    "print(rerank_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5ce4c",
   "metadata": {},
   "source": [
    "## 9. Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15341447",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_md = \"\"\"\n",
    "### Two-Pass Retrieval Architecture\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                        USER QUERY                              │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  PASS 1: BI-ENCODER RETRIEVAL (Fast, Recall-Focused)           │\n",
    "│  ─────────────────────────────────────────────────────────      │\n",
    "│  • Encode query independently                                   │\n",
    "│  • Compute cosine similarity with all documents                 │\n",
    "│  • Retrieve top-K candidates (e.g., K=10)                       │\n",
    "│  • Fast: O(1) similarity computation                            │\n",
    "│  • Moderate accuracy: no query-doc interaction                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "                    Top-10 Candidates Retrieved\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  PASS 2: CROSS-ENCODER RE-RANKING (Accurate, Precision-Focused)│\n",
    "│  ───────────────────────────────────────────────────────────    │\n",
    "│  • Encode query+document pairs JOINTLY                          │\n",
    "│  • Capture interaction signals between query and doc            │\n",
    "│  • Re-rank candidates based on relevance scores                 │\n",
    "│  • Return top-N (e.g., N=3)                                     │\n",
    "│  • Slower but more accurate: O(K) evaluations                   │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "                      Top-3 Re-ranked Results\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                 LLM GENERATION                                  │\n",
    "│  Uses top-3 highly relevant contexts                            │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Bi-Encoder vs Cross-Encoder\n",
    "\n",
    "| Aspect | Bi-Encoder | Cross-Encoder |\n",
    "|--------|------------|---------------|\n",
    "| **Encoding** | Query and docs encoded separately | Query+doc encoded together |\n",
    "| **Interaction** | No interaction signals | Captures query-doc interaction |\n",
    "| **Speed** | Very fast (pre-computed embeddings) | Slower (O(n) evaluations) |\n",
    "| **Accuracy** | Good | Excellent |\n",
    "| **Use Case** | Initial retrieval (recall) | Re-ranking (precision) |\n",
    "| **Scalability** | Millions of docs | Limited to top-K candidates |\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **Best of Both Worlds**: Speed from bi-encoders + accuracy from cross-encoders\n",
    "2. **Improved Precision**: Cross-encoders catch nuanced relevance signals missed by bi-encoders\n",
    "3. **Ranking Quality**: Better ordering of results leads to better LLM context\n",
    "4. **Scalable**: Two-pass approach is computationally feasible even at scale\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(architecture_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50b618",
   "metadata": {},
   "source": [
    "## 10. Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f39eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two approaches\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Retrieval Model',\n",
    "        'Pass 1 (Recall)',\n",
    "        'Pass 2 (Precision)',\n",
    "        'Candidates Retrieved',\n",
    "        'Final Results',\n",
    "        'Query-Doc Interaction',\n",
    "        'Speed',\n",
    "        'Accuracy',\n",
    "        'Best For',\n",
    "    ],\n",
    "    'Baseline (Bi-Encoder Only)': [\n",
    "        'Bi-Encoder (Azure OpenAI)',\n",
    "        'Yes (cosine similarity)',\n",
    "        'No',\n",
    "        '10',\n",
    "        'Top-10',\n",
    "        'No (independent encoding)',\n",
    "        'Fast',\n",
    "        'Good',\n",
    "        'Speed-critical applications',\n",
    "    ],\n",
    "    'Two-Pass (Bi-Encoder + Cross-Encoder)': [\n",
    "        'Bi-Encoder → Cross-Encoder',\n",
    "        'Yes (bi-encoder)',\n",
    "        'Yes (cross-encoder rerank)',\n",
    "        '10',\n",
    "        'Top-3 (re-ranked)',\n",
    "        'Yes (joint encoding)',\n",
    "        'Fast overall (only 10 rerank ops)',\n",
    "        'Excellent',\n",
    "        'Accuracy-critical applications',\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "display(HTML(\"<h3>Comparative Analysis</h3>\"))\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3121ad",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Two-Pass Architecture is Best Practice**:\n",
    "   - Use fast bi-encoders for initial retrieval (recall optimization)\n",
    "   - Use accurate cross-encoders for final re-ranking (precision optimization)\n",
    "   - Get both speed and accuracy\n",
    "\n",
    "2. **Why Cross-Encoders Are More Accurate**:\n",
    "   - Joint encoding of query+document captures interaction signals\n",
    "   - Can model attention between query terms and document terms\n",
    "   - Better at detecting nuanced relevance\n",
    "\n",
    "3. **Computational Trade-offs**:\n",
    "   - Bi-encoder: O(1) per query after pre-computation (fast)\n",
    "   - Cross-encoder: O(K) evaluations where K is number of candidates (slower)\n",
    "   - Two-pass: Fast for initial retrieval, accurate for final ranking\n",
    "\n",
    "4. **Practical Impact**:\n",
    "   - Improved retrieval precision → better context for LLM\n",
    "   - Better ranking → reduced noise in top results\n",
    "   - Higher quality answers with minimal latency increase\n",
    "\n",
    "### When to Use Cross-Encoder Re-Ranking\n",
    "\n",
    "✅ **Good for**:\n",
    "- High-stakes applications where accuracy is critical\n",
    "- Complex queries requiring nuanced matching\n",
    "- When you can afford slight latency increase\n",
    "- Domain-specific retrieval (use domain-tuned cross-encoders)\n",
    "\n",
    "❌ **Less suitable for**:\n",
    "- Ultra-low latency requirements (< 100ms)\n",
    "- Very large candidate sets (> 100 candidates)\n",
    "- Simple keyword matching tasks\n",
    "\n",
    "### Popular Cross-Encoder Models\n",
    "\n",
    "1. **ms-marco-MiniLM-L-6-v2** (used in this demo): Fast, good balance\n",
    "2. **ms-marco-MiniLM-L-12-v2**: More accurate, slightly slower\n",
    "3. **nli-deberta-v3-large**: Very accurate for natural language inference\n",
    "4. **Domain-specific models**: Fine-tune on your domain for best results\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Latency Budget**: Cross-encoder adds ~50-200ms depending on model and batch size\n",
    "2. **Batch Processing**: Evaluate multiple candidates in parallel for efficiency\n",
    "3. **Caching**: Cache cross-encoder scores for frequently retrieved docs\n",
    "4. **Model Selection**: Choose cross-encoder based on accuracy/speed requirements\n",
    "5. **Monitoring**: Track ranking changes and answer quality improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656ac78",
   "metadata": {},
   "source": [
    "## 12. Further Exploration\n",
    "\n",
    "Try these experiments:\n",
    "1. Adjust `similarity_top_k` (5, 10, 20) and observe impact on final results\n",
    "2. Test different cross-encoder models and compare accuracy vs speed\n",
    "3. Combine with other techniques: HyDE + reranking, hybrid search + reranking\n",
    "4. Measure latency differences between single-pass and two-pass\n",
    "5. Fine-tune a cross-encoder on your domain-specific data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
