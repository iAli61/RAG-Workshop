{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45f7022",
   "metadata": {},
   "source": [
    "# Demo #11: RAG System Evaluation and Metrics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this demo, you will learn:\n",
    "1. **Why Evaluation Matters**: Understanding that systematic, quantitative evaluation is non-negotiable for production-quality RAG systems\n",
    "2. **Key RAG Metrics**: Implementing the four core metrics for evaluating both retrieval and generation components\n",
    "3. **LLM-as-Judge Pattern**: Using Azure OpenAI to automatically score RAG system outputs\n",
    "4. **Regression Testing**: Creating benchmark datasets to track performance over time\n",
    "5. **Comparative Analysis**: Evaluating multiple RAG configurations to identify the best architecture\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### The Need for Rigorous Evaluation\n",
    "\n",
    "As stated in the curriculum:\n",
    "> \"Systematic, quantitative evaluation is non-negotiable for building production-quality RAG systems and moving beyond anecdotal 'it works on my questions' testing. A robust evaluation framework involves establishing a benchmark dataset, defining clear metrics, and using automated tools to track performance over time.\"\n",
    "\n",
    "### Core RAG Evaluation Metrics\n",
    "\n",
    "RAG evaluation focuses on assessing both the **retrieval** and **generation** components separately:\n",
    "\n",
    "| Metric | Component | Description |\n",
    "|--------|-----------|-------------|\n",
    "| **Context Relevance (Precision)** | Retrieval | Measures the signal-to-noise ratio of retrieved context. Are the retrieved chunks relevant to the query? |\n",
    "| **Context Sufficiency (Recall)** | Retrieval | Measures whether the retrieved context contains all information needed to answer the query |\n",
    "| **Answer Relevance** | Generation | Measures whether the final answer is on-topic and directly addresses the user's query |\n",
    "| **Faithfulness (Hallucination Detection)** | Generation | Measures whether the answer is factually grounded in the provided context |\n",
    "| **Answer Correctness** | Generation | Measures factual accuracy against a ground truth answer |\n",
    "\n",
    "### The \"MLOps-ification\" of RAG\n",
    "\n",
    "The maturation of RAG development has led to its **\"MLOps-ification\"**, where building a RAG system now demands the same discipline as any other machine learning system:\n",
    "- **Automated testing** with versioned test datasets\n",
    "- **Continuous monitoring** for drift detection\n",
    "- **Metric-driven development** with regression tracking\n",
    "- **A/B testing** of different architectural components\n",
    "\n",
    "## Implementation Approach\n",
    "\n",
    "We will implement:\n",
    "1. A **gold standard dataset** with hand-crafted queries and ground truth answers\n",
    "2. **LLM-based evaluators** for each metric using Azure OpenAI\n",
    "3. An **automated evaluation pipeline** that scores any RAG system\n",
    "4. A **comparative analysis** between baseline and advanced RAG configurations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96207ec8",
   "metadata": {},
   "source": [
    "## Setup: Dependencies and Azure OpenAI Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4cd08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core dependencies\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# LlamaIndex core\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Azure OpenAI\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "print(\"✓ Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334d1a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Azure OpenAI...\n",
      "✓ LLM and embedding model configured successfully\n",
      "✓ LLM and embedding model configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Azure OpenAI Configuration with fallback\n",
    "# First, try Azure OpenAI, if not available, fall back to OpenAI or HuggingFace\n",
    "\n",
    "azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if azure_api_key and azure_endpoint:\n",
    "    print(\"Using Azure OpenAI...\")\n",
    "    llm = AzureOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        deployment_name=\"gpt-4o\",  # Your deployment name\n",
    "        api_key=azure_api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=\"2024-12-01-preview\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    embed_model = AzureOpenAIEmbedding(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment_name=\"text-embedding-ada-002\",\n",
    "        api_key=azure_api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=\"2024-12-01-preview\",\n",
    "    )\n",
    "elif openai_api_key:\n",
    "    print(\"Using OpenAI (standard)...\")\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    \n",
    "    llm = OpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=openai_api_key,\n",
    "        temperature=0.0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    embed_model = OpenAIEmbedding(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        api_key=openai_api_key,\n",
    "    )\n",
    "else:\n",
    "    print(\"Using HuggingFace embeddings (free, no API key required)...\")\n",
    "    from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    \n",
    "    # Use a free HuggingFace model\n",
    "    llm = HuggingFaceInferenceAPI(\n",
    "        model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        token=os.getenv(\"HF_TOKEN\"),  # Optional, but recommended for better rate limits\n",
    "        temperature=0.0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "    )\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"✓ LLM and embedding model configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d41c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Create Gold Standard Evaluation Dataset\n",
    "\n",
    "A **gold standard dataset** is a curated collection of test queries with:\n",
    "- **Ground truth answers** (ideal, correct responses)\n",
    "- **Reference context** (the specific documents that should be retrieved)\n",
    "- **Query metadata** (difficulty level, query type, etc.)\n",
    "\n",
    "This dataset serves as a regression test suite to ensure changes to the RAG system don't degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eebacc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created gold standard dataset with 5 test cases\n",
      "\n",
      "Dataset Breakdown:\n",
      "  - Query types: {'factual', 'multi-hop', 'conceptual', 'comparison'}\n",
      "  - Difficulty levels: {'easy', 'hard', 'medium'}\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EvaluationExample:\n",
    "    \"\"\"A single test case in our gold standard dataset\"\"\"\n",
    "    query: str\n",
    "    ground_truth_answer: str\n",
    "    reference_doc_names: List[str]  # Which documents should be retrieved\n",
    "    query_type: str  # e.g., \"factual\", \"conceptual\", \"comparison\"\n",
    "    difficulty: str  # \"easy\", \"medium\", \"hard\"\n",
    "\n",
    "# Gold Standard Test Dataset\n",
    "# These queries span different difficulty levels and query types\n",
    "EVALUATION_DATASET = [\n",
    "    EvaluationExample(\n",
    "        query=\"What is the transformer architecture?\",\n",
    "        ground_truth_answer=\"The transformer architecture is a neural network architecture introduced in the 'Attention is All You Need' paper. It relies entirely on self-attention mechanisms to process sequences in parallel, replacing recurrent layers. Key components include multi-head attention, positional encoding, and feed-forward networks arranged in encoder-decoder stacks.\",\n",
    "        reference_doc_names=[\"transformer_architecture.md\"],\n",
    "        query_type=\"conceptual\",\n",
    "        difficulty=\"easy\"\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        query=\"How does BERT differ from GPT-4?\",\n",
    "        ground_truth_answer=\"BERT and GPT-4 differ in their training objectives and architecture. BERT uses bidirectional encoding with masked language modeling and next sentence prediction, making it ideal for understanding tasks. GPT-4 is an autoregressive decoder-only model trained for next-token prediction, optimized for generation tasks. BERT processes context bidirectionally while GPT-4 processes left-to-right.\",\n",
    "        reference_doc_names=[\"bert_model.md\", \"gpt4_model.md\"],\n",
    "        query_type=\"comparison\",\n",
    "        difficulty=\"medium\"\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        query=\"Explain how embeddings work in machine learning\",\n",
    "        ground_truth_answer=\"Embeddings in machine learning are dense vector representations that map discrete objects (like words, sentences, or entities) into continuous vector spaces. They capture semantic relationships where similar items have vectors close together in the embedding space. Embeddings are learned through neural networks and enable models to perform mathematical operations on semantic concepts.\",\n",
    "        reference_doc_names=[\"embeddings_ml.md\"],\n",
    "        query_type=\"conceptual\",\n",
    "        difficulty=\"easy\"\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        query=\"What are the key components of a REST API and how do they relate to Docker containers?\",\n",
    "        ground_truth_answer=\"REST APIs consist of resources identified by URLs, HTTP methods (GET, POST, PUT, DELETE), stateless communication, and standardized response formats like JSON. Docker containers provide an ideal deployment environment for REST APIs by packaging the API application with all its dependencies into isolated, portable containers. This ensures consistent behavior across development and production environments.\",\n",
    "        reference_doc_names=[\"rest_api.md\", \"docker_containers.md\"],\n",
    "        query_type=\"multi-hop\",\n",
    "        difficulty=\"hard\"\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        query=\"What is Docker?\",\n",
    "        ground_truth_answer=\"Docker is a platform for developing, shipping, and running applications in containers. Containers are lightweight, standalone packages that include everything needed to run software: code, runtime, system tools, libraries, and settings. Docker ensures applications run consistently across different computing environments.\",\n",
    "        reference_doc_names=[\"docker_containers.md\"],\n",
    "        query_type=\"factual\",\n",
    "        difficulty=\"easy\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"✓ Created gold standard dataset with {len(EVALUATION_DATASET)} test cases\")\n",
    "print(\"\\nDataset Breakdown:\")\n",
    "print(f\"  - Query types: {set(ex.query_type for ex in EVALUATION_DATASET)}\")\n",
    "print(f\"  - Difficulty levels: {set(ex.difficulty for ex in EVALUATION_DATASET)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845faad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load Documents and Build RAG Systems\n",
    "\n",
    "We'll create two RAG systems to compare:\n",
    "1. **Baseline RAG**: Simple vector search with default settings\n",
    "2. **Advanced RAG**: Optimized with better chunking and higher retrieval count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b57ae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 6 documents\n",
      "  - bert_model.md: 2358 characters\n",
      "  - docker_containers.md: 4864 characters\n",
      "  - embeddings_ml.md: 5520 characters\n",
      "  - gpt4_model.md: 2814 characters\n",
      "  - rest_api.md: 3797 characters\n",
      "  - transformer_architecture.md: 4247 characters\n"
     ]
    }
   ],
   "source": [
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"data/tech_docs\").load_data()\n",
    "print(f\"✓ Loaded {len(documents)} documents\")\n",
    "\n",
    "# Display document metadata\n",
    "for doc in documents:\n",
    "    filename = doc.metadata.get('file_name', 'unknown')\n",
    "    print(f\"  - {filename}: {len(doc.text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34850bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Baseline RAG System...\n",
      "✓ Baseline RAG built with 8 chunks (chunk_size=1024, top_k=3)\n",
      "✓ Baseline RAG built with 8 chunks (chunk_size=1024, top_k=3)\n"
     ]
    }
   ],
   "source": [
    "# System 1: Baseline RAG\n",
    "print(\"Building Baseline RAG System...\")\n",
    "baseline_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "baseline_nodes = baseline_splitter.get_nodes_from_documents(documents)\n",
    "baseline_index = VectorStoreIndex(baseline_nodes)\n",
    "baseline_query_engine = baseline_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "print(f\"✓ Baseline RAG built with {len(baseline_nodes)} chunks (chunk_size=1024, top_k=3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87fe6d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Advanced RAG System...\n",
      "✓ Advanced RAG built with 15 chunks (chunk_size=512, top_k=5)\n",
      "✓ Advanced RAG built with 15 chunks (chunk_size=512, top_k=5)\n"
     ]
    }
   ],
   "source": [
    "# System 2: Advanced RAG (Optimized)\n",
    "print(\"Building Advanced RAG System...\")\n",
    "advanced_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "advanced_nodes = advanced_splitter.get_nodes_from_documents(documents)\n",
    "advanced_index = VectorStoreIndex(advanced_nodes)\n",
    "advanced_query_engine = advanced_index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    response_mode=\"tree_summarize\"  # Better synthesis\n",
    ")\n",
    "print(f\"✓ Advanced RAG built with {len(advanced_nodes)} chunks (chunk_size=512, top_k=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122db075",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Implement Evaluation Metrics\n",
    "\n",
    "We'll implement five core metrics using the **LLM-as-Judge** pattern, where Azure OpenAI evaluates the quality of RAG outputs.\n",
    "\n",
    "### Metric 1: Context Relevance (Retrieval Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2ed334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Context Relevance metric implemented\n"
     ]
    }
   ],
   "source": [
    "def evaluate_context_relevance(query: str, retrieved_contexts: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Measures the signal-to-noise ratio of retrieved context.\n",
    "    Returns a score from 0.0 to 1.0 representing the average relevance of retrieved chunks.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query\n",
    "        retrieved_contexts: List of retrieved text chunks\n",
    "    \n",
    "    Returns:\n",
    "        Average relevance score (0.0 = all irrelevant, 1.0 = all highly relevant)\n",
    "    \"\"\"\n",
    "    if not retrieved_contexts:\n",
    "        return 0.0\n",
    "    \n",
    "    relevance_scores = []\n",
    "    \n",
    "    for context in retrieved_contexts:\n",
    "        prompt = f\"\"\"You are an expert evaluator. Rate the relevance of the following context to the query on a scale of 1-5.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Relevance Scale:\n",
    "1 = Completely irrelevant, contains no information related to the query\n",
    "2 = Minimally relevant, tangentially related but not useful\n",
    "3 = Moderately relevant, contains some useful information\n",
    "4 = Very relevant, contains substantial information to answer the query\n",
    "5 = Perfectly relevant, directly and comprehensively addresses the query\n",
    "\n",
    "Respond with ONLY a single number (1-5), no explanation.\"\"\"\n",
    "        \n",
    "        response = llm.complete(prompt)\n",
    "        try:\n",
    "            score = int(response.text.strip())\n",
    "            # Normalize to 0-1 scale\n",
    "            normalized_score = (score - 1) / 4.0\n",
    "            relevance_scores.append(normalized_score)\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Could not parse relevance score: {response.text}\")\n",
    "            relevance_scores.append(0.5)  # Default to neutral\n",
    "    \n",
    "    return np.mean(relevance_scores)\n",
    "\n",
    "print(\"✓ Context Relevance metric implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b0fba",
   "metadata": {},
   "source": [
    "### Metric 2: Context Sufficiency (Retrieval Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e443c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Context Sufficiency metric implemented\n"
     ]
    }
   ],
   "source": [
    "def evaluate_context_sufficiency(query: str, retrieved_contexts: List[str], ground_truth: str) -> float:\n",
    "    \"\"\"\n",
    "    Measures whether retrieved context contains all information needed to answer the query.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query\n",
    "        retrieved_contexts: List of retrieved text chunks\n",
    "        ground_truth: The ideal answer (to determine what info is needed)\n",
    "    \n",
    "    Returns:\n",
    "        Sufficiency score (0.0 = insufficient, 1.0 = fully sufficient)\n",
    "    \"\"\"\n",
    "    # Truncate contexts if too long\n",
    "    MAX_CONTEXT_LENGTH = 3000\n",
    "    combined_context = \"\\n\\n\".join(retrieved_contexts)\n",
    "    if len(combined_context) > MAX_CONTEXT_LENGTH:\n",
    "        combined_context = combined_context[:MAX_CONTEXT_LENGTH] + \"\\n...[truncated]\"\n",
    "    \n",
    "    # Truncate ground truth if too long\n",
    "    MAX_GT_LENGTH = 500\n",
    "    truncated_gt = ground_truth[:MAX_GT_LENGTH] if len(ground_truth) > MAX_GT_LENGTH else ground_truth\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert evaluator. Determine if the provided context contains sufficient information to answer the query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Ground Truth Answer: {truncated_gt}\n",
    "\n",
    "Retrieved Context:\n",
    "{combined_context}\n",
    "\n",
    "Question: Does the retrieved context contain all the necessary information to produce an answer similar to the ground truth?\n",
    "\n",
    "Rate on a scale of 1-5:\n",
    "1 = Completely insufficient, missing all key information\n",
    "2 = Mostly insufficient, missing most key information\n",
    "3 = Partially sufficient, contains some but not all key information\n",
    "4 = Mostly sufficient, contains most key information with minor gaps\n",
    "5 = Fully sufficient, contains all necessary information\n",
    "\n",
    "Respond with ONLY a single number (1-5), no explanation.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.complete(prompt)\n",
    "        score = int(response.text.strip())\n",
    "        return (score - 1) / 4.0  # Normalize to 0-1\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Could not parse sufficiency score: {response.text}\")\n",
    "        return 0.5\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_context_sufficiency: {str(e)}\")\n",
    "        return 0.5\n",
    "\n",
    "print(\"✓ Context Sufficiency metric implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704a946",
   "metadata": {},
   "source": [
    "### Metric 3: Answer Faithfulness (Hallucination Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614a35f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Answer Faithfulness metric implemented\n"
     ]
    }
   ],
   "source": [
    "def evaluate_faithfulness(answer: str, retrieved_contexts: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Measures whether the answer is factually grounded in the provided context.\n",
    "    Detects hallucinations - information in the answer not present in the context.\n",
    "    \n",
    "    Args:\n",
    "        answer: The generated answer\n",
    "        retrieved_contexts: The context used to generate the answer\n",
    "    \n",
    "    Returns:\n",
    "        Faithfulness score (0.0 = contains hallucinations, 1.0 = fully grounded)\n",
    "    \"\"\"\n",
    "    # Truncate contexts if too long to avoid token limits\n",
    "    MAX_CONTEXT_LENGTH = 3000\n",
    "    combined_context = \"\\n\\n\".join(retrieved_contexts)\n",
    "    if len(combined_context) > MAX_CONTEXT_LENGTH:\n",
    "        combined_context = combined_context[:MAX_CONTEXT_LENGTH] + \"\\n...[truncated]\"\n",
    "    \n",
    "    # Truncate answer if too long\n",
    "    MAX_ANSWER_LENGTH = 1000\n",
    "    truncated_answer = answer[:MAX_ANSWER_LENGTH] if len(answer) > MAX_ANSWER_LENGTH else answer\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert fact-checker. Determine if the answer contains information NOT present in the context (hallucinations).\n",
    "\n",
    "Context:\n",
    "{combined_context}\n",
    "\n",
    "Generated Answer:\n",
    "{truncated_answer}\n",
    "\n",
    "Task: Identify if the answer contains factual claims that are NOT supported by the context.\n",
    "\n",
    "Rate faithfulness on a scale of 1-5:\n",
    "1 = Severe hallucinations, most claims are unsupported\n",
    "2 = Significant hallucinations, many claims are unsupported\n",
    "3 = Moderate hallucinations, some claims are unsupported\n",
    "4 = Minor hallucinations, answer is mostly grounded with small unsupported details\n",
    "5 = Fully faithful, all claims are directly supported by the context\n",
    "\n",
    "Respond with ONLY a single number (1-5), no explanation.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.complete(prompt)\n",
    "        score = int(response.text.strip())\n",
    "        return (score - 1) / 4.0  # Normalize to 0-1\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Could not parse faithfulness score: {response.text}\")\n",
    "        return 0.5\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_faithfulness: {str(e)}\")\n",
    "        return 0.5\n",
    "\n",
    "print(\"✓ Answer Faithfulness metric implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c06462",
   "metadata": {},
   "source": [
    "### Metric 4: Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4c7027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Answer Relevance metric implemented\n"
     ]
    }
   ],
   "source": [
    "def evaluate_answer_relevance(query: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Measures whether the answer is on-topic and directly addresses the query.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query\n",
    "        answer: The generated answer\n",
    "    \n",
    "    Returns:\n",
    "        Relevance score (0.0 = off-topic, 1.0 = directly addresses query)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are an expert evaluator. Determine if the answer directly addresses the query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Question: Does the answer directly and relevantly address what the query is asking for?\n",
    "\n",
    "Rate on a scale of 1-5:\n",
    "1 = Completely off-topic, does not address the query at all\n",
    "2 = Minimally relevant, touches on the topic but doesn't answer the query\n",
    "3 = Moderately relevant, partially addresses the query\n",
    "4 = Highly relevant, addresses most aspects of the query\n",
    "5 = Perfectly relevant, directly and comprehensively addresses the query\n",
    "\n",
    "Respond with ONLY a single number (1-5), no explanation.\"\"\"\n",
    "    \n",
    "    response = llm.complete(prompt)\n",
    "    try:\n",
    "        score = int(response.text.strip())\n",
    "        return (score - 1) / 4.0  # Normalize to 0-1\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not parse answer relevance score: {response.text}\")\n",
    "        return 0.5\n",
    "\n",
    "print(\"✓ Answer Relevance metric implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2901a",
   "metadata": {},
   "source": [
    "### Metric 5: Answer Correctness (Semantic Similarity + LLM Scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2457fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Answer Correctness metric implemented\n"
     ]
    }
   ],
   "source": [
    "def evaluate_answer_correctness(generated_answer: str, ground_truth: str) -> float:\n",
    "    \"\"\"\n",
    "    Measures factual accuracy against ground truth using hybrid approach:\n",
    "    1. Semantic similarity (embedding distance)\n",
    "    2. LLM-based factual correctness scoring\n",
    "    \n",
    "    Args:\n",
    "        generated_answer: The RAG system's answer\n",
    "        ground_truth: The gold standard answer\n",
    "    \n",
    "    Returns:\n",
    "        Correctness score (0.0 = incorrect, 1.0 = correct)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Component 1: Semantic similarity via embeddings\n",
    "        gen_embedding = embed_model.get_text_embedding(generated_answer)\n",
    "        gt_embedding = embed_model.get_text_embedding(ground_truth)\n",
    "        \n",
    "        semantic_sim = cosine_similarity(\n",
    "            [gen_embedding], \n",
    "            [gt_embedding]\n",
    "        )[0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute semantic similarity: {str(e)}\")\n",
    "        semantic_sim = 0.5\n",
    "    \n",
    "    # Component 2: LLM-based factual correctness\n",
    "    # Truncate if too long\n",
    "    MAX_LENGTH = 800\n",
    "    truncated_gen = generated_answer[:MAX_LENGTH] if len(generated_answer) > MAX_LENGTH else generated_answer\n",
    "    truncated_gt = ground_truth[:MAX_LENGTH] if len(ground_truth) > MAX_LENGTH else ground_truth\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert evaluator. Compare the generated answer against the ground truth answer.\n",
    "\n",
    "Ground Truth Answer:\n",
    "{truncated_gt}\n",
    "\n",
    "Generated Answer:\n",
    "{truncated_gen}\n",
    "\n",
    "Task: Rate the factual correctness of the generated answer compared to the ground truth.\n",
    "\n",
    "Rate on a scale of 1-5:\n",
    "1 = Completely incorrect, contains major factual errors\n",
    "2 = Mostly incorrect, contains significant errors\n",
    "3 = Partially correct, some facts are right but key information is wrong or missing\n",
    "4 = Mostly correct, captures main facts with minor inaccuracies or omissions\n",
    "5 = Fully correct, factually accurate and complete\n",
    "\n",
    "Respond with ONLY a single number (1-5), no explanation.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.complete(prompt)\n",
    "        llm_score = int(response.text.strip())\n",
    "        llm_score_normalized = (llm_score - 1) / 4.0\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Could not parse correctness score: {response.text}\")\n",
    "        llm_score_normalized = 0.5\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_answer_correctness: {str(e)}\")\n",
    "        llm_score_normalized = 0.5\n",
    "    \n",
    "    # Combine both scores (weighted average)\n",
    "    final_score = 0.4 * semantic_sim + 0.6 * llm_score_normalized\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "print(\"✓ Answer Correctness metric implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f100052",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Build Automated Evaluation Pipeline\n",
    "\n",
    "Now we'll create a pipeline that:\n",
    "1. Runs queries through a RAG system\n",
    "2. Extracts retrieved context and generated answers\n",
    "3. Applies all evaluation metrics\n",
    "4. Aggregates results into a comprehensive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd02683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ EvaluationResult dataclass defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Results for a single test case\"\"\"\n",
    "    query: str\n",
    "    query_type: str\n",
    "    difficulty: str\n",
    "    generated_answer: str\n",
    "    ground_truth: str\n",
    "    retrieved_contexts: List[str]\n",
    "    context_relevance: float\n",
    "    context_sufficiency: float\n",
    "    answer_faithfulness: float\n",
    "    answer_relevance: float\n",
    "    answer_correctness: float\n",
    "    \n",
    "    @property\n",
    "    def overall_score(self) -> float:\n",
    "        \"\"\"Compute weighted average of all metrics\"\"\"\n",
    "        return (\n",
    "            0.15 * self.context_relevance +\n",
    "            0.15 * self.context_sufficiency +\n",
    "            0.25 * self.answer_faithfulness +\n",
    "            0.20 * self.answer_relevance +\n",
    "            0.25 * self.answer_correctness\n",
    "        )\n",
    "\n",
    "print(\"✓ EvaluationResult dataclass defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8949916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Automated evaluation pipeline implemented\n"
     ]
    }
   ],
   "source": [
    "def evaluate_rag_system(query_engine, test_dataset: List[EvaluationExample], system_name: str) -> List[EvaluationResult]:\n",
    "    \"\"\"\n",
    "    Automated evaluation pipeline for a RAG system.\n",
    "    \n",
    "    Args:\n",
    "        query_engine: The RAG query engine to evaluate\n",
    "        test_dataset: List of gold standard test examples\n",
    "        system_name: Name of the system being evaluated (for logging)\n",
    "    \n",
    "    Returns:\n",
    "        List of EvaluationResult objects, one per test case\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {system_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for i, example in enumerate(test_dataset, 1):\n",
    "        print(f\"[{i}/{len(test_dataset)}] Processing: {example.query[:60]}...\")\n",
    "        \n",
    "        # Step 1: Query the RAG system\n",
    "        response = query_engine.query(example.query)\n",
    "        generated_answer = str(response)\n",
    "        \n",
    "        # Step 2: Extract retrieved context\n",
    "        retrieved_contexts = [node.node.text for node in response.source_nodes]\n",
    "        \n",
    "        # Step 3: Compute all metrics\n",
    "        print(\"  Computing metrics...\")\n",
    "        \n",
    "        context_rel = evaluate_context_relevance(example.query, retrieved_contexts)\n",
    "        context_suf = evaluate_context_sufficiency(example.query, retrieved_contexts, example.ground_truth_answer)\n",
    "        faithfulness = evaluate_faithfulness(generated_answer, retrieved_contexts)\n",
    "        ans_relevance = evaluate_answer_relevance(example.query, generated_answer)\n",
    "        correctness = evaluate_answer_correctness(generated_answer, example.ground_truth_answer)\n",
    "        \n",
    "        # Step 4: Store results\n",
    "        result = EvaluationResult(\n",
    "            query=example.query,\n",
    "            query_type=example.query_type,\n",
    "            difficulty=example.difficulty,\n",
    "            generated_answer=generated_answer,\n",
    "            ground_truth=example.ground_truth_answer,\n",
    "            retrieved_contexts=retrieved_contexts,\n",
    "            context_relevance=context_rel,\n",
    "            context_sufficiency=context_suf,\n",
    "            answer_faithfulness=faithfulness,\n",
    "            answer_relevance=ans_relevance,\n",
    "            answer_correctness=correctness\n",
    "        )\n",
    "        \n",
    "        results.append(result)\n",
    "        print(f\"  ✓ Overall Score: {result.overall_score:.3f}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Automated evaluation pipeline implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd25a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Run Evaluation on Both Systems\n",
    "\n",
    "Let's evaluate both our baseline and advanced RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63af90de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating: Baseline RAG (chunk_size=1024, top_k=3)\n",
      "================================================================================\n",
      "\n",
      "[1/5] Processing: What is the transformer architecture?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.771\n",
      "\n",
      "[2/5] Processing: How does BERT differ from GPT-4?...\n",
      "  ✓ Overall Score: 0.771\n",
      "\n",
      "[2/5] Processing: How does BERT differ from GPT-4?...\n",
      "  Computing metrics...\n",
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.659\n",
      "\n",
      "[3/5] Processing: Explain how embeddings work in machine learning...\n",
      "  ✓ Overall Score: 0.659\n",
      "\n",
      "[3/5] Processing: Explain how embeddings work in machine learning...\n",
      "  Computing metrics...\n",
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.822\n",
      "\n",
      "[4/5] Processing: What are the key components of a REST API and how do they re...\n",
      "  ✓ Overall Score: 0.822\n",
      "\n",
      "[4/5] Processing: What are the key components of a REST API and how do they re...\n",
      "  Computing metrics...\n",
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.606\n",
      "\n",
      "[5/5] Processing: What is Docker?...\n",
      "  ✓ Overall Score: 0.606\n",
      "\n",
      "[5/5] Processing: What is Docker?...\n",
      "  Computing metrics...\n",
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.785\n",
      "\n",
      "  ✓ Overall Score: 0.785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Baseline RAG\n",
    "baseline_results = evaluate_rag_system(\n",
    "    baseline_query_engine,\n",
    "    EVALUATION_DATASET,\n",
    "    \"Baseline RAG (chunk_size=1024, top_k=3)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4b8dd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating: Advanced RAG (chunk_size=512, top_k=5, tree_summarize)\n",
      "================================================================================\n",
      "\n",
      "[1/5] Processing: What is the transformer architecture?...\n",
      "  Computing metrics...\n",
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.810\n",
      "\n",
      "[2/5] Processing: How does BERT differ from GPT-4?...\n",
      "  ✓ Overall Score: 0.810\n",
      "\n",
      "[2/5] Processing: How does BERT differ from GPT-4?...\n",
      "  Computing metrics...\n",
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.640\n",
      "\n",
      "[3/5] Processing: Explain how embeddings work in machine learning...\n",
      "  ✓ Overall Score: 0.640\n",
      "\n",
      "[3/5] Processing: Explain how embeddings work in machine learning...\n",
      "  Computing metrics...\n",
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.841\n",
      "\n",
      "[4/5] Processing: What are the key components of a REST API and how do they re...\n",
      "  ✓ Overall Score: 0.841\n",
      "\n",
      "[4/5] Processing: What are the key components of a REST API and how do they re...\n",
      "  Computing metrics...\n",
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.616\n",
      "\n",
      "[5/5] Processing: What is Docker?...\n",
      "  ✓ Overall Score: 0.616\n",
      "\n",
      "[5/5] Processing: What is Docker?...\n",
      "  Computing metrics...\n",
      "  Computing metrics...\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error in evaluate_faithfulness: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "  ✓ Overall Score: 0.789\n",
      "\n",
      "  ✓ Overall Score: 0.789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Advanced RAG\n",
    "advanced_results = evaluate_rag_system(\n",
    "    advanced_query_engine,\n",
    "    EVALUATION_DATASET,\n",
    "    \"Advanced RAG (chunk_size=512, top_k=5, tree_summarize)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899164b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Comparative Analysis and Visualization\n",
    "\n",
    "Now let's analyze and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6535d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "DETAILED RESULTS BY QUERY\n",
      "====================================================================================================\n",
      "  System                                                 Query       Type Difficulty Context Relevance Context Sufficiency Faithfulness Answer Relevance Correctness Overall Score\n",
      "Baseline              What is the transformer architecture?... conceptual       easy             0.583               1.000        0.500            1.000       0.832         0.771\n",
      "Baseline                   How does BERT differ from GPT-4?... comparison     medium             0.583               0.500        0.500            1.000       0.686         0.659\n",
      "Baseline    Explain how embeddings work in machine learning... conceptual       easy             0.667               1.000        0.500            1.000       0.989         0.822\n",
      "Baseline What are the key components of a REST API and how ...  multi-hop       hard             0.333               0.500        0.500            0.750       0.824         0.606\n",
      "Baseline                                    What is Docker?...    factual       easy             0.417               1.000        0.500            1.000       0.989         0.785\n",
      "Advanced              What is the transformer architecture?... conceptual       easy             0.850               1.000        0.500            1.000       0.832         0.810\n",
      "Advanced                   How does BERT differ from GPT-4?... comparison     medium             0.450               0.500        0.500            1.000       0.689         0.640\n",
      "Advanced    Explain how embeddings work in machine learning... conceptual       easy             0.800               1.000        0.500            1.000       0.986         0.841\n",
      "Advanced What are the key components of a REST API and how ...  multi-hop       hard             0.400               0.750        0.500            0.750       0.676         0.616\n",
      "Advanced                                    What is Docker?...    factual       easy             0.450               1.000        0.500            1.000       0.984         0.789\n"
     ]
    }
   ],
   "source": [
    "def create_summary_table(results: List[EvaluationResult], system_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Create a summary DataFrame from evaluation results\"\"\"\n",
    "    data = []\n",
    "    for r in results:\n",
    "        data.append({\n",
    "            'System': system_name,\n",
    "            'Query': r.query[:50] + '...',\n",
    "            'Type': r.query_type,\n",
    "            'Difficulty': r.difficulty,\n",
    "            'Context Relevance': f\"{r.context_relevance:.3f}\",\n",
    "            'Context Sufficiency': f\"{r.context_sufficiency:.3f}\",\n",
    "            'Faithfulness': f\"{r.answer_faithfulness:.3f}\",\n",
    "            'Answer Relevance': f\"{r.answer_relevance:.3f}\",\n",
    "            'Correctness': f\"{r.answer_correctness:.3f}\",\n",
    "            'Overall Score': f\"{r.overall_score:.3f}\"\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create summary tables\n",
    "baseline_df = create_summary_table(baseline_results, \"Baseline\")\n",
    "advanced_df = create_summary_table(advanced_results, \"Advanced\")\n",
    "\n",
    "# Combine for comparison\n",
    "combined_df = pd.concat([baseline_df, advanced_df], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DETAILED RESULTS BY QUERY\")\n",
    "print(\"=\"*100)\n",
    "print(combined_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e915fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "AGGREGATE PERFORMANCE COMPARISON\n",
      "====================================================================================================\n",
      "                 Metric Baseline RAG Advanced RAG Improvement\n",
      "  Avg Context Relevance        0.517        0.590       14.2%\n",
      "Avg Context Sufficiency        0.800        0.850        6.2%\n",
      "       Avg Faithfulness        0.500        0.500        0.0%\n",
      "   Avg Answer Relevance        0.950        0.950        0.0%\n",
      "        Avg Correctness        0.864        0.833       -3.6%\n",
      "      Avg Overall Score        0.729        0.739        1.5%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_aggregate_metrics(results: List[EvaluationResult]) -> Dict[str, float]:\n",
    "    \"\"\"Compute aggregate statistics across all test cases\"\"\"\n",
    "    return {\n",
    "        'Avg Context Relevance': np.mean([r.context_relevance for r in results]),\n",
    "        'Avg Context Sufficiency': np.mean([r.context_sufficiency for r in results]),\n",
    "        'Avg Faithfulness': np.mean([r.answer_faithfulness for r in results]),\n",
    "        'Avg Answer Relevance': np.mean([r.answer_relevance for r in results]),\n",
    "        'Avg Correctness': np.mean([r.answer_correctness for r in results]),\n",
    "        'Avg Overall Score': np.mean([r.overall_score for r in results]),\n",
    "    }\n",
    "\n",
    "# Compute aggregates\n",
    "baseline_agg = compute_aggregate_metrics(baseline_results)\n",
    "advanced_agg = compute_aggregate_metrics(advanced_results)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Metric': list(baseline_agg.keys()),\n",
    "    'Baseline RAG': [f\"{v:.3f}\" for v in baseline_agg.values()],\n",
    "    'Advanced RAG': [f\"{v:.3f}\" for v in advanced_agg.values()],\n",
    "    'Improvement': [\n",
    "        f\"{((advanced_agg[k] - baseline_agg[k]) / baseline_agg[k] * 100):.1f}%\" \n",
    "        if baseline_agg[k] > 0 else \"N/A\"\n",
    "        for k in baseline_agg.keys()\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"AGGREGATE PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7171912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BASELINE: Performance by Query Type\n",
      "================================================================================\n",
      "            Count  Avg Overall Score  Avg Correctness  Avg Faithfulness\n",
      "factual       1.0           0.784874         0.989497               0.5\n",
      "multi-hop     1.0           0.606123         0.824491               0.5\n",
      "conceptual    2.0           0.796375         0.910498               0.5\n",
      "comparison    1.0           0.658892         0.685569               0.5\n",
      "\n",
      "================================================================================\n",
      "ADVANCED: Performance by Query Type\n",
      "================================================================================\n",
      "            Count  Avg Overall Score  Avg Correctness  Avg Faithfulness\n",
      "factual       1.0           0.788601         0.984403               0.5\n",
      "multi-hop     1.0           0.616385         0.675540               0.5\n",
      "conceptual    2.0           0.825982         0.908928               0.5\n",
      "comparison    1.0           0.639692         0.688769               0.5\n",
      "\n",
      "================================================================================\n",
      "BASELINE: Performance by Difficulty\n",
      "================================================================================\n",
      "        Count  Avg Overall Score  Avg Correctness  Avg Faithfulness\n",
      "easy      3.0           0.792541         0.936831               0.5\n",
      "hard      1.0           0.606123         0.824491               0.5\n",
      "medium    1.0           0.658892         0.685569               0.5\n",
      "\n",
      "================================================================================\n",
      "ADVANCED: Performance by Difficulty\n",
      "================================================================================\n",
      "        Count  Avg Overall Score  Avg Correctness  Avg Faithfulness\n",
      "easy      3.0           0.813522         0.934086               0.5\n",
      "hard      1.0           0.616385         0.675540               0.5\n",
      "medium    1.0           0.639692         0.688769               0.5\n"
     ]
    }
   ],
   "source": [
    "# Performance by query type\n",
    "def analyze_by_category(results: List[EvaluationResult], category_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Analyze performance breakdown by category (type or difficulty)\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # Get unique categories\n",
    "    categories = set(getattr(r, category_key) for r in results)\n",
    "    \n",
    "    for cat in categories:\n",
    "        cat_results = [r for r in results if getattr(r, category_key) == cat]\n",
    "        data[cat] = {\n",
    "            'Count': len(cat_results),\n",
    "            'Avg Overall Score': np.mean([r.overall_score for r in cat_results]),\n",
    "            'Avg Correctness': np.mean([r.answer_correctness for r in cat_results]),\n",
    "            'Avg Faithfulness': np.mean([r.answer_faithfulness for r in cat_results]),\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(data).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE: Performance by Query Type\")\n",
    "print(\"=\"*80)\n",
    "baseline_by_type = analyze_by_category(baseline_results, 'query_type')\n",
    "print(baseline_by_type.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVANCED: Performance by Query Type\")\n",
    "print(\"=\"*80)\n",
    "advanced_by_type = analyze_by_category(advanced_results, 'query_type')\n",
    "print(advanced_by_type.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE: Performance by Difficulty\")\n",
    "print(\"=\"*80)\n",
    "baseline_by_diff = analyze_by_category(baseline_results, 'difficulty')\n",
    "print(baseline_by_diff.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVANCED: Performance by Difficulty\")\n",
    "print(\"=\"*80)\n",
    "advanced_by_diff = analyze_by_category(advanced_results, 'difficulty')\n",
    "print(advanced_by_diff.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c2454",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Detailed Example Analysis\n",
    "\n",
    "Let's examine a specific query in detail to understand the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10fb7a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "DETAILED EXAMPLE COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "Query: What is the transformer architecture?\n",
      "Type: conceptual | Difficulty: easy\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GROUND TRUTH ANSWER:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The transformer architecture is a neural network architecture introduced in the 'Attention is All You Need' paper. It relies entirely on self-attention mechanisms to process sequences in parallel, replacing recurrent layers. Key components include multi-head attention, positional encoding, and feed-forward networks arranged in encoder-decoder stacks.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE RAG ANSWER:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The transformer architecture is a deep learning model that revolutionized natural language processing by using an attention-based mechanism instead of the sequential processing found in RNNs and LSTMs. It allows for parallel processing of sequences and effectively captures long-range dependencies. Key components include self-attention mechanisms, multi-head attention, position encodings, and feed-forward networks. The architecture consists of an encoder and a decoder, each with multiple layers that incorporate attention, normalization, and residual connections. Transformers offer advantages such as parallelization, efficient handling of long-range dependencies, and computational efficiency, making them suitable for various applications beyond NLP, including computer vision and speech processing.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ADVANCED RAG ANSWER:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The transformer architecture is a deep learning model that revolutionized natural language processing by using an attention-based mechanism instead of the sequential processing found in RNNs and LSTMs. It enables parallel processing of sequences and effectively captures long-range dependencies. The core components include self-attention mechanisms, multi-head attention, position encodings, and feed-forward networks. The architecture consists of an encoder and a decoder, each with multiple identical layers featuring attention mechanisms, layer normalization, feed-forward networks, and residual connections. Transformers offer advantages such as parallelization, computational efficiency, and the ability to leverage transfer learning. They have been adapted for various applications beyond NLP, including computer vision, speech processing, and more.\n",
      "\n",
      "====================================================================================================\n",
      "METRIC COMPARISON\n",
      "====================================================================================================\n",
      "             Metric Baseline Advanced Difference     Winner\n",
      "  Context Relevance    0.583    0.850     +0.267 ✓ Advanced\n",
      "Context Sufficiency    1.000    1.000     +0.000        Tie\n",
      "Answer Faithfulness    0.500    0.500     +0.000        Tie\n",
      "   Answer Relevance    1.000    1.000     +0.000        Tie\n",
      " Answer Correctness    0.832    0.832     -0.000 ✓ Baseline\n",
      "      Overall Score    0.771    0.810     +0.040 ✓ Advanced\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RETRIEVED CONTEXT COMPARISON\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Baseline retrieved 3 chunks\n",
      "Advanced retrieved 5 chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_detailed_comparison(baseline_result: EvaluationResult, advanced_result: EvaluationResult):\n",
    "    \"\"\"Display side-by-side detailed comparison for a single query\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED EXAMPLE COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    print(f\"\\nQuery: {baseline_result.query}\")\n",
    "    print(f\"Type: {baseline_result.query_type} | Difficulty: {baseline_result.difficulty}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"GROUND TRUTH ANSWER:\")\n",
    "    print(\"-\"*100)\n",
    "    print(baseline_result.ground_truth)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"BASELINE RAG ANSWER:\")\n",
    "    print(\"-\"*100)\n",
    "    print(baseline_result.generated_answer)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"ADVANCED RAG ANSWER:\")\n",
    "    print(\"-\"*100)\n",
    "    print(advanced_result.generated_answer)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"METRIC COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    metrics = [\n",
    "        ('Context Relevance', baseline_result.context_relevance, advanced_result.context_relevance),\n",
    "        ('Context Sufficiency', baseline_result.context_sufficiency, advanced_result.context_sufficiency),\n",
    "        ('Answer Faithfulness', baseline_result.answer_faithfulness, advanced_result.answer_faithfulness),\n",
    "        ('Answer Relevance', baseline_result.answer_relevance, advanced_result.answer_relevance),\n",
    "        ('Answer Correctness', baseline_result.answer_correctness, advanced_result.answer_correctness),\n",
    "        ('Overall Score', baseline_result.overall_score, advanced_result.overall_score),\n",
    "    ]\n",
    "    \n",
    "    metric_df = pd.DataFrame([\n",
    "        {\n",
    "            'Metric': name,\n",
    "            'Baseline': f\"{baseline:.3f}\",\n",
    "            'Advanced': f\"{advanced:.3f}\",\n",
    "            'Difference': f\"{(advanced - baseline):+.3f}\",\n",
    "            'Winner': '✓ Advanced' if advanced > baseline else ('✓ Baseline' if baseline > advanced else 'Tie')\n",
    "        }\n",
    "        for name, baseline, advanced in metrics\n",
    "    ])\n",
    "    \n",
    "    print(metric_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"RETRIEVED CONTEXT COMPARISON\")\n",
    "    print(\"-\"*100)\n",
    "    print(f\"\\nBaseline retrieved {len(baseline_result.retrieved_contexts)} chunks\")\n",
    "    print(f\"Advanced retrieved {len(advanced_result.retrieved_contexts)} chunks\\n\")\n",
    "\n",
    "# Display detailed comparison for the first query (easy conceptual)\n",
    "display_detailed_comparison(baseline_results[0], advanced_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68454bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "DETAILED EXAMPLE COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "Query: What are the key components of a REST API and how do they relate to Docker containers?\n",
      "Type: multi-hop | Difficulty: hard\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GROUND TRUTH ANSWER:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "REST APIs consist of resources identified by URLs, HTTP methods (GET, POST, PUT, DELETE), stateless communication, and standardized response formats like JSON. Docker containers provide an ideal deployment environment for REST APIs by packaging the API application with all its dependencies into isolated, portable containers. This ensures consistent behavior across development and production environments.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE RAG ANSWER:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The key components of a REST API include client-server architecture, statelessness, cacheability, uniform interface, and layered system. These components ensure that REST APIs are scalable, efficient, and easy to interact with. In relation to Docker containers, both REST APIs and Docker containers emphasize scalability and efficiency. Docker containers provide isolated environments for applications, which can be beneficial for deploying REST APIs as they allow for consistent and portable application environments. Additionally, Docker's use of images and containers aligns with the stateless nature of REST APIs, as containers can be ephemeral and stateless, similar to how REST APIs handle requests independently without storing client context. Both technologies support scalability and flexibility, making them suitable for modern web services and applications.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ADVANCED RAG ANSWER:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The key components of a REST API include client-server architecture, statelessness, cacheability, uniform interface, and layered system. These components ensure that REST APIs are scalable, efficient, and easy to interact with. In relation to Docker containers, REST APIs can be used to manage and interact with Docker's core components, such as the Docker Daemon, through its REST API interface. This allows for operations like starting, stopping, and managing containers programmatically. Docker containers, being isolated and lightweight, can host applications that expose REST APIs, enabling seamless communication and integration with other services and systems. Both REST APIs and Docker containers emphasize scalability, efficiency, and modularity, making them complementary in modern application development and deployment.\n",
      "\n",
      "====================================================================================================\n",
      "METRIC COMPARISON\n",
      "====================================================================================================\n",
      "             Metric Baseline Advanced Difference     Winner\n",
      "  Context Relevance    0.333    0.400     +0.067 ✓ Advanced\n",
      "Context Sufficiency    0.500    0.750     +0.250 ✓ Advanced\n",
      "Answer Faithfulness    0.500    0.500     +0.000        Tie\n",
      "   Answer Relevance    0.750    0.750     +0.000        Tie\n",
      " Answer Correctness    0.824    0.676     -0.149 ✓ Baseline\n",
      "      Overall Score    0.606    0.616     +0.010 ✓ Advanced\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RETRIEVED CONTEXT COMPARISON\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Baseline retrieved 3 chunks\n",
      "Advanced retrieved 5 chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display detailed comparison for a harder query (multi-hop)\n",
    "display_detailed_comparison(baseline_results[3], advanced_results[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1304f2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Export Results for Regression Testing\n",
    "\n",
    "Save evaluation results to enable continuous monitoring and regression detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5732ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results exported to baseline_eval_results.json\n",
      "✓ Results exported to advanced_eval_results.json\n",
      "\n",
      "✓ Evaluation results saved for regression testing\n"
     ]
    }
   ],
   "source": [
    "def export_evaluation_results(results: List[EvaluationResult], system_name: str, output_file: str):\n",
    "    \"\"\"Export evaluation results to JSON for version control and tracking\"\"\"\n",
    "    \n",
    "    export_data = {\n",
    "        'system_name': system_name,\n",
    "        'evaluation_date': '2025-10-16',\n",
    "        'aggregate_metrics': compute_aggregate_metrics(results),\n",
    "        'detailed_results': [\n",
    "            {\n",
    "                'query': r.query,\n",
    "                'query_type': r.query_type,\n",
    "                'difficulty': r.difficulty,\n",
    "                'metrics': {\n",
    "                    'context_relevance': r.context_relevance,\n",
    "                    'context_sufficiency': r.context_sufficiency,\n",
    "                    'answer_faithfulness': r.answer_faithfulness,\n",
    "                    'answer_relevance': r.answer_relevance,\n",
    "                    'answer_correctness': r.answer_correctness,\n",
    "                    'overall_score': r.overall_score\n",
    "                },\n",
    "                'generated_answer': r.generated_answer,\n",
    "                'num_retrieved_chunks': len(r.retrieved_contexts)\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results exported to {output_file}\")\n",
    "\n",
    "# Export both systems\n",
    "export_evaluation_results(baseline_results, \"Baseline RAG\", \"baseline_eval_results.json\")\n",
    "export_evaluation_results(advanced_results, \"Advanced RAG\", \"advanced_eval_results.json\")\n",
    "\n",
    "print(\"\\n✓ Evaluation results saved for regression testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb6d813",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways and Best Practices\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Evaluation is Multi-Dimensional**\n",
    "   - RAG systems must be evaluated along multiple axes: retrieval quality (relevance, sufficiency) and generation quality (faithfulness, relevance, correctness)\n",
    "   - No single metric tells the full story—you need a comprehensive suite\n",
    "\n",
    "2. **LLM-as-Judge is Powerful**\n",
    "   - Using a powerful LLM (like GPT-4) as an evaluator enables automated, nuanced assessment of complex qualities like \"faithfulness\" and \"relevance\"\n",
    "   - This automation makes continuous evaluation feasible\n",
    "\n",
    "3. **Gold Standard Datasets are Essential**\n",
    "   - A curated benchmark dataset with ground truth answers is the foundation of rigorous evaluation\n",
    "   - This enables regression testing: detect when changes degrade performance\n",
    "\n",
    "4. **Comparative Analysis Drives Optimization**\n",
    "   - Evaluating multiple configurations side-by-side reveals which architectural choices matter\n",
    "   - In our example: smaller chunks + higher top_k improved performance across most metrics\n",
    "\n",
    "5. **Category-Specific Analysis Reveals Weaknesses**\n",
    "   - Breaking down performance by query type (factual, conceptual, multi-hop) and difficulty helps identify specific failure modes\n",
    "   - This guides targeted improvements\n",
    "\n",
    "### Production Best Practices (from Curriculum)\n",
    "\n",
    "1. **Establish a Gold Standard Early**: Create your benchmark dataset at the start of development, not as an afterthought\n",
    "\n",
    "2. **Automate Testing Pipelines**: Integrate RAG evaluation into CI/CD workflows to test every change automatically\n",
    "\n",
    "3. **Monitor for Drift**: Continuously monitor production metrics to detect degradation as data or models change over time\n",
    "\n",
    "4. **Map Metrics to Failure Points**: Use evaluation results to diagnose specific failure modes:\n",
    "   - Low context relevance → FP2: Retrieval failure\n",
    "   - Low faithfulness → FP4: Generation failure (hallucination)\n",
    "   - Low correctness but high faithfulness → FP3: Missing information in context\n",
    "\n",
    "5. **Version Control Your Evaluations**: Store evaluation results in version control alongside your code to track performance evolution\n",
    "\n",
    "### The \"MLOps-ification\" of RAG\n",
    "\n",
    "As the curriculum emphasizes:\n",
    "> \"The maturation of RAG development has led to its 'MLOps-ification,' where building a RAG system now demands the same discipline as any other machine learning system: automated testing, versioned datasets, continuous monitoring for drift, and metric-driven development.\"\n",
    "\n",
    "This demo provides the foundation for treating RAG development with the same rigor as traditional ML engineering.\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- **RAG Evaluation Metrics**: Best Practices for Evaluating RAG Systems - Patronus AI (Reference 75)\n",
    "- **Evaluating retrieval in RAGs**: a practical framework - Tweag (Reference 73)  \n",
    "- **RAG systems**: Best practices to master evaluation - Google Cloud (Reference 74)\n",
    "- **Seven Failure Points** When Engineering a RAG System - arXiv (Reference 10)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
