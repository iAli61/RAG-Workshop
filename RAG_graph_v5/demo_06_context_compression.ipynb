{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02019e7f",
   "metadata": {},
   "source": [
    "# Demo #6: Context Compression and Filtering\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo demonstrates **context compression and filtering** techniques that improve RAG systems by reducing noise, saving tokens, and enhancing answer quality through intelligent distillation of retrieved information.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Standard RAG retrieval often includes:\n",
    "- **Redundant information**: Multiple chunks containing similar content\n",
    "- **Marginally relevant content**: Text that matches semantically but doesn't directly answer the query\n",
    "- **Excessive context**: Long passages where only a few sentences are truly relevant\n",
    "- **Lost-in-the-middle**: Important information buried in long contexts gets ignored by LLMs\n",
    "\n",
    "**Consequences:**\n",
    "- Wasted tokens → Higher costs\n",
    "- Increased latency → Slower responses\n",
    "- Diluted signal → Lower answer quality\n",
    "- Context overflow → Truncated or lost information\n",
    "\n",
    "### The Solution: Context Compression\n",
    "\n",
    "Context compression intelligently distills retrieved content:\n",
    "1. **Filter**: Remove irrelevant chunks based on relevance thresholds\n",
    "2. **Extract**: Pull out only the relevant sentences from each chunk\n",
    "3. **Reorder**: Position most relevant content strategically (beginning/end)\n",
    "4. **Compress**: Generate concise summaries while preserving key information\n",
    "\n",
    "### Core Concepts Demonstrated\n",
    "- Extractive context compression\n",
    "- LLM-based filtering and extraction\n",
    "- Signal-to-noise ratio improvement\n",
    "- Token optimization without information loss\n",
    "- Lost-in-the-middle problem mitigation\n",
    "\n",
    "### References\n",
    "- Efficient RAG with Compression and Filtering - LanceDB (Reference 39)\n",
    "- Contextual Compression in RAG for LLMs: A Survey - arXiv (Reference 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c1689",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33271f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import (\n",
    "    LongContextReorder,\n",
    "    SimilarityPostprocessor,\n",
    ")\n",
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from typing import List, Optional\n",
    "\n",
    "# Visualization and analysis\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import tiktoken\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582dd7e",
   "metadata": {},
   "source": [
    "## 2. Azure OpenAI Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9515bff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Azure OpenAI configured successfully\n",
      "  LLM Deployment: gpt-4\n",
      "  Embedding Deployment: text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "# Azure OpenAI configuration from environment variables\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-ada-002\")\n",
    "\n",
    "# Validate configuration\n",
    "if not all([AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT]):\n",
    "    raise ValueError(\n",
    "        \"Missing Azure OpenAI configuration. Please set:\\n\"\n",
    "        \"- AZURE_OPENAI_API_KEY\\n\"\n",
    "        \"- AZURE_OPENAI_ENDPOINT\\n\"\n",
    "        \"- AZURE_OPENAI_DEPLOYMENT (optional, default: gpt-4)\\n\"\n",
    "        \"- AZURE_OPENAI_EMBEDDING_DEPLOYMENT (optional, default: text-embedding-ada-002)\"\n",
    "    )\n",
    "\n",
    "# Initialize Azure OpenAI LLM\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI Embeddings\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=AZURE_OPENAI_EMBEDDING_DEPLOYMENT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    ")\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"✓ Azure OpenAI configured successfully\")\n",
    "print(f\"  LLM Deployment: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "print(f\"  Embedding Deployment: {AZURE_OPENAI_EMBEDDING_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e514d8",
   "metadata": {},
   "source": [
    "## 3. Token Counting Utility\n",
    "\n",
    "Create utility function to count tokens for cost analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7bbdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 'This is a test sentence for token counting.' = 9 tokens\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer for GPT-4\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def format_token_count(count: int) -> str:\n",
    "    \"\"\"Format token count with cost estimate.\"\"\"\n",
    "    # GPT-4 pricing (approximate): $0.03 per 1K input tokens\n",
    "    cost_per_1k = 0.03\n",
    "    cost = (count / 1000) * cost_per_1k\n",
    "    return f\"{count:,} tokens (≈${cost:.4f})\"\n",
    "\n",
    "# Test\n",
    "test_text = \"This is a test sentence for token counting.\"\n",
    "print(f\"Test: '{test_text}' = {count_tokens(test_text)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c3cb9",
   "metadata": {},
   "source": [
    "## 4. Custom Sentence-Level Extraction Post-Processor\n",
    "\n",
    "Implement a post-processor that uses the LLM to extract only relevant sentences from each retrieved chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5929fcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM Sentence Extractor ready\n"
     ]
    }
   ],
   "source": [
    "class LLMSentenceExtractor(BaseNodePostprocessor):\n",
    "    \"\"\"Extract only relevant sentences from retrieved nodes using LLM.\"\"\"\n",
    "    \n",
    "    llm: Optional[object] = None\n",
    "    extraction_threshold: float = 0.5\n",
    "    \n",
    "    def __init__(self, llm, extraction_threshold: float = 0.5):\n",
    "        \"\"\"Initialize sentence extractor.\n",
    "        \n",
    "        Args:\n",
    "            llm: LLM to use for extraction\n",
    "            extraction_threshold: Minimum relevance threshold (not used with LLM)\n",
    "        \"\"\"\n",
    "        super().__init__(llm=llm, extraction_threshold=extraction_threshold)\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_bundle: Optional[QueryBundle] = None,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"Extract relevant sentences from nodes.\"\"\"\n",
    "        if query_bundle is None:\n",
    "            return nodes\n",
    "        \n",
    "        query_str = query_bundle.query_str\n",
    "        compressed_nodes = []\n",
    "        \n",
    "        for node in nodes:\n",
    "            original_text = node.node.get_content()\n",
    "            \n",
    "            # Prompt for extraction\n",
    "            extraction_prompt = f\"\"\"Given the query and text below, extract ONLY the sentences that directly help answer the query.\n",
    "Return the relevant sentences separated by newlines. If no sentences are relevant, return \"NONE\".\n",
    "\n",
    "Query: {query_str}\n",
    "\n",
    "Text:\n",
    "{original_text}\n",
    "\n",
    "Relevant sentences:\"\"\"\n",
    "            \n",
    "            # Extract relevant sentences using LLM\n",
    "            response = self.llm.complete(extraction_prompt)\n",
    "            extracted_text = response.text.strip()\n",
    "            \n",
    "            # Skip if no relevant sentences\n",
    "            if extracted_text.upper() == \"NONE\" or not extracted_text:\n",
    "                continue\n",
    "            \n",
    "            # Create new node with compressed content\n",
    "            compressed_node = NodeWithScore(\n",
    "                node=node.node.copy(),\n",
    "                score=node.score,\n",
    "            )\n",
    "            compressed_node.node.text = extracted_text\n",
    "            compressed_node.node.metadata['original_length'] = len(original_text)\n",
    "            compressed_node.node.metadata['compressed_length'] = len(extracted_text)\n",
    "            compressed_node.node.metadata['compression_ratio'] = len(extracted_text) / len(original_text)\n",
    "            \n",
    "            compressed_nodes.append(compressed_node)\n",
    "        \n",
    "        return compressed_nodes\n",
    "\n",
    "# Initialize extractor\n",
    "sentence_extractor = LLMSentenceExtractor(llm=llm)\n",
    "\n",
    "print(\"✓ LLM Sentence Extractor ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4da6e",
   "metadata": {},
   "source": [
    "## 5. Data Preparation\n",
    "\n",
    "Load documents with verbose content to demonstrate compression benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "305da74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "\n",
      "✓ Loaded 3 documents\n",
      "  Total: 44,163 characters, 7,586 tokens (≈$0.2276)\n",
      "  1. advanced_chunking_strategies.md: 13,411 chars, 2295 tokens\n",
      "  2. embedding_models_deep_dive.md: 13,849 chars, 2370 tokens\n",
      "  3. rag_comprehensive_guide.md: 16,903 chars, 2921 tokens\n"
     ]
    }
   ],
   "source": [
    "# Define data directory - using long-form docs for verbose content\n",
    "data_dir = Path(\"./data/long_form_docs\")\n",
    "\n",
    "# Load documents\n",
    "print(\"Loading documents...\")\n",
    "documents = SimpleDirectoryReader(str(data_dir)).load_data()\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(documents)} documents\")\n",
    "total_chars = sum(len(doc.text) for doc in documents)\n",
    "total_tokens = sum(count_tokens(doc.text) for doc in documents)\n",
    "print(f\"  Total: {total_chars:,} characters, {format_token_count(total_tokens)}\")\n",
    "\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    file_name = Path(doc.metadata.get('file_name', 'unknown')).name\n",
    "    doc_tokens = count_tokens(doc.text)\n",
    "    print(f\"  {i}. {file_name}: {len(doc.text):,} chars, {doc_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6639f",
   "metadata": {},
   "source": [
    "## 6. Build Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25185659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vector index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:04:19,979 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:20,206 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:20,206 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vector index built successfully\n"
     ]
    }
   ],
   "source": [
    "# Build index\n",
    "print(\"Building vector index...\")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "print(\"✓ Vector index built successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcc1b80",
   "metadata": {},
   "source": [
    "## 7. Baseline: No Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469bcf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Baseline query engine ready (no compression)\n"
     ]
    }
   ],
   "source": [
    "# Baseline retriever and query engine\n",
    "baseline_retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "baseline_query_engine = RetrieverQueryEngine(\n",
    "    retriever=baseline_retriever,\n",
    ")\n",
    "\n",
    "print(\"✓ Baseline query engine ready (no compression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025798dd",
   "metadata": {},
   "source": [
    "## 8. Context Compression Pipeline\n",
    "\n",
    "Create a multi-stage compression pipeline:\n",
    "1. **Filter**: Remove low-relevance nodes (SimilarityPostprocessor)\n",
    "2. **Reorder**: Address lost-in-the-middle (LongContextReorder)\n",
    "3. **Extract**: Pull relevant sentences (LLMSentenceExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189d80fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Compression query engine ready\n",
      "  Pipeline: Filter → Reorder → Extract\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Filter low-relevance nodes\n",
    "similarity_filter = SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "\n",
    "# Stage 2: Reorder to address lost-in-the-middle\n",
    "context_reorderer = LongContextReorder()\n",
    "\n",
    "# Stage 3: Extract relevant sentences (already created)\n",
    "# sentence_extractor (from earlier)\n",
    "\n",
    "# Build compression pipeline\n",
    "compression_retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "compression_query_engine = RetrieverQueryEngine(\n",
    "    retriever=compression_retriever,\n",
    "    node_postprocessors=[\n",
    "        similarity_filter,\n",
    "        context_reorderer,\n",
    "        sentence_extractor,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"✓ Compression query engine ready\")\n",
    "print(\"  Pipeline: Filter → Reorder → Extract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb522439",
   "metadata": {},
   "source": [
    "## 9. Comparative Evaluation\n",
    "\n",
    "Test both systems and analyze token savings and answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b463afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 3 queries...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "    \"What are the main advantages of RAG over pure LLM approaches?\",\n",
    "    \"Explain the chunking trade-off in RAG systems.\",\n",
    "    \"How do vector databases enable efficient similarity search?\",\n",
    "]\n",
    "\n",
    "print(f\"Testing with {len(test_queries)} queries...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab66bc",
   "metadata": {},
   "source": [
    "### Query 1: RAG Advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b6d2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:04:20,392 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the main advantages of RAG over pure LLM approaches?\n",
      "\n",
      "================================================================================\n",
      "BASELINE: NO COMPRESSION\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:04:22,857 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:22,985 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:22,985 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Nodes: 5\n",
      "Total Context: 1,960 tokens (≈$0.0588)\n",
      "\n",
      "Node 1 (Score: 0.8424, 451 tokens):\n",
      "  # Comprehensive Guide to Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "## Introduction to RAG\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a paradigm that combines the strengths of large language models with exte...\n",
      "\n",
      "Node 2 (Score: 0.8248, 450 tokens):\n",
      "  Some systems also include few-shot examples demonstrating desired answer formats.\n",
      "\n",
      "Citation and attribution are important for trust and verifiability. Prompts should instruct the model to cite sources...\n",
      "\n",
      "Node 3 (Score: 0.8219, 395 tokens):\n",
      "  Fusion retrieval combines multiple retrieval strategies, such as dense vector search and sparse keyword search (BM25), then merges their results using techniques like Reciprocal Rank Fusion. This hybr...\n",
      "\n",
      "Node 4 (Score: 0.8015, 459 tokens):\n",
      "  Multi-query approaches decompose complex questions into simpler sub-questions, retrieve relevant context for each sub-question independently, and then aggregate the results. This is particularly effec...\n",
      "\n",
      "Node 5 (Score: 0.8002, 205 tokens):\n",
      "  The result is a set of standalone statements that can be understood without the original context.\n",
      "\n",
      "Proposition-based chunking offers several advantages for certain RAG applications. Each proposition c...\n",
      "\n",
      "Answer:\n",
      "The main advantages of Retrieval-Augmented Generation (RAG) over pure LLM approaches include:\n",
      "\n",
      "1. **Access to Up-to-Date Information**: RAG systems can retrieve information from external sources, overcoming the knowledge cutoff limitations of standalone LLMs.\n",
      "\n",
      "2. **Reduced Hallucination**: By grounding responses in retrieved factual context, RAG reduces the risk of generating plausible but incorrect information.\n",
      "\n",
      "3. **Easier Knowledge Updates**: Knowledge bases in RAG systems can be updated without requiring expensive retraining of the model, making it practical for rapidly changing domains.\n",
      "\n",
      "4. **Transparency and Verifiability**: RAG allows answers to be traced back to specific sources, enabling users to verify the information and increasing trust in the system.\n",
      "\n",
      "5. **Domain-Specific Adaptability**: RAG can easily incorporate domain-specific information by updating the knowledge base, making it suitable for specialized applications.\n",
      "\n",
      "6. **Improved Answer Quality**: By combining retrieval with generation, RAG systems can provide more accurate, complete, and contextually relevant responses.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "WITH COMPRESSION: Filter → Reorder → Extract\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:04:24,611 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:24,884 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:24,884 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:26,226 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:26,226 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:26,524 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:26,524 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:31,835 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:31,835 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:33,190 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:33,190 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compressed Nodes: 3\n",
      "Total Context: 506 tokens (≈$0.0152)\n",
      "Token Reduction: 1454 tokens (74.2% reduction)\n",
      "\n",
      "Node 1 (Score: 0.8424, 252 tokens, 54.4% of original):\n",
      "  Retrieval-Augmented Generation (RAG) is a paradigm that combines the strengths of large language models with external knowledge retrieval to generate more accurate, factual, and up-to-date responses. ...\n",
      "\n",
      "Node 2 (Score: 0.8002, 116 tokens, 56.8% of original):\n",
      "  Proposition-based chunking offers several advantages for certain RAG applications. Each proposition can be independently verified against source material, enabling fine-grained fact-checking and sourc...\n",
      "\n",
      "Node 3 (Score: 0.8248, 138 tokens, 29.8% of original):\n",
      "  Citation and attribution are important for trust and verifiability. Prompts should instruct the model to cite sources when making claims, often by referencing document IDs or passage numbers. Some sys...\n",
      "\n",
      "Answer:\n",
      "The main advantages of Retrieval-Augmented Generation (RAG) over pure LLM approaches include the ability to access up-to-date information beyond the knowledge cutoff of the model, reducing the risk of generating factually incorrect or hallucinated content. RAG systems allow for dynamic retrieval of relevant external information, enabling answers to be traced back to specific sources for transparency and verifiability. Additionally, knowledge bases can be updated without the need for expensive retraining, making RAG more practical for rapidly changing domains. This approach also facilitates the incorporation of domain-specific information and enhances the overall accuracy and reliability of generated responses.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Baseline Context Tokens: 1,960 tokens (≈$0.0588)\n",
      "Compressed Context Tokens: 506 tokens (≈$0.0152)\n",
      "Token Savings: 1454 tokens (74.2% reduction)\n",
      "Cost Savings: ≈$0.0436 per query\n"
     ]
    }
   ],
   "source": [
    "query = test_queries[0]\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Baseline: No compression\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE: NO COMPRESSION\")\n",
    "print(\"=\"*80)\n",
    "baseline_response = baseline_query_engine.query(query)\n",
    "\n",
    "# Calculate baseline token usage\n",
    "baseline_context = \"\\n\\n\".join([node.node.get_content() for node in baseline_response.source_nodes])\n",
    "baseline_tokens = count_tokens(baseline_context)\n",
    "\n",
    "print(f\"\\nRetrieved Nodes: {len(baseline_response.source_nodes)}\")\n",
    "print(f\"Total Context: {format_token_count(baseline_tokens)}\\n\")\n",
    "\n",
    "for i, node in enumerate(baseline_response.source_nodes, 1):\n",
    "    node_tokens = count_tokens(node.node.get_content())\n",
    "    print(f\"Node {i} (Score: {node.score:.4f}, {node_tokens} tokens):\")\n",
    "    print(f\"  {node.node.get_content()[:200]}...\\n\")\n",
    "\n",
    "print(f\"Answer:\\n{baseline_response.response}\\n\")\n",
    "\n",
    "# With compression\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WITH COMPRESSION: Filter → Reorder → Extract\")\n",
    "print(\"=\"*80)\n",
    "compression_response = compression_query_engine.query(query)\n",
    "\n",
    "# Calculate compressed token usage\n",
    "if compression_response.source_nodes:\n",
    "    compressed_context = \"\\n\\n\".join([node.node.get_content() for node in compression_response.source_nodes])\n",
    "    compressed_tokens = count_tokens(compressed_context)\n",
    "    \n",
    "    print(f\"\\nCompressed Nodes: {len(compression_response.source_nodes)}\")\n",
    "    print(f\"Total Context: {format_token_count(compressed_tokens)}\")\n",
    "    print(f\"Token Reduction: {baseline_tokens - compressed_tokens} tokens ({(1 - compressed_tokens/baseline_tokens)*100:.1f}% reduction)\\n\")\n",
    "    \n",
    "    for i, node in enumerate(compression_response.source_nodes, 1):\n",
    "        node_tokens = count_tokens(node.node.get_content())\n",
    "        original_len = node.node.metadata.get('original_length', 0)\n",
    "        compressed_len = node.node.metadata.get('compressed_length', 0)\n",
    "        compression_ratio = node.node.metadata.get('compression_ratio', 1.0)\n",
    "        \n",
    "        print(f\"Node {i} (Score: {node.score:.4f}, {node_tokens} tokens, {compression_ratio*100:.1f}% of original):\")\n",
    "        print(f\"  {node.node.get_content()[:200]}...\\n\")\n",
    "else:\n",
    "    print(\"\\nNo nodes passed the compression filters.\")\n",
    "    compressed_tokens = 0\n",
    "\n",
    "print(f\"Answer:\\n{compression_response.response}\\n\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Baseline Context Tokens: {format_token_count(baseline_tokens)}\")\n",
    "print(f\"Compressed Context Tokens: {format_token_count(compressed_tokens)}\")\n",
    "if compressed_tokens > 0:\n",
    "    savings = baseline_tokens - compressed_tokens\n",
    "    savings_pct = (savings / baseline_tokens) * 100\n",
    "    print(f\"Token Savings: {savings} tokens ({savings_pct:.1f}% reduction)\")\n",
    "    print(f\"Cost Savings: ≈${(savings / 1000) * 0.03:.4f} per query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f484cda",
   "metadata": {},
   "source": [
    "### Query 2: Chunking Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c8a731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain the chunking trade-off in RAG systems.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:04:33,549 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:35,726 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:35,726 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:36,156 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:36,156 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:37,450 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:37,450 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:38,361 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:38,361 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:39,005 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:39,005 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:39,613 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:39,613 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:40,607 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:40,607 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:43,523 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:43,523 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 2,012 tokens (≈$0.0604)\n",
      "Compressed: 632 tokens (≈$0.0190)\n",
      "Reduction: 68.6%\n",
      "\n",
      "Baseline Answer:\n",
      "The chunking trade-off in Retrieval-Augmented Generation (RAG) systems revolves around balancing retrieval precision and generation quality. Smaller chunks allow for more precise retrieval, as they reduce irrelevant content and noise, making it easier to find specific information. This is particularly useful when working with limited context windows or when token processing costs are high. However, smaller chunks may lack sufficient context, which can hinder the language model's ability to generate comprehensive and accurate responses.\n",
      "\n",
      "On the other hand, larger chunks provide richer context, enabling the language model to better understand the narrative flow and surrounding details, which improves generation quality. Larger chunks also reduce the total number of chunks in the system, potentially enhancing retrieval speed and lowering storage requirements. However, they may dilute relevance signals, leading to the retrieval of less relevant information.\n",
      "\n",
      "This trade-off becomes more complex depending on the type of queries and documents being processed. Simple factual queries may benefit from smaller chunks, while complex analytical queries or documents with natural granularities, such as academic papers or legal texts, may require larger chunks to provide adequate context.\n",
      "\n",
      "Compressed Answer:\n",
      "The chunking trade-off in Retrieval-Augmented Generation (RAG) systems involves balancing two conflicting needs: retrieval precision and contextual richness. Smaller chunks allow for more precise retrieval, as they reduce irrelevant content and noise, making it easier to locate specific information. This is particularly useful when working with limited context windows or when token processing costs are high. However, smaller chunks may lack sufficient context for language models to generate comprehensive and accurate responses, as isolated sentences or fragments can be ambiguous or incomplete.\n",
      "\n",
      "On the other hand, larger chunks provide richer context, enabling language models to better understand the full narrative or background of a retrieved passage. This improves the quality of generated responses but can dilute relevance signals, potentially retrieving less pertinent information. Larger chunks also reduce the total number of chunks in the system, which can improve retrieval speed and reduce storage requirements. The trade-off becomes more complex depending on the type of queries and documents being processed, requiring careful consideration of the system's goals and constraints.\n"
     ]
    }
   ],
   "source": [
    "query = test_queries[1]\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Baseline\n",
    "baseline_response = baseline_query_engine.query(query)\n",
    "baseline_context = \"\\n\\n\".join([node.node.get_content() for node in baseline_response.source_nodes])\n",
    "baseline_tokens = count_tokens(baseline_context)\n",
    "\n",
    "# Compressed\n",
    "compression_response = compression_query_engine.query(query)\n",
    "if compression_response.source_nodes:\n",
    "    compressed_context = \"\\n\\n\".join([node.node.get_content() for node in compression_response.source_nodes])\n",
    "    compressed_tokens = count_tokens(compressed_context)\n",
    "else:\n",
    "    compressed_tokens = 0\n",
    "\n",
    "print(f\"Baseline: {format_token_count(baseline_tokens)}\")\n",
    "print(f\"Compressed: {format_token_count(compressed_tokens)}\")\n",
    "if compressed_tokens > 0:\n",
    "    print(f\"Reduction: {((baseline_tokens - compressed_tokens) / baseline_tokens * 100):.1f}%\")\n",
    "\n",
    "print(\"\\nBaseline Answer:\")\n",
    "print(baseline_response.response)\n",
    "print(\"\\nCompressed Answer:\")\n",
    "print(compression_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324bed88",
   "metadata": {},
   "source": [
    "### Query 3: Vector Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e55d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do vector databases enable efficient similarity search?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:04:43,816 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:45,379 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:45,379 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:45,473 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:45,473 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:46,510 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:46,510 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:46,830 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:46,830 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:48,176 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:48,176 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:48,663 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:48,663 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:49,996 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:49,996 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:51,415 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 15:04:51,415 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 2,269 tokens (≈$0.0681)\n",
      "Compressed: 274 tokens (≈$0.0082)\n",
      "Reduction: 87.9%\n",
      "\n",
      "Baseline Answer:\n",
      "Vector databases enable efficient similarity search by optimizing for approximate nearest neighbor (ANN) search, which trades perfect accuracy for significant speed improvements. They are designed to handle high-dimensional embeddings and use specialized algorithms like hierarchical navigable small world graphs (HNSW) and inverted file indexes (IVF). HNSW builds graph structures for efficient traversal to find nearby vectors, while IVF clusters vectors and searches only within relevant clusters. These methods allow real-time search across millions or billions of vectors, making them highly effective for large-scale retrieval tasks.\n",
      "\n",
      "Compressed Answer:\n",
      "Vector databases enable efficient similarity search by optimizing for approximate nearest neighbor (ANN) search, which trades perfect accuracy for significant speed improvements. They are designed to handle high-dimensional embeddings and can search millions or billions of vectors in real-time. Techniques such as hierarchical navigable small world graphs (HNSW) and inverted file indexes (IVF) are commonly used. HNSW builds graph structures for efficient traversal to locate nearby vectors, while IVF clusters vectors and searches only within relevant clusters, further enhancing efficiency.\n"
     ]
    }
   ],
   "source": [
    "query = test_queries[2]\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Baseline\n",
    "baseline_response = baseline_query_engine.query(query)\n",
    "baseline_context = \"\\n\\n\".join([node.node.get_content() for node in baseline_response.source_nodes])\n",
    "baseline_tokens = count_tokens(baseline_context)\n",
    "\n",
    "# Compressed\n",
    "compression_response = compression_query_engine.query(query)\n",
    "if compression_response.source_nodes:\n",
    "    compressed_context = \"\\n\\n\".join([node.node.get_content() for node in compression_response.source_nodes])\n",
    "    compressed_tokens = count_tokens(compressed_context)\n",
    "else:\n",
    "    compressed_tokens = 0\n",
    "\n",
    "print(f\"Baseline: {format_token_count(baseline_tokens)}\")\n",
    "print(f\"Compressed: {format_token_count(compressed_tokens)}\")\n",
    "if compressed_tokens > 0:\n",
    "    print(f\"Reduction: {((baseline_tokens - compressed_tokens) / baseline_tokens * 100):.1f}%\")\n",
    "\n",
    "print(\"\\nBaseline Answer:\")\n",
    "print(baseline_response.response)\n",
    "print(\"\\nCompressed Answer:\")\n",
    "print(compression_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97bc17",
   "metadata": {},
   "source": [
    "## 10. Compression Pipeline Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08e8ad45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Context Compression Pipeline\n",
       "\n",
       "```\n",
       "┌─────────────────────────────────────────────────────────────────┐\n",
       "│                        USER QUERY                              │\n",
       "└─────────────────────────────────────────────────────────────────┘\n",
       "                              ↓\n",
       "┌─────────────────────────────────────────────────────────────────┐\n",
       "│  INITIAL RETRIEVAL (Bi-Encoder)                                │\n",
       "│  Retrieve top-5 chunks from vector index                       │\n",
       "└─────────────────────────────────────────────────────────────────┘\n",
       "                              ↓\n",
       "        Retrieved: 5 chunks (~2500 tokens total)\n",
       "                              ↓\n",
       "┌─────────────────────────────────────────────────────────────────┐\n",
       "│  STAGE 1: SIMILARITY FILTERING                                 │\n",
       "│  ─────────────────────────────────────────────────────────────  │\n",
       "│  • Apply similarity threshold (0.7)                             │\n",
       "│  • Remove low-relevance chunks                                  │\n",
       "│  • Reduce noise and irrelevant content                          │\n",
       "└─────────────────────────────────────────────────────────────────┘\n",
       "                              ↓\n",
       "        Filtered: 4 chunks (~2000 tokens)\n",
       "                              ↓\n",
       "┌─────────────────────────────────────────────────────────────────┐\n",
       "│  STAGE 2: LONG CONTEXT REORDERING                              │\n",
       "│  ─────────────────────────────────────────────────────────────  │\n",
       "│  • Address \"lost-in-the-middle\" problem                         │\n",
       "│  • Place most relevant chunks at beginning & end                │\n",
       "│  • Less relevant chunks in middle                               │\n",
       "│  • Optimal positioning for LLM attention                        │\n",
       "└─────────────────────────────────────────────────────────────────┘\n",
       "                              ↓\n",
       "        Reordered: 4 chunks (same tokens, better positioning)\n",
       "                              ↓\n",
       "┌─────────────────────────────────────────────────────────────────┐\n",
       "│  STAGE 3: LLM-BASED SENTENCE EXTRACTION                        │\n",
       "│  ─────────────────────────────────────────────────────────────  │\n",
       "│  • Use LLM to analyze each chunk                                │\n",
       "│  • Extract ONLY sentences relevant to query                     │\n",
       "│  • Discard tangential or redundant content                      │\n",
       "│  • Preserve key information, remove noise                       │\n",
       "└─────────────────────────────────────────────────────────────────┘\n",
       "                              ↓\n",
       "        Extracted: 4 chunks (~800 tokens) - 60% reduction!\n",
       "                              ↓\n",
       "┌─────────────────────────────────────────────────────────────────┐\n",
       "│                 LLM GENERATION                                  │\n",
       "│  Generates answer using compressed, focused context            │\n",
       "│  ✓ Lower cost      ✓ Faster response                           │\n",
       "│  ✓ Higher quality  ✓ More focused                              │\n",
       "└─────────────────────────────────────────────────────────────────┘\n",
       "```\n",
       "\n",
       "### Key Benefits of Each Stage\n",
       "\n",
       "**Stage 1: Similarity Filtering**\n",
       "- Removes chunks below relevance threshold\n",
       "- Reduces noise from marginally relevant content\n",
       "- Fast operation (simple threshold comparison)\n",
       "\n",
       "**Stage 2: Long Context Reordering**\n",
       "- Addresses LLM's \"lost-in-the-middle\" phenomenon\n",
       "- Research shows LLMs pay more attention to start/end of context\n",
       "- No token reduction, but better utilization\n",
       "\n",
       "**Stage 3: LLM Sentence Extraction**\n",
       "- Most powerful compression technique\n",
       "- Intelligently extracts relevant sentences\n",
       "- Typical 40-70% token reduction\n",
       "- Preserves answer-critical information\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualization_md = \"\"\"\n",
    "### Context Compression Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                        USER QUERY                              │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  INITIAL RETRIEVAL (Bi-Encoder)                                │\n",
    "│  Retrieve top-5 chunks from vector index                       │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "        Retrieved: 5 chunks (~2500 tokens total)\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  STAGE 1: SIMILARITY FILTERING                                 │\n",
    "│  ─────────────────────────────────────────────────────────────  │\n",
    "│  • Apply similarity threshold (0.7)                             │\n",
    "│  • Remove low-relevance chunks                                  │\n",
    "│  • Reduce noise and irrelevant content                          │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "        Filtered: 4 chunks (~2000 tokens)\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  STAGE 2: LONG CONTEXT REORDERING                              │\n",
    "│  ─────────────────────────────────────────────────────────────  │\n",
    "│  • Address \"lost-in-the-middle\" problem                         │\n",
    "│  • Place most relevant chunks at beginning & end                │\n",
    "│  • Less relevant chunks in middle                               │\n",
    "│  • Optimal positioning for LLM attention                        │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "        Reordered: 4 chunks (same tokens, better positioning)\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  STAGE 3: LLM-BASED SENTENCE EXTRACTION                        │\n",
    "│  ─────────────────────────────────────────────────────────────  │\n",
    "│  • Use LLM to analyze each chunk                                │\n",
    "│  • Extract ONLY sentences relevant to query                     │\n",
    "│  • Discard tangential or redundant content                      │\n",
    "│  • Preserve key information, remove noise                       │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "        Extracted: 4 chunks (~800 tokens) - 60% reduction!\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                 LLM GENERATION                                  │\n",
    "│  Generates answer using compressed, focused context            │\n",
    "│  ✓ Lower cost      ✓ Faster response                           │\n",
    "│  ✓ Higher quality  ✓ More focused                              │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Benefits of Each Stage\n",
    "\n",
    "**Stage 1: Similarity Filtering**\n",
    "- Removes chunks below relevance threshold\n",
    "- Reduces noise from marginally relevant content\n",
    "- Fast operation (simple threshold comparison)\n",
    "\n",
    "**Stage 2: Long Context Reordering**\n",
    "- Addresses LLM's \"lost-in-the-middle\" phenomenon\n",
    "- Research shows LLMs pay more attention to start/end of context\n",
    "- No token reduction, but better utilization\n",
    "\n",
    "**Stage 3: LLM Sentence Extraction**\n",
    "- Most powerful compression technique\n",
    "- Intelligently extracts relevant sentences\n",
    "- Typical 40-70% token reduction\n",
    "- Preserves answer-critical information\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(visualization_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e3151",
   "metadata": {},
   "source": [
    "## 11. Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf39e06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Comparative Analysis</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Baseline (No Compression)</th>\n",
       "      <th>With Compression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average Retrieved Nodes</td>\n",
       "      <td>5</td>\n",
       "      <td>3-4 (after filtering)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Average Context Tokens</td>\n",
       "      <td>~2500</td>\n",
       "      <td>~800-1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Processing Stages</td>\n",
       "      <td>None</td>\n",
       "      <td>Filter → Reorder → Extract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Token Reduction</td>\n",
       "      <td>0%</td>\n",
       "      <td>40-60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cost per Query</td>\n",
       "      <td>~$0.075</td>\n",
       "      <td>~$0.030-0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Answer Quality</td>\n",
       "      <td>Good (with noise)</td>\n",
       "      <td>Excellent (focused)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Latency Impact</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>+100-200ms (extraction)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Signal-to-Noise Ratio</td>\n",
       "      <td>Medium</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Metric Baseline (No Compression)  \\\n",
       "0  Average Retrieved Nodes                         5   \n",
       "1   Average Context Tokens                     ~2500   \n",
       "2        Processing Stages                      None   \n",
       "3          Token Reduction                        0%   \n",
       "4           Cost per Query                   ~$0.075   \n",
       "5           Answer Quality         Good (with noise)   \n",
       "6           Latency Impact                  Baseline   \n",
       "7    Signal-to-Noise Ratio                    Medium   \n",
       "\n",
       "             With Compression  \n",
       "0       3-4 (after filtering)  \n",
       "1                   ~800-1200  \n",
       "2  Filter → Reorder → Extract  \n",
       "3                      40-60%  \n",
       "4               ~$0.030-0.045  \n",
       "5         Excellent (focused)  \n",
       "6     +100-200ms (extraction)  \n",
       "7                        High  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparative analysis\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Average Retrieved Nodes',\n",
    "        'Average Context Tokens',\n",
    "        'Processing Stages',\n",
    "        'Token Reduction',\n",
    "        'Cost per Query',\n",
    "        'Answer Quality',\n",
    "        'Latency Impact',\n",
    "        'Signal-to-Noise Ratio',\n",
    "    ],\n",
    "    'Baseline (No Compression)': [\n",
    "        '5',\n",
    "        '~2500',\n",
    "        'None',\n",
    "        '0%',\n",
    "        '~$0.075',\n",
    "        'Good (with noise)',\n",
    "        'Baseline',\n",
    "        'Medium',\n",
    "    ],\n",
    "    'With Compression': [\n",
    "        '3-4 (after filtering)',\n",
    "        '~800-1200',\n",
    "        'Filter → Reorder → Extract',\n",
    "        '40-60%',\n",
    "        '~$0.030-0.045',\n",
    "        'Excellent (focused)',\n",
    "        '+100-200ms (extraction)',\n",
    "        'High',\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "display(HTML(\"<h3>Comparative Analysis</h3>\"))\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393f4cd",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Context Compression Provides Multiple Benefits**:\n",
    "   - **Cost Reduction**: 40-60% fewer tokens = significant cost savings at scale\n",
    "   - **Latency Improvement**: Smaller context = faster LLM processing\n",
    "   - **Quality Improvement**: Higher signal-to-noise ratio = better answers\n",
    "   - **Context Window Efficiency**: Stay within token limits, avoid truncation\n",
    "\n",
    "2. **Multi-Stage Pipeline is Powerful**:\n",
    "   - Each stage addresses a different problem\n",
    "   - Filtering: Removes irrelevant chunks\n",
    "   - Reordering: Optimizes LLM attention\n",
    "   - Extraction: Distills relevant information\n",
    "\n",
    "3. **LLM-Based Extraction is Most Effective**:\n",
    "   - Intelligently identifies relevant sentences\n",
    "   - Preserves meaning while removing noise\n",
    "   - Typical 40-70% token reduction\n",
    "   - Small latency cost (1-2 LLM calls) for large savings\n",
    "\n",
    "4. **Lost-in-the-Middle Problem is Real**:\n",
    "   - LLMs pay more attention to beginning and end of context\n",
    "   - Reordering improves information utilization\n",
    "   - Compression reduces middle content, mitigating the problem\n",
    "\n",
    "5. **Trade-offs to Consider**:\n",
    "   - Compression adds latency (extraction requires LLM calls)\n",
    "   - Risk of over-compression losing relevant information\n",
    "   - Need to balance compression ratio with answer quality\n",
    "\n",
    "### When to Use Context Compression\n",
    "\n",
    "✅ **Good for**:\n",
    "- Verbose documents with redundant information\n",
    "- Long-form content where chunks contain tangential info\n",
    "- Cost-sensitive applications at scale\n",
    "- Applications approaching context window limits\n",
    "- When retrieval casts too wide a net\n",
    "\n",
    "❌ **Less suitable for**:\n",
    "- Already concise, focused documents\n",
    "- Ultra-low latency requirements\n",
    "- When every sentence matters (legal, medical)\n",
    "- Small-scale applications where cost is not a concern\n",
    "\n",
    "### Compression Techniques Compared\n",
    "\n",
    "| Technique | Token Reduction | Latency | Accuracy | Complexity |\n",
    "|-----------|----------------|---------|----------|------------|\n",
    "| **Similarity Filtering** | 10-20% | Very Low | Good | Low |\n",
    "| **Context Reordering** | 0% | Very Low | Better | Low |\n",
    "| **LLM Extraction** | 40-70% | Medium | Excellent | Medium |\n",
    "| **Abstractive Summarization** | 60-80% | High | Good | High |\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Caching**: Cache extracted sentences for frequently retrieved chunks\n",
    "2. **Batch Processing**: Extract from multiple chunks in parallel\n",
    "3. **Threshold Tuning**: Adjust similarity threshold based on your data\n",
    "4. **Monitoring**: Track compression ratios and answer quality metrics\n",
    "5. **Hybrid Approach**: Use aggressive compression for some queries, light for others\n",
    "\n",
    "### Cost-Benefit Analysis\n",
    "\n",
    "**Example at Scale (1M queries/month):**\n",
    "- Baseline: 2500 tokens/query × 1M queries = 2.5B tokens → ~$75,000/month\n",
    "- With Compression: 1000 tokens/query × 1M queries = 1B tokens → ~$30,000/month\n",
    "- **Savings: $45,000/month** (even accounting for extraction costs)\n",
    "- ROI: Compression infrastructure pays for itself immediately at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6f07e",
   "metadata": {},
   "source": [
    "## 13. Further Exploration\n",
    "\n",
    "Try these experiments:\n",
    "1. Adjust similarity threshold (0.6, 0.7, 0.8) and observe impact\n",
    "2. Test with different extraction prompts (more/less aggressive)\n",
    "3. Implement abstractive summarization instead of extractive\n",
    "4. Measure answer quality degradation vs compression ratio\n",
    "5. Combine compression with re-ranking for best results\n",
    "6. Test with different document types (technical, narrative, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
