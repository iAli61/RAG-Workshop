{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17541a5",
   "metadata": {},
   "source": [
    "# Demo #4: Hierarchical Retrieval with Sentence Window\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo demonstrates **Hierarchical Retrieval** using the **Sentence Window** technique, which addresses a fundamental challenge in RAG systems: the trade-off between retrieval precision and context sufficiency.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Traditional RAG systems face a dilemma:\n",
    "- **Small chunks**: Provide precise retrieval (easier to match user queries) but lack sufficient context for the LLM to generate comprehensive answers\n",
    "- **Large chunks**: Provide rich context but dilute relevance signals, causing less relevant information to be retrieved\n",
    "\n",
    "### The Solution: Sentence Window Retrieval\n",
    "\n",
    "Sentence Window Retrieval solves this by **separating retrieval granularity from generation context**:\n",
    "1. **Index small units**: Embed and index individual sentences or small text segments for precise retrieval\n",
    "2. **Return expanded context**: When a sentence is retrieved, provide the surrounding \"window\" of sentences to the LLM\n",
    "\n",
    "This gives us the best of both worlds: precise retrieval matching with sufficient context for generation.\n",
    "\n",
    "### Core Concepts Demonstrated\n",
    "- Hierarchical retrieval strategies\n",
    "- Sentence Window Retrieval\n",
    "- Separation of retrieval granularity from generation context\n",
    "- Lost-in-the-middle problem mitigation\n",
    "\n",
    "### References\n",
    "- Advanced Retrieval-Augmented Generation: From Theory to LlamaIndex Implementation (Reference 37)\n",
    "- Develop a RAG Solution - Chunking Phase - Azure Architecture (Reference 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb55fb",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64281b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceWindowNodeParser,\n",
    "    SentenceSplitter,\n",
    ")\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "# Visualization\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc7967",
   "metadata": {},
   "source": [
    "## 2. Azure OpenAI Configuration\n",
    "\n",
    "Configure Azure OpenAI for both LLM and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04312a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Azure OpenAI configured successfully\n",
      "  LLM Deployment: gpt-4\n",
      "  Embedding Deployment: text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "# Azure OpenAI configuration from environment variables\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-ada-002\")\n",
    "\n",
    "# Validate configuration\n",
    "if not all([AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT]):\n",
    "    raise ValueError(\n",
    "        \"Missing Azure OpenAI configuration. Please set:\\n\"\n",
    "        \"- AZURE_OPENAI_API_KEY\\n\"\n",
    "        \"- AZURE_OPENAI_ENDPOINT\\n\"\n",
    "        \"- AZURE_OPENAI_DEPLOYMENT (optional, default: gpt-4)\\n\"\n",
    "        \"- AZURE_OPENAI_EMBEDDING_DEPLOYMENT (optional, default: text-embedding-ada-002)\"\n",
    "    )\n",
    "\n",
    "# Initialize Azure OpenAI LLM\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI Embeddings\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=AZURE_OPENAI_EMBEDDING_DEPLOYMENT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    ")\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"✓ Azure OpenAI configured successfully\")\n",
    "print(f\"  LLM Deployment: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "print(f\"  Embedding Deployment: {AZURE_OPENAI_EMBEDDING_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e0bd2",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Load long-form documents from `data/long_form_docs/`. These documents are 1000+ words and ideal for demonstrating the benefits of sentence window retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "669028c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "\n",
      "✓ Loaded 3 documents\n",
      "  1. advanced_chunking_strategies.md (13411 chars)\n",
      "  2. embedding_models_deep_dive.md (13849 chars)\n",
      "  3. rag_comprehensive_guide.md (16903 chars)\n"
     ]
    }
   ],
   "source": [
    "# Define data directory\n",
    "data_dir = Path(\"./data/long_form_docs\")\n",
    "\n",
    "# Load documents\n",
    "print(\"Loading documents...\")\n",
    "documents = SimpleDirectoryReader(str(data_dir)).load_data()\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"  {i}. {Path(doc.metadata.get('file_name', 'unknown')).name} ({len(doc.text)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddce441",
   "metadata": {},
   "source": [
    "## 4. Baseline: Standard Chunking Strategy\n",
    "\n",
    "First, let's create a baseline RAG system with standard fixed-size chunking to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92fe882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 19 baseline chunks\n",
      "\n",
      "Example baseline chunk (first 300 chars):\n",
      "Modern vector databases can search millions of embeddings in milliseconds, making bi-encoders practical for large-scale retrieval.\n",
      "\n",
      "However, bi-encoders have a fundamental limitation: they can't model interactions between query and document text. The query \"capital of France\" and the document \"Paris...\n"
     ]
    }
   ],
   "source": [
    "# Create standard sentence splitter\n",
    "baseline_parser = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Parse documents into nodes\n",
    "baseline_nodes = baseline_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"✓ Created {len(baseline_nodes)} baseline chunks\")\n",
    "print(f\"\\nExample baseline chunk (first 300 chars):\")\n",
    "print(f\"{baseline_nodes[10].text[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c9f3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building baseline vector index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:52:43,894 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:44,236 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:44,236 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Baseline RAG system ready\n"
     ]
    }
   ],
   "source": [
    "# Build baseline index\n",
    "print(\"Building baseline vector index...\")\n",
    "baseline_index = VectorStoreIndex(baseline_nodes)\n",
    "\n",
    "# Create baseline query engine\n",
    "baseline_query_engine = baseline_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "print(\"✓ Baseline RAG system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d718dd3",
   "metadata": {},
   "source": [
    "## 5. Advanced: Sentence Window Retrieval\n",
    "\n",
    "Now let's implement the Sentence Window approach:\n",
    "1. Parse documents into individual sentences\n",
    "2. Store metadata about surrounding sentences (the \"window\")\n",
    "3. Retrieve sentences precisely, then expand to include window context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d151feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 344 sentence nodes with window metadata\n",
      "\n",
      "Example sentence node:\n",
      "Original sentence: However, implementation is more complex, processing is slower, and chunk sizes become variable, requiring careful handling downstream.\n",
      "\n",
      "...\n",
      "\n",
      "Window context (includes surrounding sentences): By prompting a language model to identify logical breakpoints in a document, systems can achieve human-like understanding of where natural divisions occur.  The LLM might be asked to segment a long document into sections that each cover a distinct topic, or to identify the minimal units of informati...\n"
     ]
    }
   ],
   "source": [
    "# Create Sentence Window Node Parser\n",
    "# window_size=3 means: retrieve 1 sentence, but include 3 sentences before and 3 after\n",
    "sentence_window_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,  # Number of sentences before and after to include\n",
    "    window_metadata_key=\"window\",  # Metadata key for expanded window\n",
    "    original_text_metadata_key=\"original_sentence\",  # Key for original sentence\n",
    ")\n",
    "\n",
    "# Parse documents into sentence nodes with window metadata\n",
    "sentence_nodes = sentence_window_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"✓ Created {len(sentence_nodes)} sentence nodes with window metadata\")\n",
    "print(f\"\\nExample sentence node:\")\n",
    "example_node = sentence_nodes[50]\n",
    "print(f\"Original sentence: {example_node.text[:200]}...\")\n",
    "print(f\"\\nWindow context (includes surrounding sentences): {example_node.metadata.get('window', '')[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e09545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sentence window vector index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:52:44,558 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:44,670 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:44,670 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:44,803 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:44,803 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:44,930 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:44,930 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,068 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,068 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,192 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,192 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,319 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,319 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,454 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,454 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,590 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:45,590 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:47,445 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:47,445 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:47,644 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:47,644 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:47,788 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:47,788 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:47,905 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:47,905 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,031 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,031 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,154 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,154 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,278 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,278 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,526 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,526 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,648 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,648 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,774 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:48,774 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:49,027 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:49,027 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:49,155 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:49,155 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:49,750 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:49,750 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:49,870 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:49,870 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,004 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,004 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,126 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,126 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,247 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,247 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,362 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,362 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,487 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:50,487 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,402 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,402 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,528 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,528 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,658 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,658 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,786 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,786 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,917 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:51,917 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:52,035 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:52,035 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:52,155 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:52,155 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sentence Window RAG system ready\n",
      "  - Retrieval: Precise sentence-level matching\n",
      "  - Generation: Expanded 7-sentence context window (3 before + 1 target + 3 after)\n"
     ]
    }
   ],
   "source": [
    "# Build sentence window index\n",
    "print(\"Building sentence window vector index...\")\n",
    "sentence_window_index = VectorStoreIndex(sentence_nodes)\n",
    "\n",
    "# Create Metadata Replacement Post-Processor\n",
    "# This replaces the retrieved sentence with its expanded window context before sending to LLM\n",
    "metadata_replacement_postprocessor = MetadataReplacementPostProcessor(\n",
    "    target_metadata_key=\"window\",\n",
    ")\n",
    "\n",
    "# Create sentence window query engine with post-processor\n",
    "sentence_window_query_engine = sentence_window_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\",\n",
    "    node_postprocessors=[metadata_replacement_postprocessor],\n",
    ")\n",
    "\n",
    "print(\"✓ Sentence Window RAG system ready\")\n",
    "print(\"  - Retrieval: Precise sentence-level matching\")\n",
    "print(\"  - Generation: Expanded 7-sentence context window (3 before + 1 target + 3 after)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461405a",
   "metadata": {},
   "source": [
    "## 6. Comparative Evaluation\n",
    "\n",
    "Let's test both systems with queries that benefit from the sentence window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaeaf1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 3 queries...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "    \"What are the main limitations of pure LLM approaches that RAG addresses?\",\n",
    "    \"Explain the trade-off between chunk size and retrieval precision in RAG systems.\",\n",
    "    \"How do embedding models capture semantic meaning in text?\",\n",
    "]\n",
    "\n",
    "print(f\"Testing with {len(test_queries)} queries...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0eb141",
   "metadata": {},
   "source": [
    "### Query 1: LLM Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7af59c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:52:52,487 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the main limitations of pure LLM approaches that RAG addresses?\n",
      "\n",
      "================================================================================\n",
      "BASELINE RETRIEVAL (Standard Chunking)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:52:54,131 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:54,289 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:52:54,289 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "The main limitations of pure LLM approaches that RAG addresses include:\n",
      "\n",
      "1. **Knowledge Cutoff**: LLMs cannot access information published after their training data cutoff, making them unable to provide up-to-date responses.\n",
      "2. **Hallucination**: LLMs may generate plausible-sounding but factually incorrect or fabricated information.\n",
      "3. **Expensive Updates**: Updating an LLM's knowledge requires costly retraining, which is impractical for rapidly changing domains.\n",
      "4. **Lack of Transparency**: LLMs do not provide clear sources for their information, making it difficult to verify or audit their outputs. \n",
      "\n",
      "RAG mitigates these issues by incorporating a retrieval step to dynamically fetch relevant and factual information from external sources before generating responses.\n",
      "\n",
      "\n",
      "Retrieved Chunks (3):\n",
      "\n",
      "--- Chunk 1 (Score: 0.8205) ---\n",
      "# Comprehensive Guide to Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "## Introduction to RAG\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a paradigm that combines the strengths of large language models with external knowledge retrieval to generate more accurate, factual, and up-to-date responses. Unlike standa...\n",
      "\n",
      "--- Chunk 2 (Score: 0.8159) ---\n",
      "Some systems also include few-shot examples demonstrating desired answer formats.\n",
      "\n",
      "Citation and attribution are important for trust and verifiability. Prompts should instruct the model to cite sources when making claims, often by referencing document IDs or passage numbers. Some systems go further, ...\n",
      "\n",
      "--- Chunk 3 (Score: 0.8032) ---\n",
      "Fusion retrieval combines multiple retrieval strategies, such as dense vector search and sparse keyword search (BM25), then merges their results using techniques like Reciprocal Rank Fusion. This hybrid approach leverages the complementary strengths of different retrieval paradigms - dense vectors f...\n",
      "\n",
      "================================================================================\n",
      "SENTENCE WINDOW RETRIEVAL\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:52:55,191 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "The main limitations of pure LLM approaches that RAG addresses are:  \n",
      "1. LLMs have a knowledge cutoff date, meaning they cannot access information published after their training concluded.  \n",
      "2. LLMs may hallucinate or generate plausible-sounding but factually incorrect content, even within their training data.  \n",
      "3. Updating an LLM's knowledge requires expensive retraining, making it impractical for domains that evolve rapidly.  \n",
      "4. LLMs lack transparency about their information sources, making it difficult to verify or audit their outputs.  \n",
      "\n",
      "\n",
      "Retrieved Contexts (3):\n",
      "\n",
      "--- Context Window 1 (Score: 0.8873) ---\n",
      "Original Sentence: The fundamental motivation behind RAG stems from several limitations of pure LLM approaches. ...\n",
      "\n",
      "Expanded Window Context: # Comprehensive Guide to Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "## Introduction to RAG\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a paradigm that combines the strengths of large language models with external knowledge retrieval to generate more accurate, factual, and up-to-date responses.  Unlike standalone LLMs that rely solely on their parametric knowledge acquired during training, RAG systems dyna...\n",
      "\n",
      "--- Context Window 2 (Score: 0.8491) ---\n",
      "Original Sentence: Finally, LLMs lack transparency about their information sources, making it difficult to verify or audit their outputs.\n",
      "\n",
      "...\n",
      "\n",
      "Expanded Window Context: First, LLMs have a knowledge cutoff date - they cannot access information published after their training concluded.  Second, even within their training data, LLMs may hallucinate or confabulate information, generating plausible-sounding but factually incorrect content.  Third, updating an LLM's knowledge requires expensive retraining, making it impractical for rapidly evolving domains.  Finally, L...\n",
      "\n",
      "--- Context Window 3 (Score: 0.8336) ---\n",
      "Original Sentence: Unlike standalone LLMs that rely solely on their parametric knowledge acquired during training, RAG systems dynamically retrieve relevant information from external sources before generating responses....\n",
      "\n",
      "Expanded Window Context: # Comprehensive Guide to Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "## Introduction to RAG\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a paradigm that combines the strengths of large language models with external knowledge retrieval to generate more accurate, factual, and up-to-date responses.  Unlike standalone LLMs that rely solely on their parametric knowledge acquired during training, RAG systems dyna...\n"
     ]
    }
   ],
   "source": [
    "query = test_queries[0]\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Baseline retrieval\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE RETRIEVAL (Standard Chunking)\")\n",
    "print(\"=\"*80)\n",
    "baseline_response = baseline_query_engine.query(query)\n",
    "print(f\"\\nAnswer:\\n{baseline_response.response}\\n\")\n",
    "print(f\"\\nRetrieved Chunks ({len(baseline_response.source_nodes)}):\")\n",
    "for i, node in enumerate(baseline_response.source_nodes, 1):\n",
    "    print(f\"\\n--- Chunk {i} (Score: {node.score:.4f}) ---\")\n",
    "    print(f\"{node.text[:300]}...\")\n",
    "\n",
    "# Sentence window retrieval\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTENCE WINDOW RETRIEVAL\")\n",
    "print(\"=\"*80)\n",
    "window_response = sentence_window_query_engine.query(query)\n",
    "print(f\"\\nAnswer:\\n{window_response.response}\\n\")\n",
    "print(f\"\\nRetrieved Contexts ({len(window_response.source_nodes)}):\")\n",
    "for i, node in enumerate(window_response.source_nodes, 1):\n",
    "    print(f\"\\n--- Context Window {i} (Score: {node.score:.4f}) ---\")\n",
    "    print(f\"Original Sentence: {node.metadata.get('original_sentence', node.text)[:200]}...\")\n",
    "    print(f\"\\nExpanded Window Context: {node.text[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71519036",
   "metadata": {},
   "source": [
    "### Query 2: Chunking Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58228fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:52:55,308 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain the trade-off between chunk size and retrieval precision in RAG systems.\n",
      "\n",
      "================================================================================\n",
      "BASELINE RETRIEVAL\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:52:58,672 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "In Retrieval-Augmented Generation (RAG) systems, the trade-off between chunk size and retrieval precision revolves around balancing the need for precise information retrieval with the need for sufficient context during generation.\n",
      "\n",
      "Smaller chunks enhance retrieval precision by reducing irrelevant content in the results. They are more semantically focused, which minimizes noise and ensures that the retrieved information is highly relevant to the query. This is particularly beneficial when working with limited context windows or when token processing costs are high.\n",
      "\n",
      "However, smaller chunks can lack the broader context needed for effective generation. Language models perform better when they have access to richer surrounding information, which helps them interpret and generate responses more accurately. Larger chunks provide this richer context, offering narrative flow and background details that improve the quality of the generated output. Additionally, larger chunks reduce the total number of chunks in the system, potentially improving retrieval speed and lowering storage requirements.\n",
      "\n",
      "The challenge lies in finding the optimal chunk size that balances these competing objectives, as the ideal size may vary depending on the type of query and the nature of the documents being processed.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SENTENCE WINDOW RETRIEVAL\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:52:58,897 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:53:00,403 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:53:00,403 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "In RAG systems, the trade-off between chunk size and retrieval precision revolves around balancing the granularity of information retrieval and the context provided for generation. Smaller chunks allow for more precise retrieval, as each chunk can be narrowly focused and independently scored for relevance. This makes it easier to locate specific information. However, smaller chunks may lack sufficient context, which can negatively impact the quality of the generated responses. On the other hand, larger chunks provide richer context, which can improve the quality of the generated answers by giving the model more comprehensive information. However, larger chunks may dilute relevance signals, making it harder to retrieve the most pertinent information. This trade-off is a fundamental challenge in designing effective chunking strategies for RAG systems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = test_queries[1]\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Baseline\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE RETRIEVAL\")\n",
    "print(\"=\"*80)\n",
    "baseline_response = baseline_query_engine.query(query)\n",
    "print(f\"\\nAnswer:\\n{baseline_response.response}\\n\")\n",
    "\n",
    "# Sentence Window\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTENCE WINDOW RETRIEVAL\")\n",
    "print(\"=\"*80)\n",
    "window_response = sentence_window_query_engine.query(query)\n",
    "print(f\"\\nAnswer:\\n{window_response.response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd8d5b",
   "metadata": {},
   "source": [
    "### Query 3: Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "846e6f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:53:00,539 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do embedding models capture semantic meaning in text?\n",
      "\n",
      "================================================================================\n",
      "BASELINE RETRIEVAL\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:53:01,945 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:53:02,042 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 14:53:02,042 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "Embedding models capture semantic meaning in text by representing it as dense vectors in a high-dimensional space. These vectors encode abstract semantic features, allowing text with similar meanings to produce similar embeddings. Modern embedding models, such as those based on transformers, process entire sentences or passages, considering word relationships and context. This enables them to capture not only individual word meanings but also compositional semantics, reflecting how words combine to convey meaning. The training process positions semantically related texts close together in the embedding space, optimizing for tasks like semantic similarity and retrieval.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SENTENCE WINDOW RETRIEVAL\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:53:03,353 - INFO - HTTP Request: POST https://aoai-sweden-505.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "Embedding models capture semantic meaning in text by mapping variable-length text sequences to fixed-dimensional dense vectors in a high-dimensional space. These vectors are designed so that semantically similar texts are positioned close to each other in the embedding space, while unrelated texts are placed farther apart. This is achieved through training neural networks to optimize the positioning of texts based on their semantic relationships. The learned representations capture not only individual word meanings but also compositional semantics, reflecting how words combine to create meaning. This enables embedding models to represent abstract semantic features effectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = test_queries[2]\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Baseline\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE RETRIEVAL\")\n",
    "print(\"=\"*80)\n",
    "baseline_response = baseline_query_engine.query(query)\n",
    "print(f\"\\nAnswer:\\n{baseline_response.response}\\n\")\n",
    "\n",
    "# Sentence Window\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTENCE WINDOW RETRIEVAL\")\n",
    "print(\"=\"*80)\n",
    "window_response = sentence_window_query_engine.query(query)\n",
    "print(f\"\\nAnswer:\\n{window_response.response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e076e56",
   "metadata": {},
   "source": [
    "## 7. Quantitative Analysis\n",
    "\n",
    "Let's analyze the key differences between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13f7004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Comparative Analysis</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Baseline (Standard)</th>\n",
       "      <th>Sentence Window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Nodes</td>\n",
       "      <td>19</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avg Retrieval Unit Size (chars)</td>\n",
       "      <td>2493</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avg Context Size (chars)</td>\n",
       "      <td>2493</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Retrieval Precision</td>\n",
       "      <td>Medium (512 chars)</td>\n",
       "      <td>High (sentence-level)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Context Sufficiency</td>\n",
       "      <td>Medium (512 chars)</td>\n",
       "      <td>High (7-sentence window)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Metric Baseline (Standard)  \\\n",
       "0                      Total Nodes                  19   \n",
       "1  Avg Retrieval Unit Size (chars)                2493   \n",
       "2         Avg Context Size (chars)                2493   \n",
       "3              Retrieval Precision  Medium (512 chars)   \n",
       "4              Context Sufficiency  Medium (512 chars)   \n",
       "\n",
       "            Sentence Window  \n",
       "0                       344  \n",
       "1                       128  \n",
       "2                       888  \n",
       "3     High (sentence-level)  \n",
       "4  High (7-sentence window)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze chunk/node characteristics\n",
    "baseline_lengths = [len(node.text) for node in baseline_nodes]\n",
    "sentence_lengths = [len(node.text) for node in sentence_nodes]\n",
    "window_lengths = [len(node.metadata.get('window', node.text)) for node in sentence_nodes]\n",
    "\n",
    "analysis_data = {\n",
    "    'Metric': [\n",
    "        'Total Nodes',\n",
    "        'Avg Retrieval Unit Size (chars)',\n",
    "        'Avg Context Size (chars)',\n",
    "        'Retrieval Precision',\n",
    "        'Context Sufficiency',\n",
    "    ],\n",
    "    'Baseline (Standard)': [\n",
    "        len(baseline_nodes),\n",
    "        f\"{sum(baseline_lengths)/len(baseline_lengths):.0f}\",\n",
    "        f\"{sum(baseline_lengths)/len(baseline_lengths):.0f}\",\n",
    "        'Medium (512 chars)',\n",
    "        'Medium (512 chars)',\n",
    "    ],\n",
    "    'Sentence Window': [\n",
    "        len(sentence_nodes),\n",
    "        f\"{sum(sentence_lengths)/len(sentence_lengths):.0f}\",\n",
    "        f\"{sum(window_lengths)/len(window_lengths):.0f}\",\n",
    "        'High (sentence-level)',\n",
    "        'High (7-sentence window)',\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_analysis = pd.DataFrame(analysis_data)\n",
    "display(HTML(\"<h3>Comparative Analysis</h3>\"))\n",
    "display(df_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f850f1",
   "metadata": {},
   "source": [
    "## 8. Visualization: Retrieval vs Context Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01286c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Sentence Window Architecture\n",
       "\n",
       "```\n",
       "┌─────────────────────────────────────────────────────────────────┐\n",
       "│                    ORIGINAL DOCUMENT                            │\n",
       "│  Sentence 1. Sentence 2. Sentence 3. [TARGET]. Sentence 5.     │\n",
       "│  Sentence 6. Sentence 7. ...                                    │\n",
       "└─────────────────────────────────────────────────────────────────┘\n",
       "                                ↓\n",
       "                    ┌───────────────────────┐\n",
       "                    │  INDEXING PHASE       │\n",
       "                    └───────────────────────┘\n",
       "                                ↓\n",
       "        ┌───────────────────────────────────────────────┐\n",
       "        │  Each sentence embedded INDIVIDUALLY          │\n",
       "        │  + Metadata stores surrounding window         │\n",
       "        └───────────────────────────────────────────────┘\n",
       "                                ↓\n",
       "                    ┌───────────────────────┐\n",
       "                    │  USER QUERY           │\n",
       "                    └───────────────────────┘\n",
       "                                ↓\n",
       "        ┌───────────────────────────────────────────────┐\n",
       "        │  RETRIEVAL: Match on SENTENCE level           │\n",
       "        │  ✓ High precision (small retrieval unit)      │\n",
       "        └───────────────────────────────────────────────┘\n",
       "                                ↓\n",
       "        ┌───────────────────────────────────────────────┐\n",
       "        │  POST-PROCESSING: Expand to WINDOW            │\n",
       "        │  Sentence 1. Sentence 2. Sentence 3.          │\n",
       "        │  [TARGET]. Sentence 5. Sentence 6.            │\n",
       "        │  Sentence 7.                                  │\n",
       "        └───────────────────────────────────────────────┘\n",
       "                                ↓\n",
       "        ┌───────────────────────────────────────────────┐\n",
       "        │  GENERATION: LLM gets EXPANDED context        │\n",
       "        │  ✓ High context sufficiency (7 sentences)     │\n",
       "        └───────────────────────────────────────────────┘\n",
       "```\n",
       "\n",
       "### Key Benefits:\n",
       "1. **Precise Retrieval**: Semantic search operates on small, focused units (sentences)\n",
       "2. **Rich Context**: LLM receives expanded windows with surrounding sentences\n",
       "3. **Lost-in-the-Middle Mitigation**: Focused retrieval reduces noise in context\n",
       "4. **Flexible Window Sizing**: Adjust `window_size` parameter based on task needs\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the concept\n",
    "visualization_md = \"\"\"\n",
    "### Sentence Window Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    ORIGINAL DOCUMENT                            │\n",
    "│  Sentence 1. Sentence 2. Sentence 3. [TARGET]. Sentence 5.     │\n",
    "│  Sentence 6. Sentence 7. ...                                    │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                                ↓\n",
    "                    ┌───────────────────────┐\n",
    "                    │  INDEXING PHASE       │\n",
    "                    └───────────────────────┘\n",
    "                                ↓\n",
    "        ┌───────────────────────────────────────────────┐\n",
    "        │  Each sentence embedded INDIVIDUALLY          │\n",
    "        │  + Metadata stores surrounding window         │\n",
    "        └───────────────────────────────────────────────┘\n",
    "                                ↓\n",
    "                    ┌───────────────────────┐\n",
    "                    │  USER QUERY           │\n",
    "                    └───────────────────────┘\n",
    "                                ↓\n",
    "        ┌───────────────────────────────────────────────┐\n",
    "        │  RETRIEVAL: Match on SENTENCE level           │\n",
    "        │  ✓ High precision (small retrieval unit)      │\n",
    "        └───────────────────────────────────────────────┘\n",
    "                                ↓\n",
    "        ┌───────────────────────────────────────────────┐\n",
    "        │  POST-PROCESSING: Expand to WINDOW            │\n",
    "        │  Sentence 1. Sentence 2. Sentence 3.          │\n",
    "        │  [TARGET]. Sentence 5. Sentence 6.            │\n",
    "        │  Sentence 7.                                  │\n",
    "        └───────────────────────────────────────────────┘\n",
    "                                ↓\n",
    "        ┌───────────────────────────────────────────────┐\n",
    "        │  GENERATION: LLM gets EXPANDED context        │\n",
    "        │  ✓ High context sufficiency (7 sentences)     │\n",
    "        └───────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Benefits:\n",
    "1. **Precise Retrieval**: Semantic search operates on small, focused units (sentences)\n",
    "2. **Rich Context**: LLM receives expanded windows with surrounding sentences\n",
    "3. **Lost-in-the-Middle Mitigation**: Focused retrieval reduces noise in context\n",
    "4. **Flexible Window Sizing**: Adjust `window_size` parameter based on task needs\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(visualization_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250181d",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Hierarchical Retrieval Solves the Chunking Dilemma**: By separating retrieval granularity from generation context, we achieve both precise matching and sufficient context.\n",
    "\n",
    "2. **Sentence Window Technique**:\n",
    "   - Index at sentence level for precise semantic matching\n",
    "   - Retrieve with high confidence scores\n",
    "   - Expand to multi-sentence windows for LLM generation\n",
    "   \n",
    "3. **Configurable Trade-offs**:\n",
    "   - `window_size` parameter controls context expansion\n",
    "   - Smaller windows (1-2): For precise, focused answers\n",
    "   - Larger windows (3-5): For comprehensive, contextual answers\n",
    "\n",
    "4. **Lost-in-the-Middle Problem**: By retrieving focused units and expanding only what's needed, we reduce the \"lost in the middle\" phenomenon where LLMs miss information buried in long contexts.\n",
    "\n",
    "5. **Performance Benefits**:\n",
    "   - Improved retrieval precision (sentence-level matching)\n",
    "   - Better answer quality (sufficient context)\n",
    "   - Reduced noise (focused retrieval units)\n",
    "\n",
    "### When to Use Sentence Window Retrieval\n",
    "\n",
    "✅ **Good for**:\n",
    "- Long-form documents where context matters\n",
    "- Questions requiring nuanced, contextual understanding\n",
    "- Domains where sentences are semantically rich\n",
    "- When retrieval precision is critical\n",
    "\n",
    "❌ **Less suitable for**:\n",
    "- Very short documents\n",
    "- Structured data (tables, lists)\n",
    "- When global document context is required\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Window Size Tuning**: Experiment with different `window_size` values based on your documents and use cases\n",
    "2. **Storage Overhead**: Storing window metadata increases index size\n",
    "3. **Retrieval Latency**: Sentence-level indexing creates more nodes, which may impact search time at scale\n",
    "4. **Hybrid Approaches**: Consider combining with other techniques like re-ranking or query transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f40fb6",
   "metadata": {},
   "source": [
    "## 10. Further Exploration\n",
    "\n",
    "Try these experiments:\n",
    "1. Adjust `window_size` (1, 3, 5, 10) and observe answer quality changes\n",
    "2. Test with different document types (technical docs, narratives, etc.)\n",
    "3. Combine with other techniques from previous demos (HyDE, hybrid search)\n",
    "4. Compare with Auto-Merging Retrieval (another hierarchical approach)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
