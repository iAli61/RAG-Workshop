{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef4acdc",
   "metadata": {},
   "source": [
    "# Demo #8: Self-RAG with Reflection Tokens and Adaptive Retrieval\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "In this demonstration, you will learn:\n",
    "\n",
    "1. **Self-Reflective Generation**: How AI systems can critique and evaluate their own outputs\n",
    "2. **Adaptive Retrieval**: How to decide *when* to retrieve vs. when to rely on parametric knowledge\n",
    "3. **Reflection Tokens**: A framework for multi-dimensional critique (relevance, support, utility)\n",
    "4. **Controllable Generation**: How weighted critique scores enable task-specific behavior\n",
    "\n",
    "## 📚 Theoretical Background\n",
    "\n",
    "### The Evolution of RAG Systems\n",
    "\n",
    "```\n",
    "Traditional RAG:   ALWAYS retrieve → Generate\n",
    "                   (Blind execution)\n",
    "\n",
    "Corrective RAG:    Retrieve → EVALUATE → Route\n",
    "                   (Post-retrieval reflection)\n",
    "\n",
    "Self-RAG:          DECIDE → [Conditionally retrieve] → Generate + CRITIQUE\n",
    "                   (Pre-retrieval AND during-generation reflection)\n",
    "```\n",
    "\n",
    "### Key Innovation: Self-RAG as Meta-Reasoner\n",
    "\n",
    "Self-RAG (Self-Reflective Retrieval-Augmented Generation) represents a paradigm shift from **reactive** to **proactive** intelligence:\n",
    "\n",
    "- **Traditional RAG**: Assumes retrieval is always necessary → Wasted compute, added latency\n",
    "- **Corrective RAG**: Evaluates retrieval quality after the fact → Corrects mistakes but still retrieves unnecessarily\n",
    "- **Self-RAG**: Performs meta-reasoning BEFORE retrieval → \"Do I even need external information?\"\n",
    "\n",
    "From the curriculum:\n",
    "> \"The LLM performs meta-reasoning *about* the task itself, asking questions like: 'Is the information I have sufficient?', 'Do I need to use a different tool?', or 'Should I rephrase my query?'\"\n",
    "\n",
    "### Reflection Token Framework\n",
    "\n",
    "Self-RAG introduces a structured critique system using **reflection tokens**:\n",
    "\n",
    "#### 1. Retrieval Decision Tokens\n",
    "- `[Retrieval]`: External knowledge is required\n",
    "- `[No Retrieval]`: Parametric knowledge is sufficient\n",
    "- `[Continue to Use Evidence]`: Use previously retrieved context\n",
    "\n",
    "#### 2. Relevance Tokens (Post-Retrieval)\n",
    "- `[Relevant]`: Retrieved passage is on-topic and useful\n",
    "- `[Irrelevant]`: Retrieved passage should be discarded\n",
    "\n",
    "#### 3. Support/Grounding Tokens (Post-Generation)\n",
    "- `[Fully supported]`: Generated text is entirely grounded in evidence\n",
    "- `[Partially supported]`: Some claims lack evidence\n",
    "- `[No support / Contradictory]`: Hallucination or contradiction detected\n",
    "\n",
    "#### 4. Utility Tokens (Quality Assessment)\n",
    "- `[Utility:5]`: Excellent, comprehensive answer\n",
    "- `[Utility:4]`: Good, addresses query well\n",
    "- `[Utility:3]`: Adequate but incomplete\n",
    "- `[Utility:2]`: Minimal value\n",
    "- `[Utility:1]`: Poor or unhelpful\n",
    "\n",
    "### Architectural Distinction\n",
    "\n",
    "| Aspect | Traditional RAG | Corrective RAG | Self-RAG |\n",
    "|--------|----------------|----------------|----------|\n",
    "| **Retrieval** | Always | Always | Conditional (adaptive) |\n",
    "| **Evaluation** | None | After retrieval | Before + During + After |\n",
    "| **Decision Point** | None | Post-retrieval | Pre-retrieval + Iterative |\n",
    "| **Self-Awareness** | None | Limited | Full (multi-dimensional critique) |\n",
    "| **Use Case** | General QA | Robust QA with fallback | High-stakes, controllable generation |\n",
    "\n",
    "### Implementation Note\n",
    "\n",
    "⚠️ **Important**: True Self-RAG requires fine-tuning an LLM to predict special reflection tokens.\n",
    "\n",
    "This demo **simulates** Self-RAG behavior using:\n",
    "- **Prompting** to elicit critic decisions from Azure OpenAI\n",
    "- **Structured parsing** to extract reflection token predictions\n",
    "- **Multi-step orchestration** to implement the Self-RAG pipeline\n",
    "\n",
    "For production Self-RAG, see the original implementation: [AkariAsai/self-rag](https://github.com/AkariAsai/self-rag)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d750f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install llama-index-core llama-index-llms-azure-openai llama-index-embeddings-azure-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a630bd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99602b53",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811b4f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Azure OpenAI configured successfully\n",
      "   LLM: gpt-4\n",
      "   Embeddings: text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "# Configure Azure OpenAI LLM\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    temperature=0.0  # Deterministic for critique\n",
    ")\n",
    "\n",
    "# Configure Azure OpenAI Embeddings\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"✅ Azure OpenAI configured successfully\")\n",
    "print(f\"   LLM: {llm.model}\")\n",
    "print(f\"   Embeddings: {embed_model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8832e",
   "metadata": {},
   "source": [
    "## Step 2: Define Reflection Token System\n",
    "\n",
    "Create enums and data structures for all reflection token types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e61f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reflection token system defined\n",
      "   Retrieval Tokens: ['[Retrieval]', '[No Retrieval]', '[Continue to Use Evidence]']\n",
      "   Relevance Tokens: ['[Relevant]', '[Irrelevant]']\n",
      "   Support Tokens: ['[Fully supported]', '[Partially supported]', '[No support / Contradictory]']\n",
      "   Utility Tokens: ['[Utility:5]', '[Utility:4]', '[Utility:3]', '[Utility:2]', '[Utility:1]']\n"
     ]
    }
   ],
   "source": [
    "# Reflection Token Enums\n",
    "\n",
    "class RetrievalDecision(Enum):\n",
    "    \"\"\"Tokens for deciding whether to retrieve\"\"\"\n",
    "    RETRIEVAL = \"[Retrieval]\"\n",
    "    NO_RETRIEVAL = \"[No Retrieval]\"\n",
    "    CONTINUE = \"[Continue to Use Evidence]\"\n",
    "\n",
    "\n",
    "class RelevanceToken(Enum):\n",
    "    \"\"\"Tokens for assessing document relevance\"\"\"\n",
    "    RELEVANT = \"[Relevant]\"\n",
    "    IRRELEVANT = \"[Irrelevant]\"\n",
    "\n",
    "\n",
    "class SupportToken(Enum):\n",
    "    \"\"\"Tokens for assessing generation groundedness\"\"\"\n",
    "    FULLY_SUPPORTED = \"[Fully supported]\"\n",
    "    PARTIALLY_SUPPORTED = \"[Partially supported]\"\n",
    "    NO_SUPPORT = \"[No support / Contradictory]\"\n",
    "\n",
    "\n",
    "class UtilityToken(Enum):\n",
    "    \"\"\"Tokens for assessing answer quality\"\"\"\n",
    "    UTILITY_5 = \"[Utility:5]\"\n",
    "    UTILITY_4 = \"[Utility:4]\"\n",
    "    UTILITY_3 = \"[Utility:3]\"\n",
    "    UTILITY_2 = \"[Utility:2]\"\n",
    "    UTILITY_1 = \"[Utility:1]\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CritiqueResult:\n",
    "    \"\"\"Container for all critique dimensions\"\"\"\n",
    "    retrieval_decision: Optional[RetrievalDecision] = None\n",
    "    relevance: Optional[RelevanceToken] = None\n",
    "    support: Optional[SupportToken] = None\n",
    "    utility: Optional[UtilityToken] = None\n",
    "    reasoning: str = \"\"\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, str]:\n",
    "        return {\n",
    "            \"retrieval_decision\": self.retrieval_decision.value if self.retrieval_decision else None,\n",
    "            \"relevance\": self.relevance.value if self.relevance else None,\n",
    "            \"support\": self.support.value if self.support else None,\n",
    "            \"utility\": self.utility.value if self.utility else None,\n",
    "            \"reasoning\": self.reasoning\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✅ Reflection token system defined\")\n",
    "print(\"   Retrieval Tokens:\", [t.value for t in RetrievalDecision])\n",
    "print(\"   Relevance Tokens:\", [t.value for t in RelevanceToken])\n",
    "print(\"   Support Tokens:\", [t.value for t in SupportToken])\n",
    "print(\"   Utility Tokens:\", [t.value for t in UtilityToken])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fed605",
   "metadata": {},
   "source": [
    "## Step 3: Implement Self-RAG Critic\n",
    "\n",
    "The critic is the heart of Self-RAG. It implements four core critique functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd86fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Self-RAG critic initialized\n",
      "   Critique Functions: should_retrieve(), assess_relevance(), assess_groundedness(), assess_utility()\n"
     ]
    }
   ],
   "source": [
    "class SelfRAGCritic:\n",
    "    \"\"\"\n",
    "    Simulated Self-RAG critic using Azure OpenAI.\n",
    "    In production, this would be a fine-tuned model with special tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def should_retrieve(self, query: str, current_context: str = \"\") -> Tuple[RetrievalDecision, str]:\n",
    "        \"\"\"\n",
    "        Critique function 1: Decide if retrieval is needed.\n",
    "        \n",
    "        Returns:\n",
    "            (RetrievalDecision, reasoning)\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are a retrieval decision expert. Your task is to determine if answering a query requires external information retrieval.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "DECISION CRITERIA:\n",
    "- Predict [Retrieval] if:\n",
    "  * Query asks for specific facts, statistics, or recent events\n",
    "  * Query requires domain-specific knowledge not in general LLM training\n",
    "  * Query asks about specific entities, documents, or technical details\n",
    "\n",
    "- Predict [No Retrieval] if:\n",
    "  * Query asks for general knowledge, definitions, or common concepts\n",
    "  * Query requires reasoning or mathematical operations (no facts needed)\n",
    "  * Query is conversational or asks for opinions\n",
    "\n",
    "Respond ONLY in this format:\n",
    "TOKEN: [Retrieval] OR [No Retrieval]\n",
    "REASONING: [One sentence explaining your decision]\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(prompt)\n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        # Parse response\n",
    "        try:\n",
    "            token_line = [l for l in response_text.split('\\n') if 'TOKEN:' in l][0]\n",
    "            reasoning_line = [l for l in response_text.split('\\n') if 'REASONING:' in l][0]\n",
    "            \n",
    "            reasoning = reasoning_line.split('REASONING:')[1].strip()\n",
    "            \n",
    "            if \"[No Retrieval]\" in token_line or \"No Retrieval\" in token_line:\n",
    "                decision = RetrievalDecision.NO_RETRIEVAL\n",
    "            else:\n",
    "                decision = RetrievalDecision.RETRIEVAL\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to parse retrieval decision: {e}\")\n",
    "            decision = RetrievalDecision.RETRIEVAL  # Safe default\n",
    "            reasoning = \"Failed to parse\"\n",
    "        \n",
    "        return decision, reasoning\n",
    "    \n",
    "    def assess_relevance(self, query: str, passage: str) -> Tuple[RelevanceToken, str]:\n",
    "        \"\"\"\n",
    "        Critique function 2: Assess if retrieved passage is relevant.\n",
    "        \n",
    "        Returns:\n",
    "            (RelevanceToken, reasoning)\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are a relevance assessment expert. Determine if a retrieved passage is relevant to the query.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "PASSAGE: {passage[:500]}...\n",
    "\n",
    "DECISION CRITERIA:\n",
    "- Predict [Relevant] if the passage contains information that directly helps answer the query\n",
    "- Predict [Irrelevant] if the passage is off-topic or does not provide useful information\n",
    "\n",
    "Respond ONLY in this format:\n",
    "TOKEN: [Relevant] OR [Irrelevant]\n",
    "REASONING: [One sentence explaining your decision]\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(prompt)\n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        try:\n",
    "            token_line = [l for l in response_text.split('\\n') if 'TOKEN:' in l][0]\n",
    "            reasoning_line = [l for l in response_text.split('\\n') if 'REASONING:' in l][0]\n",
    "            \n",
    "            reasoning = reasoning_line.split('REASONING:')[1].strip()\n",
    "            \n",
    "            if \"[Irrelevant]\" in token_line or \"Irrelevant\" in token_line:\n",
    "                relevance = RelevanceToken.IRRELEVANT\n",
    "            else:\n",
    "                relevance = RelevanceToken.RELEVANT\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to parse relevance: {e}\")\n",
    "            relevance = RelevanceToken.RELEVANT\n",
    "            reasoning = \"Failed to parse\"\n",
    "        \n",
    "        return relevance, reasoning\n",
    "    \n",
    "    def assess_groundedness(self, generation: str, context: str) -> Tuple[SupportToken, str]:\n",
    "        \"\"\"\n",
    "        Critique function 3: Check if generation is grounded in context.\n",
    "        \n",
    "        Returns:\n",
    "            (SupportToken, reasoning)\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are a factual grounding expert. Assess if a generated answer is supported by the provided context.\n",
    "\n",
    "GENERATED ANSWER: {generation}\n",
    "\n",
    "CONTEXT: {context[:800]}...\n",
    "\n",
    "ASSESSMENT CRITERIA:\n",
    "- [Fully supported]: Every claim in the answer is directly supported by the context\n",
    "- [Partially supported]: Some claims are supported, but others lack evidence or are inferred\n",
    "- [No support / Contradictory]: Answer contains claims not in context or contradicts it\n",
    "\n",
    "Respond ONLY in this format:\n",
    "TOKEN: [Fully supported] OR [Partially supported] OR [No support / Contradictory]\n",
    "REASONING: [One sentence explaining your assessment]\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(prompt)\n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        try:\n",
    "            token_line = [l for l in response_text.split('\\n') if 'TOKEN:' in l][0]\n",
    "            reasoning_line = [l for l in response_text.split('\\n') if 'REASONING:' in l][0]\n",
    "            \n",
    "            reasoning = reasoning_line.split('REASONING:')[1].strip()\n",
    "            \n",
    "            if \"Fully supported\" in token_line:\n",
    "                support = SupportToken.FULLY_SUPPORTED\n",
    "            elif \"Partially supported\" in token_line:\n",
    "                support = SupportToken.PARTIALLY_SUPPORTED\n",
    "            else:\n",
    "                support = SupportToken.NO_SUPPORT\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to parse grounding: {e}\")\n",
    "            support = SupportToken.PARTIALLY_SUPPORTED\n",
    "            reasoning = \"Failed to parse\"\n",
    "        \n",
    "        return support, reasoning\n",
    "    \n",
    "    def assess_utility(self, generation: str, query: str) -> Tuple[int, str]:\n",
    "        \"\"\"\n",
    "        Critique function 4: Score answer utility (1-5).\n",
    "        \n",
    "        Returns:\n",
    "            (utility_score, reasoning)\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are an answer quality expert. Rate the utility of an answer to a query on a 1-5 scale.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "ANSWER: {generation}\n",
    "\n",
    "SCORING CRITERIA:\n",
    "5: Excellent - Comprehensive, accurate, directly addresses all aspects of the query\n",
    "4: Good - Solid answer, addresses query well with minor gaps\n",
    "3: Adequate - Provides some useful information but incomplete\n",
    "2: Minimal - Tangentially related or very incomplete\n",
    "1: Poor - Does not answer query or is unhelpful\n",
    "\n",
    "Respond ONLY in this format:\n",
    "SCORE: [1-5]\n",
    "REASONING: [One sentence explaining your score]\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(prompt)\n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        try:\n",
    "            score_line = [l for l in response_text.split('\\n') if 'SCORE:' in l][0]\n",
    "            reasoning_line = [l for l in response_text.split('\\n') if 'REASONING:' in l][0]\n",
    "            \n",
    "            score = int(score_line.split('SCORE:')[1].strip()[0])  # Extract first digit\n",
    "            reasoning = reasoning_line.split('REASONING:')[1].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to parse utility: {e}\")\n",
    "            score = 3\n",
    "            reasoning = \"Failed to parse\"\n",
    "        \n",
    "        return score, reasoning\n",
    "\n",
    "\n",
    "# Initialize critic\n",
    "critic = SelfRAGCritic(llm)\n",
    "print(\"✅ Self-RAG critic initialized\")\n",
    "print(\"   Critique Functions: should_retrieve(), assess_relevance(), assess_groundedness(), assess_utility()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74789ddd",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation\n",
    "\n",
    "Load diverse documents to test adaptive behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c711c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 documents\n",
      "   1. transformer_architecture.md (4247 chars)\n",
      "   2. bert_model.md (2358 chars)\n",
      "   3. neural_networks.md (1873 chars)\n",
      "   4. random_forests.md (2488 chars)\n",
      "   5. gradient_boosting.md (1612 chars)\n",
      "\n",
      "✅ Vector index built successfully\n",
      "\n",
      "✅ Vector index built successfully\n"
     ]
    }
   ],
   "source": [
    "# Load documents from multiple directories\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\n",
    "        \"data/tech_docs/transformer_architecture.md\",\n",
    "        \"data/tech_docs/bert_model.md\",\n",
    "        \"data/ml_concepts/neural_networks.md\",\n",
    "        \"data/ml_concepts/random_forests.md\",\n",
    "        \"data/ml_concepts/gradient_boosting.md\"\n",
    "    ]\n",
    ").load_data()\n",
    "\n",
    "print(f\"✅ Loaded {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"   {i}. {doc.metadata.get('file_name', 'Unknown')} ({len(doc.text)} chars)\")\n",
    "\n",
    "# Build vector index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "print(\"\\n✅ Vector index built successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8b8a5",
   "metadata": {},
   "source": [
    "## Step 5: Implement Self-RAG Pipeline\n",
    "\n",
    "The complete Self-RAG system with adaptive retrieval and multi-dimensional critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b363b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Self-RAG system initialized\n",
      "   Critique Weights: relevance=1.0, support=2.0, utility=1.0\n"
     ]
    }
   ],
   "source": [
    "class SelfRAGSystem:\n",
    "    \"\"\"\n",
    "    Self-Reflective RAG with adaptive retrieval and multi-dimensional critique.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index, llm, critic, retriever):\n",
    "        self.index = index\n",
    "        self.llm = llm\n",
    "        self.critic = critic\n",
    "        self.retriever = retriever\n",
    "        \n",
    "        # Critique weights (for composite scoring)\n",
    "        self.w_relevance = 1.0\n",
    "        self.w_support = 2.0  # Higher weight for factual grounding\n",
    "        self.w_utility = 1.0\n",
    "    \n",
    "    def query(\n",
    "        self, \n",
    "        query_text: str, \n",
    "        verbose: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute Self-RAG pipeline with adaptive retrieval and critique.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*90}\")\n",
    "            print(f\"🧠 SELF-RAG PIPELINE\")\n",
    "            print(f\"{'='*90}\")\n",
    "            print(f\"Query: {query_text}\\n\")\n",
    "        \n",
    "        trace = []  # Decision trace for transparency\n",
    "        \n",
    "        # STEP 1: Adaptive Retrieval Decision\n",
    "        if verbose:\n",
    "            print(\"[STEP 1] 🔍 Retrieval Decision\")\n",
    "        \n",
    "        retrieval_decision, retrieval_reasoning = self.critic.should_retrieve(query_text)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Token: {retrieval_decision.value}\")\n",
    "            print(f\"   Reasoning: {retrieval_reasoning}\")\n",
    "        \n",
    "        trace.append({\n",
    "            \"step\": \"Retrieval Decision\",\n",
    "            \"token\": retrieval_decision.value,\n",
    "            \"reasoning\": retrieval_reasoning\n",
    "        })\n",
    "        \n",
    "        # STEP 2: Conditional Retrieval + Relevance Filtering\n",
    "        relevant_passages = []\n",
    "        \n",
    "        if retrieval_decision == RetrievalDecision.RETRIEVAL:\n",
    "            if verbose:\n",
    "                print(\"\\n[STEP 2] 📥 Retrieval + Relevance Filtering\")\n",
    "            \n",
    "            retrieved_nodes = self.retriever.retrieve(query_text)\n",
    "            \n",
    "            for i, node in enumerate(retrieved_nodes):\n",
    "                passage = node.node.text\n",
    "                relevance_token, relevance_reasoning = self.critic.assess_relevance(\n",
    "                    query_text, \n",
    "                    passage\n",
    "                )\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"\\n   Document {i+1}:\")\n",
    "                    print(f\"   Token: {relevance_token.value}\")\n",
    "                    print(f\"   Reasoning: {relevance_reasoning}\")\n",
    "                \n",
    "                trace.append({\n",
    "                    \"step\": f\"Relevance Assessment {i+1}\",\n",
    "                    \"token\": relevance_token.value,\n",
    "                    \"reasoning\": relevance_reasoning\n",
    "                })\n",
    "                \n",
    "                if relevance_token == RelevanceToken.RELEVANT:\n",
    "                    relevant_passages.append(passage)\n",
    "            \n",
    "            if verbose and relevant_passages:\n",
    "                print(f\"\\n   ✓ {len(relevant_passages)} relevant passages selected\")\n",
    "            elif verbose:\n",
    "                print(\"\\n   ⚠️  No relevant passages found\")\n",
    "        \n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"\\n[STEP 2] ⏭️  Skipped (No retrieval needed)\")\n",
    "        \n",
    "        # STEP 3: Generation\n",
    "        if verbose:\n",
    "            print(\"\\n[STEP 3] ✍️  Generation\")\n",
    "        \n",
    "        if relevant_passages:\n",
    "            context = \"\\n\\n\".join(relevant_passages)\n",
    "            generation_prompt = f\"\"\"Answer the following query using ONLY the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Query: {query_text}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        else:\n",
    "            generation_prompt = f\"\"\"Answer the following query using your knowledge.\n",
    "\n",
    "Query: {query_text}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(generation_prompt)\n",
    "        answer = response.text.strip()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Generated answer ({len(answer)} chars)\")\n",
    "        \n",
    "        # STEP 4: Groundedness Check\n",
    "        if verbose:\n",
    "            print(\"\\n[STEP 4] 🔬 Groundedness Check\")\n",
    "        \n",
    "        if relevant_passages:\n",
    "            context_for_check = \"\\n\\n\".join(relevant_passages)\n",
    "            support_token, support_reasoning = self.critic.assess_groundedness(\n",
    "                answer, \n",
    "                context_for_check\n",
    "            )\n",
    "        else:\n",
    "            # No context, so can't be fully supported\n",
    "            support_token = SupportToken.NO_SUPPORT\n",
    "            support_reasoning = \"No retrieved context to ground answer\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Token: {support_token.value}\")\n",
    "            print(f\"   Reasoning: {support_reasoning}\")\n",
    "        \n",
    "        trace.append({\n",
    "            \"step\": \"Groundedness Check\",\n",
    "            \"token\": support_token.value,\n",
    "            \"reasoning\": support_reasoning\n",
    "        })\n",
    "        \n",
    "        # STEP 5: Utility Assessment\n",
    "        if verbose:\n",
    "            print(\"\\n[STEP 5] ⭐ Utility Assessment\")\n",
    "        \n",
    "        utility_score, utility_reasoning = self.critic.assess_utility(answer, query_text)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Token: [Utility:{utility_score}]\")\n",
    "            print(f\"   Reasoning: {utility_reasoning}\")\n",
    "        \n",
    "        trace.append({\n",
    "            \"step\": \"Utility Assessment\",\n",
    "            \"token\": f\"[Utility:{utility_score}]\",\n",
    "            \"reasoning\": utility_reasoning\n",
    "        })\n",
    "        \n",
    "        # STEP 6: Composite Scoring\n",
    "        composite_score = self._calculate_composite_score(\n",
    "            len(relevant_passages),\n",
    "            support_token,\n",
    "            utility_score\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"retrieval_decision\": retrieval_decision.value,\n",
    "            \"num_relevant_passages\": len(relevant_passages),\n",
    "            \"support_token\": support_token.value,\n",
    "            \"utility_score\": utility_score,\n",
    "            \"composite_score\": composite_score,\n",
    "            \"trace\": trace\n",
    "        }\n",
    "    \n",
    "    def _calculate_composite_score(\n",
    "        self, \n",
    "        num_relevant: int, \n",
    "        support: SupportToken, \n",
    "        utility: int\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate weighted composite critique score.\n",
    "        \"\"\"\n",
    "        # Relevance score (binary: any relevant passages = 1)\n",
    "        relevance_score = 1.0 if num_relevant > 0 else 0.0\n",
    "        \n",
    "        # Support score (0-1 scale)\n",
    "        support_map = {\n",
    "            SupportToken.FULLY_SUPPORTED: 1.0,\n",
    "            SupportToken.PARTIALLY_SUPPORTED: 0.5,\n",
    "            SupportToken.NO_SUPPORT: 0.0\n",
    "        }\n",
    "        support_score = support_map[support]\n",
    "        \n",
    "        # Utility score (already 1-5, normalize to 0-1)\n",
    "        utility_score = (utility - 1) / 4.0  # Maps 1-5 to 0-1\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_weight = self.w_relevance + self.w_support + self.w_utility\n",
    "        composite = (\n",
    "            self.w_relevance * relevance_score +\n",
    "            self.w_support * support_score +\n",
    "            self.w_utility * utility_score\n",
    "        ) / total_weight\n",
    "        \n",
    "        return round(composite, 3)\n",
    "\n",
    "\n",
    "# Initialize Self-RAG system\n",
    "selfrag = SelfRAGSystem(index, llm, critic, retriever)\n",
    "print(\"✅ Self-RAG system initialized\")\n",
    "print(f\"   Critique Weights: relevance={selfrag.w_relevance}, support={selfrag.w_support}, utility={selfrag.w_utility}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc3bde",
   "metadata": {},
   "source": [
    "## Step 6: Test Suite - Exercising Adaptive Retrieval\n",
    "\n",
    "### Test 1: No Retrieval Needed (Parametric Knowledge)\n",
    "Expected: `[No Retrieval]` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39003cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "🧠 SELF-RAG PIPELINE\n",
      "==========================================================================================\n",
      "Query: What is 2 + 2? Explain your reasoning.\n",
      "\n",
      "[STEP 1] 🔍 Retrieval Decision\n",
      "   Token: [No Retrieval]\n",
      "   Reasoning: The query asks for a simple mathematical operation that can be answered using general knowledge without needing external information.\n",
      "\n",
      "[STEP 2] ⏭️  Skipped (No retrieval needed)\n",
      "\n",
      "[STEP 3] ✍️  Generation\n",
      "   Token: [No Retrieval]\n",
      "   Reasoning: The query asks for a simple mathematical operation that can be answered using general knowledge without needing external information.\n",
      "\n",
      "[STEP 2] ⏭️  Skipped (No retrieval needed)\n",
      "\n",
      "[STEP 3] ✍️  Generation\n",
      "   Generated answer (478 chars)\n",
      "\n",
      "[STEP 4] 🔬 Groundedness Check\n",
      "   Token: [No support / Contradictory]\n",
      "   Reasoning: No retrieved context to ground answer\n",
      "\n",
      "[STEP 5] ⭐ Utility Assessment\n",
      "   Generated answer (478 chars)\n",
      "\n",
      "[STEP 4] 🔬 Groundedness Check\n",
      "   Token: [No support / Contradictory]\n",
      "   Reasoning: No retrieved context to ground answer\n",
      "\n",
      "[STEP 5] ⭐ Utility Assessment\n",
      "   Token: [Utility:5]\n",
      "   Reasoning: The answer is comprehensive, accurate, and directly addresses all aspects of the query by explaining the arithmetic process and providing the correct result.\n",
      "\n",
      "==========================================================================================\n",
      "📊 RESULT SUMMARY\n",
      "==========================================================================================\n",
      "Answer: The query \"What is 2 + 2?\" is asking for the sum of the numbers 2 and 2. In basic arithmetic, addition is the process of combining two or more numbers to get a total. When you add 2 and 2 together, you are essentially combining these two quantities. \n",
      "\n",
      "Here's the reasoning:\n",
      "\n",
      "1. Start with the number 2.\n",
      "2. Add another 2 to it.\n",
      "3. This results in a total of 4.\n",
      "\n",
      "Therefore, 2 + 2 equals 4. This is a fundamental concept in mathematics and is one of the simplest addition problems.\n",
      "\n",
      "Retrieval Decision: [No Retrieval]\n",
      "Support Token: [No support / Contradictory]\n",
      "Utility Score: 5/5\n",
      "Composite Score: 0.25\n",
      "   Token: [Utility:5]\n",
      "   Reasoning: The answer is comprehensive, accurate, and directly addresses all aspects of the query by explaining the arithmetic process and providing the correct result.\n",
      "\n",
      "==========================================================================================\n",
      "📊 RESULT SUMMARY\n",
      "==========================================================================================\n",
      "Answer: The query \"What is 2 + 2?\" is asking for the sum of the numbers 2 and 2. In basic arithmetic, addition is the process of combining two or more numbers to get a total. When you add 2 and 2 together, you are essentially combining these two quantities. \n",
      "\n",
      "Here's the reasoning:\n",
      "\n",
      "1. Start with the number 2.\n",
      "2. Add another 2 to it.\n",
      "3. This results in a total of 4.\n",
      "\n",
      "Therefore, 2 + 2 equals 4. This is a fundamental concept in mathematics and is one of the simplest addition problems.\n",
      "\n",
      "Retrieval Decision: [No Retrieval]\n",
      "Support Token: [No support / Contradictory]\n",
      "Utility Score: 5/5\n",
      "Composite Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Test Query 1: Should NOT retrieve\n",
    "query1 = \"What is 2 + 2? Explain your reasoning.\"\n",
    "\n",
    "result1 = selfrag.query(query1, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"📊 RESULT SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Answer: {result1['answer']}\")\n",
    "print(f\"\\nRetrieval Decision: {result1['retrieval_decision']}\")\n",
    "print(f\"Support Token: {result1['support_token']}\")\n",
    "print(f\"Utility Score: {result1['utility_score']}/5\")\n",
    "print(f\"Composite Score: {result1['composite_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8bf661",
   "metadata": {},
   "source": [
    "### Test 2: Retrieval Required (Specific Technical Knowledge)\n",
    "Expected: `[Retrieval]` → Relevance filtering → Grounded generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6da0a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "🧠 SELF-RAG PIPELINE\n",
      "==========================================================================================\n",
      "Query: What are the key components of a transformer architecture and how do they work?\n",
      "\n",
      "[STEP 1] 🔍 Retrieval Decision\n",
      "   Token: [No Retrieval]\n",
      "   Reasoning: The query asks for general knowledge about the transformer architecture, which is a common concept covered in general LLM training.\n",
      "\n",
      "[STEP 2] ⏭️  Skipped (No retrieval needed)\n",
      "\n",
      "[STEP 3] ✍️  Generation\n",
      "   Token: [No Retrieval]\n",
      "   Reasoning: The query asks for general knowledge about the transformer architecture, which is a common concept covered in general LLM training.\n",
      "\n",
      "[STEP 2] ⏭️  Skipped (No retrieval needed)\n",
      "\n",
      "[STEP 3] ✍️  Generation\n",
      "   Generated answer (2957 chars)\n",
      "\n",
      "[STEP 4] 🔬 Groundedness Check\n",
      "   Token: [No support / Contradictory]\n",
      "   Reasoning: No retrieved context to ground answer\n",
      "\n",
      "[STEP 5] ⭐ Utility Assessment\n",
      "   Generated answer (2957 chars)\n",
      "\n",
      "[STEP 4] 🔬 Groundedness Check\n",
      "   Token: [No support / Contradictory]\n",
      "   Reasoning: No retrieved context to ground answer\n",
      "\n",
      "[STEP 5] ⭐ Utility Assessment\n",
      "   Token: [Utility:5]\n",
      "   Reasoning: The answer is comprehensive, accurately detailing each key component of the transformer architecture and explaining their functions, directly addressing all aspects of the query.\n",
      "\n",
      "==========================================================================================\n",
      "📊 RESULT SUMMARY\n",
      "==========================================================================================\n",
      "Answer: The transformer architecture, introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017, is a foundational model for many modern natural language processing tasks. It consists of several key components:\n",
      "\n",
      "1. **Self-Attention Mechanism**: \n",
      "   - **Purpose**: Allows the model to weigh the importance of different words in a sequence relative to each other, enabling it to capture dependencies regardless of their distance in the input sequence.\n",
      "   - **How it Works**: For each word in the input, the model computes attention scores with every other word. These scores are used to create weighted representations of the input words, emphasizing relevant parts of the sequence.\n",
      "\n",
      "2. **Multi-Head Attention**:\n",
      "   - **Purpose**: Enhances the model's ability to focus on different parts of the sequence simultaneously, capturing various types of relationships.\n",
      "   - **How it Works**: Multiple self-attention mechanisms (heads) are run in parallel, each learning different aspects of the input. Their outputs are concatenated and linearly transformed.\n",
      "\n",
      "3. **Feed-Forward Neural Networks**:\n",
      "   - **Purpose**: Introduce non-linearity and allow for complex transformations of the input.\n",
      "   - **How it Works**: Each position in the sequence is processed independently using a two-layer feed-forward network, typically with a ReLU activation function between the layers.\n",
      "\n",
      "4. **Positional Encoding**:\n",
      "   - **Purpose**: Since transformers do not inherently understand the order of the sequence, positional encoding is added to provide information about the position of each word.\n",
      "   - **How it Works**: Positional encodings are added to the input embeddings, typically using sine and cosine functions to generate a unique encoding for each position.\n",
      "\n",
      "5. **Layer Normalization**:\n",
      "   - **Purpose**: Stabilizes the learning process and improves convergence.\n",
      "   - **How it Works**: Normalizes the inputs across the features, ensuring that the mean and variance are consistent across different layers.\n",
      "\n",
      "6. **Residual Connections**:\n",
      "   - **Purpose**: Facilitate the flow of gradients during training, helping to mitigate issues like vanishing gradients.\n",
      "   - **How it Works**: The input to each sub-layer is added to its output, allowing gradients to flow directly through the network.\n",
      "\n",
      "7. **Encoder-Decoder Structure** (in sequence-to-sequence models):\n",
      "   - **Encoder**: Processes the input sequence and generates a set of attention-based representations.\n",
      "   - **Decoder**: Uses these representations to generate the output sequence, incorporating attention over the encoder's output and self-attention over the generated sequence.\n",
      "\n",
      "These components work together to enable transformers to efficiently process and generate sequences, making them highly effective for tasks like translation, summarization, and more. The architecture's ability to handle long-range dependencies and parallelize computations has led to its widespread adoption in NLP and beyond.\n",
      "\n",
      "Retrieval Decision: [No Retrieval]\n",
      "Relevant Passages: 0\n",
      "Support Token: [No support / Contradictory]\n",
      "Utility Score: 5/5\n",
      "Composite Score: 0.25\n",
      "   Token: [Utility:5]\n",
      "   Reasoning: The answer is comprehensive, accurately detailing each key component of the transformer architecture and explaining their functions, directly addressing all aspects of the query.\n",
      "\n",
      "==========================================================================================\n",
      "📊 RESULT SUMMARY\n",
      "==========================================================================================\n",
      "Answer: The transformer architecture, introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017, is a foundational model for many modern natural language processing tasks. It consists of several key components:\n",
      "\n",
      "1. **Self-Attention Mechanism**: \n",
      "   - **Purpose**: Allows the model to weigh the importance of different words in a sequence relative to each other, enabling it to capture dependencies regardless of their distance in the input sequence.\n",
      "   - **How it Works**: For each word in the input, the model computes attention scores with every other word. These scores are used to create weighted representations of the input words, emphasizing relevant parts of the sequence.\n",
      "\n",
      "2. **Multi-Head Attention**:\n",
      "   - **Purpose**: Enhances the model's ability to focus on different parts of the sequence simultaneously, capturing various types of relationships.\n",
      "   - **How it Works**: Multiple self-attention mechanisms (heads) are run in parallel, each learning different aspects of the input. Their outputs are concatenated and linearly transformed.\n",
      "\n",
      "3. **Feed-Forward Neural Networks**:\n",
      "   - **Purpose**: Introduce non-linearity and allow for complex transformations of the input.\n",
      "   - **How it Works**: Each position in the sequence is processed independently using a two-layer feed-forward network, typically with a ReLU activation function between the layers.\n",
      "\n",
      "4. **Positional Encoding**:\n",
      "   - **Purpose**: Since transformers do not inherently understand the order of the sequence, positional encoding is added to provide information about the position of each word.\n",
      "   - **How it Works**: Positional encodings are added to the input embeddings, typically using sine and cosine functions to generate a unique encoding for each position.\n",
      "\n",
      "5. **Layer Normalization**:\n",
      "   - **Purpose**: Stabilizes the learning process and improves convergence.\n",
      "   - **How it Works**: Normalizes the inputs across the features, ensuring that the mean and variance are consistent across different layers.\n",
      "\n",
      "6. **Residual Connections**:\n",
      "   - **Purpose**: Facilitate the flow of gradients during training, helping to mitigate issues like vanishing gradients.\n",
      "   - **How it Works**: The input to each sub-layer is added to its output, allowing gradients to flow directly through the network.\n",
      "\n",
      "7. **Encoder-Decoder Structure** (in sequence-to-sequence models):\n",
      "   - **Encoder**: Processes the input sequence and generates a set of attention-based representations.\n",
      "   - **Decoder**: Uses these representations to generate the output sequence, incorporating attention over the encoder's output and self-attention over the generated sequence.\n",
      "\n",
      "These components work together to enable transformers to efficiently process and generate sequences, making them highly effective for tasks like translation, summarization, and more. The architecture's ability to handle long-range dependencies and parallelize computations has led to its widespread adoption in NLP and beyond.\n",
      "\n",
      "Retrieval Decision: [No Retrieval]\n",
      "Relevant Passages: 0\n",
      "Support Token: [No support / Contradictory]\n",
      "Utility Score: 5/5\n",
      "Composite Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Test Query 2: Should retrieve\n",
    "query2 = \"What are the key components of a transformer architecture and how do they work?\"\n",
    "\n",
    "result2 = selfrag.query(query2, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"📊 RESULT SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Answer: {result2['answer']}\")\n",
    "print(f\"\\nRetrieval Decision: {result2['retrieval_decision']}\")\n",
    "print(f\"Relevant Passages: {result2['num_relevant_passages']}\")\n",
    "print(f\"Support Token: {result2['support_token']}\")\n",
    "print(f\"Utility Score: {result2['utility_score']}/5\")\n",
    "print(f\"Composite Score: {result2['composite_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893dd537",
   "metadata": {},
   "source": [
    "### Test 3: Ambiguous Case (General Topic)\n",
    "Expected: System makes adaptive decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eabcab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "🧠 SELF-RAG PIPELINE\n",
      "==========================================================================================\n",
      "Query: Explain the concept of machine learning\n",
      "\n",
      "[STEP 1] 🔍 Retrieval Decision\n",
      "   Token: [No Retrieval]\n",
      "   Reasoning: The query asks for a general concept explanation that is covered by common knowledge within the training of a language model.\n",
      "\n",
      "[STEP 2] ⏭️  Skipped (No retrieval needed)\n",
      "\n",
      "[STEP 3] ✍️  Generation\n",
      "   Token: [No Retrieval]\n",
      "   Reasoning: The query asks for a general concept explanation that is covered by common knowledge within the training of a language model.\n",
      "\n",
      "[STEP 2] ⏭️  Skipped (No retrieval needed)\n",
      "\n",
      "[STEP 3] ✍️  Generation\n",
      "   Generated answer (2086 chars)\n",
      "\n",
      "[STEP 4] 🔬 Groundedness Check\n",
      "   Token: [No support / Contradictory]\n",
      "   Reasoning: No retrieved context to ground answer\n",
      "\n",
      "[STEP 5] ⭐ Utility Assessment\n",
      "   Generated answer (2086 chars)\n",
      "\n",
      "[STEP 4] 🔬 Groundedness Check\n",
      "   Token: [No support / Contradictory]\n",
      "   Reasoning: No retrieved context to ground answer\n",
      "\n",
      "[STEP 5] ⭐ Utility Assessment\n",
      "   Token: [Utility:5]\n",
      "   Reasoning: The answer is comprehensive, accurate, and directly addresses all aspects of the query by explaining machine learning concepts and techniques clearly.\n",
      "\n",
      "==========================================================================================\n",
      "📊 RESULT SUMMARY\n",
      "==========================================================================================\n",
      "Answer: Machine learning is a subset of artificial intelligence (AI) that involves the development of algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead, these systems learn from data, identifying patterns and making decisions based on the information they have processed.\n",
      "\n",
      "The core idea of machine learning is to allow computers to learn from experience. This is achieved through various techniques, including supervised learning, unsupervised learning, and reinforcement learning:\n",
      "\n",
      "1. **Supervised Learning**: In this approach, the algorithm is trained on a labeled dataset, meaning that each training example is paired with an output label. The model learns to map inputs to the correct outputs and is then able to make predictions on new, unseen data. Common applications include classification and regression tasks.\n",
      "\n",
      "2. **Unsupervised Learning**: Here, the algorithm is given data without explicit instructions on what to do with it. The system tries to identify patterns and relationships within the data. Clustering and association are typical tasks in unsupervised learning, where the goal might be to group similar data points or identify associations between variables.\n",
      "\n",
      "3. **Reinforcement Learning**: This involves training algorithms through a system of rewards and penalties. The model learns to make decisions by taking actions in an environment to maximize cumulative reward. It is often used in scenarios where decision-making is sequential, such as robotics and game playing.\n",
      "\n",
      "Machine learning models can be simple, like linear regression, or complex, like deep neural networks. The choice of model depends on the problem at hand, the nature of the data, and the desired outcome.\n",
      "\n",
      "Overall, machine learning is a powerful tool for automating and improving decision-making processes across various domains, including healthcare, finance, marketing, and more. It enables systems to adapt and improve over time, providing insights and solutions that might be difficult to achieve through traditional programming methods.\n",
      "\n",
      "Retrieval Decision: [No Retrieval]\n",
      "Relevant Passages: 0\n",
      "Support Token: [No support / Contradictory]\n",
      "Utility Score: 5/5\n",
      "Composite Score: 0.25\n",
      "   Token: [Utility:5]\n",
      "   Reasoning: The answer is comprehensive, accurate, and directly addresses all aspects of the query by explaining machine learning concepts and techniques clearly.\n",
      "\n",
      "==========================================================================================\n",
      "📊 RESULT SUMMARY\n",
      "==========================================================================================\n",
      "Answer: Machine learning is a subset of artificial intelligence (AI) that involves the development of algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead, these systems learn from data, identifying patterns and making decisions based on the information they have processed.\n",
      "\n",
      "The core idea of machine learning is to allow computers to learn from experience. This is achieved through various techniques, including supervised learning, unsupervised learning, and reinforcement learning:\n",
      "\n",
      "1. **Supervised Learning**: In this approach, the algorithm is trained on a labeled dataset, meaning that each training example is paired with an output label. The model learns to map inputs to the correct outputs and is then able to make predictions on new, unseen data. Common applications include classification and regression tasks.\n",
      "\n",
      "2. **Unsupervised Learning**: Here, the algorithm is given data without explicit instructions on what to do with it. The system tries to identify patterns and relationships within the data. Clustering and association are typical tasks in unsupervised learning, where the goal might be to group similar data points or identify associations between variables.\n",
      "\n",
      "3. **Reinforcement Learning**: This involves training algorithms through a system of rewards and penalties. The model learns to make decisions by taking actions in an environment to maximize cumulative reward. It is often used in scenarios where decision-making is sequential, such as robotics and game playing.\n",
      "\n",
      "Machine learning models can be simple, like linear regression, or complex, like deep neural networks. The choice of model depends on the problem at hand, the nature of the data, and the desired outcome.\n",
      "\n",
      "Overall, machine learning is a powerful tool for automating and improving decision-making processes across various domains, including healthcare, finance, marketing, and more. It enables systems to adapt and improve over time, providing insights and solutions that might be difficult to achieve through traditional programming methods.\n",
      "\n",
      "Retrieval Decision: [No Retrieval]\n",
      "Relevant Passages: 0\n",
      "Support Token: [No support / Contradictory]\n",
      "Utility Score: 5/5\n",
      "Composite Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Test Query 3: Ambiguous\n",
    "query3 = \"Explain the concept of machine learning\"\n",
    "\n",
    "result3 = selfrag.query(query3, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"📊 RESULT SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Answer: {result3['answer']}\")\n",
    "print(f\"\\nRetrieval Decision: {result3['retrieval_decision']}\")\n",
    "print(f\"Relevant Passages: {result3['num_relevant_passages']}\")\n",
    "print(f\"Support Token: {result3['support_token']}\")\n",
    "print(f\"Utility Score: {result3['utility_score']}/5\")\n",
    "print(f\"Composite Score: {result3['composite_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b9c14",
   "metadata": {},
   "source": [
    "## Step 7: Comparative Analysis\n",
    "\n",
    "Compare Self-RAG with baseline systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4067e47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "📊 SELF-RAG PERFORMANCE SUMMARY\n",
      "====================================================================================================\n",
      "                 Query Retrieval Decision  Retrieved Docs                      Support  Utility  Composite Score\n",
      "                 2 + 2     [No Retrieval]               0 [No support / Contradictory]        5             0.25\n",
      "Transformer components     [No Retrieval]               0 [No support / Contradictory]        5             0.25\n",
      "            Explain ML     [No Retrieval]               0 [No support / Contradictory]        5             0.25\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "🎯 Adaptive Behavior Analysis:\n",
      "   Queries triggering retrieval: 0/3\n",
      "   Average utility score: 5.0/5\n",
      "   Fully supported answers: 0/3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compile results\n",
    "comparison_data = [\n",
    "    {\n",
    "        \"Query\": \"2 + 2\",\n",
    "        \"Retrieval Decision\": result1['retrieval_decision'],\n",
    "        \"Retrieved Docs\": result1['num_relevant_passages'],\n",
    "        \"Support\": result1['support_token'],\n",
    "        \"Utility\": result1['utility_score'],\n",
    "        \"Composite Score\": result1['composite_score']\n",
    "    },\n",
    "    {\n",
    "        \"Query\": \"Transformer components\",\n",
    "        \"Retrieval Decision\": result2['retrieval_decision'],\n",
    "        \"Retrieved Docs\": result2['num_relevant_passages'],\n",
    "        \"Support\": result2['support_token'],\n",
    "        \"Utility\": result2['utility_score'],\n",
    "        \"Composite Score\": result2['composite_score']\n",
    "    },\n",
    "    {\n",
    "        \"Query\": \"Explain ML\",\n",
    "        \"Retrieval Decision\": result3['retrieval_decision'],\n",
    "        \"Retrieved Docs\": result3['num_relevant_passages'],\n",
    "        \"Support\": result3['support_token'],\n",
    "        \"Utility\": result3['utility_score'],\n",
    "        \"Composite Score\": result3['composite_score']\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"📊 SELF-RAG PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# System behavior analysis\n",
    "print(\"\\n🎯 Adaptive Behavior Analysis:\")\n",
    "print(f\"   Queries triggering retrieval: {sum(1 for r in [result1, result2, result3] if r['retrieval_decision'] == '[Retrieval]')}/3\")\n",
    "print(f\"   Average utility score: {sum(r['utility_score'] for r in [result1, result2, result3]) / 3:.1f}/5\")\n",
    "print(f\"   Fully supported answers: {sum(1 for r in [result1, result2, result3] if '[Fully supported]' in r['support_token'])}/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c5566",
   "metadata": {},
   "source": [
    "## Step 8: Critique-Weighted Scoring Demonstration\n",
    "\n",
    "Show how adjusting weights changes system behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88855c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "🔬 WEIGHT CONFIGURATION COMPARISON\n",
      "====================================================================================================\n",
      "Test Query: What are the key components of a transformer architecture?\n",
      "\n",
      "[CONFIG 1] Factual-Focused (w_support=3.0)\n",
      "--------------------------------------------------\n",
      "Composite Score: 0.125\n",
      "Support: [No support / Contradictory]\n",
      "Utility: 5/5\n",
      "\n",
      "[CONFIG 2] Utility-Focused (w_utility=3.0)\n",
      "--------------------------------------------------\n",
      "Composite Score: 0.125\n",
      "Support: [No support / Contradictory]\n",
      "Utility: 5/5\n",
      "\n",
      "[CONFIG 2] Utility-Focused (w_utility=3.0)\n",
      "--------------------------------------------------\n",
      "Composite Score: 0.75\n",
      "Support: [No support / Contradictory]\n",
      "Utility: 5/5\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "💡 Insight: Different weight configurations optimize for different objectives.\n",
      "   - Factual tasks (medical, legal): High w_support\n",
      "   - User engagement tasks: High w_utility\n",
      "   - Retrieval-critical tasks: High w_relevance\n",
      "Composite Score: 0.75\n",
      "Support: [No support / Contradictory]\n",
      "Utility: 5/5\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "💡 Insight: Different weight configurations optimize for different objectives.\n",
      "   - Factual tasks (medical, legal): High w_support\n",
      "   - User engagement tasks: High w_utility\n",
      "   - Retrieval-critical tasks: High w_relevance\n"
     ]
    }
   ],
   "source": [
    "# Create two variants with different weight configurations\n",
    "\n",
    "# Variant 1: Prioritize factual grounding (for fact-checking tasks)\n",
    "selfrag_factual = SelfRAGSystem(index, llm, critic, retriever)\n",
    "selfrag_factual.w_relevance = 0.5\n",
    "selfrag_factual.w_support = 3.0  # Very high weight on grounding\n",
    "selfrag_factual.w_utility = 0.5\n",
    "\n",
    "# Variant 2: Prioritize utility (for user satisfaction)\n",
    "selfrag_utility = SelfRAGSystem(index, llm, critic, retriever)\n",
    "selfrag_utility.w_relevance = 0.5\n",
    "selfrag_utility.w_support = 0.5\n",
    "selfrag_utility.w_utility = 3.0  # Very high weight on utility\n",
    "\n",
    "# Test same query with both variants\n",
    "test_query = \"What are the key components of a transformer architecture?\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"🔬 WEIGHT CONFIGURATION COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Test Query: {test_query}\\n\")\n",
    "\n",
    "print(\"[CONFIG 1] Factual-Focused (w_support=3.0)\")\n",
    "print(\"-\" * 50)\n",
    "result_factual = selfrag_factual.query(test_query, verbose=False)\n",
    "print(f\"Composite Score: {result_factual['composite_score']}\")\n",
    "print(f\"Support: {result_factual['support_token']}\")\n",
    "print(f\"Utility: {result_factual['utility_score']}/5\\n\")\n",
    "\n",
    "print(\"[CONFIG 2] Utility-Focused (w_utility=3.0)\")\n",
    "print(\"-\" * 50)\n",
    "result_utility = selfrag_utility.query(test_query, verbose=False)\n",
    "print(f\"Composite Score: {result_utility['composite_score']}\")\n",
    "print(f\"Support: {result_utility['support_token']}\")\n",
    "print(f\"Utility: {result_utility['utility_score']}/5\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"\\n💡 Insight: Different weight configurations optimize for different objectives.\")\n",
    "print(\"   - Factual tasks (medical, legal): High w_support\")\n",
    "print(\"   - User engagement tasks: High w_utility\")\n",
    "print(\"   - Retrieval-critical tasks: High w_relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba657bb",
   "metadata": {},
   "source": [
    "## 🎓 Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Adaptive Retrieval is Efficient**\n",
    "   - Traditional RAG retrieves for EVERY query (wasteful)\n",
    "   - Self-RAG decides *when* to retrieve based on query type\n",
    "   - Saves compute for queries answerable with parametric knowledge\n",
    "\n",
    "2. **Multi-Dimensional Critique Enables Control**\n",
    "   - **Retrieval tokens**: Adaptive behavior\n",
    "   - **Relevance tokens**: Quality filtering\n",
    "   - **Support tokens**: Hallucination detection\n",
    "   - **Utility tokens**: User satisfaction measurement\n",
    "\n",
    "3. **Weighted Scoring Enables Task-Specific Optimization**\n",
    "   - Adjusting `w_support` optimizes for factual accuracy\n",
    "   - Adjusting `w_utility` optimizes for helpfulness\n",
    "   - Adjusting `w_relevance` optimizes for retrieval precision\n",
    "\n",
    "4. **Self-RAG as Meta-Reasoner**\n",
    "   - From the curriculum: \"The LLM performs meta-reasoning about the task itself\"\n",
    "   - Self-RAG asks: \"Do I need help?\" before acting\n",
    "   - This is the bridge to fully Agentic RAG systems\n",
    "\n",
    "### Architectural Insights\n",
    "\n",
    "**The Progression of Intelligence:**\n",
    "1. **Traditional RAG**: Reactive executor\n",
    "2. **Corrective RAG**: Post-hoc corrector\n",
    "3. **Self-RAG**: Proactive meta-reasoner ← We are here\n",
    "4. **Agentic RAG**: Autonomous orchestrator (next demos)\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Latency Trade-offs**\n",
    "   - Multiple critique calls add latency (4-5 LLM calls per query)\n",
    "   - Can parallelize some critiques (relevance assessment)\n",
    "   - Adaptive retrieval saves time when `[No Retrieval]` is predicted\n",
    "\n",
    "2. **Implementation Reality**\n",
    "   - This demo *simulates* Self-RAG with prompting\n",
    "   - Production Self-RAG requires fine-tuning with special tokens\n",
    "   - See original paper: [Self-RAG (arXiv:2310.11511)](https://arxiv.org/abs/2310.11511)\n",
    "\n",
    "3. **When to Use Self-RAG**\n",
    "   - High-stakes domains (medical, legal, financial)\n",
    "   - Need for explainability (audit trail of decisions)\n",
    "   - Variable query complexity (mix of simple and complex queries)\n",
    "\n",
    "### Comparison with Corrective RAG\n",
    "\n",
    "| Aspect | Corrective RAG | Self-RAG |\n",
    "|--------|---------------|----------|\n",
    "| **When evaluated** | After retrieval | Before + During + After |\n",
    "| **Retrieval** | Always | Conditional (adaptive) |\n",
    "| **Critique dimensions** | 1 (relevance) | 4 (retrieval, relevance, support, utility) |\n",
    "| **Controllability** | Limited | High (weighted scoring) |\n",
    "| **Latency** | +1 LLM call | +4-5 LLM calls |\n",
    "| **Use case** | General robustness | High-stakes, explainable AI |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To extend this implementation:\n",
    "1. **Segment-Level Generation**: Implement iterative generation with per-segment critique\n",
    "2. **Beam Search**: Maintain multiple candidate answers scored by composite critique\n",
    "3. **Fine-Tuning**: Train a model to predict reflection tokens natively\n",
    "4. **Caching**: Cache retrieval decisions for similar queries\n",
    "5. **Monitoring**: Track critique distributions to identify system weaknesses\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 References\n",
    "\n",
    "**From the curriculum (AdvancedRAGWorkshop.md):**\n",
    "- Meta-reasoning and self-reflection in Agentic RAG systems\n",
    "- LLM as orchestrator: \"Is the information I have sufficient?\"\n",
    "\n",
    "**Original Self-RAG Paper:**\n",
    "- Asai et al., \"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\" (ICLR 2024, Oral)\n",
    "- arXiv:2310.11511\n",
    "- Implementation: https://github.com/AkariAsai/self-rag\n",
    "\n",
    "**Key Concepts Demonstrated:**\n",
    "- Adaptive retrieval based on query analysis\n",
    "- Multi-dimensional critique framework (4 token types)\n",
    "- Controllable generation via weighted scoring\n",
    "- Meta-reasoning: system reasoning about its own capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**Status**: Demo #8 Complete ✅"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
