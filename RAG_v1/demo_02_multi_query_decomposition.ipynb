{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8386c9ca",
   "metadata": {},
   "source": [
    "# Demo #2: Multi-Query and Sub-Query Decomposition\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates advanced query handling techniques in RAG systems by implementing:\n",
    "- **Sub-query decomposition**: Breaking complex questions into smaller, independent sub-queries\n",
    "- **Multi-query generation**: Creating multiple query variations from different perspectives\n",
    "- **Multi-hop reasoning**: Enabling comprehensive information gathering across interconnected topics\n",
    "\n",
    "### Why Query Decomposition Matters\n",
    "\n",
    "Complex queries often contain multiple facets or implicit sub-questions. A single retrieval pass may miss critical information because:\n",
    "- The query is too broad to match specific documents effectively\n",
    "- Different parts of the answer exist in separate documents\n",
    "- The query requires synthesizing information from multiple sources\n",
    "\n",
    "By decomposing complex queries into simpler sub-queries, we can:\n",
    "- Improve retrieval recall (find more relevant documents)\n",
    "- Enable multi-hop reasoning (connect information across documents)\n",
    "- Generate more comprehensive and accurate answers\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. How to detect and decompose complex queries\n",
    "2. How to execute parallel retrieval for multiple sub-queries\n",
    "3. How to aggregate and organize multi-source contexts\n",
    "4. Advanced techniques like Diverse Multi-Query Rewriting (DMQR-RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dcc802",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai langchain-community langchain-chroma\n",
    "!pip install -q openai chromadb sentence-transformers tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf9b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Other imports\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cae999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "# Option 1: Set as environment variable (recommended)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Option 2: Load from file (for workshop)\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"✓ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f035cb",
   "metadata": {},
   "source": [
    "## 2. Create Sample Knowledge Base\n",
    "\n",
    "We'll create a knowledge base with interconnected information about smartphones. This will require multi-hop reasoning to answer complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about smartphones - designed to require multi-hop reasoning\n",
    "sample_documents = [\n",
    "    # iPhone 15 Pro information\n",
    "    \"\"\"The iPhone 15 Pro features Apple's A17 Pro chip with a 3nm process, making it incredibly \n",
    "    energy efficient. The device includes a 48MP main camera with advanced computational photography \n",
    "    capabilities. Apple has introduced a new Action Button replacing the traditional mute switch.\"\"\",\n",
    "    \n",
    "    \"\"\"iPhone 15 Pro battery life is rated at up to 23 hours of video playback. The device supports \n",
    "    fast charging up to 20W and MagSafe wireless charging at 15W. The battery capacity is approximately \n",
    "    3,274 mAh for the standard Pro model.\"\"\",\n",
    "    \n",
    "    \"\"\"The iPhone 15 Pro camera system includes a 48MP main camera, 12MP ultra-wide, and a 12MP \n",
    "    telephoto with 3x optical zoom. The main camera can capture ProRAW images and supports 4K video \n",
    "    at 60fps with ProRes recording capabilities.\"\"\",\n",
    "    \n",
    "    # Samsung Galaxy S24 Ultra information\n",
    "    \"\"\"Samsung Galaxy S24 Ultra is powered by the Snapdragon 8 Gen 3 processor in most markets, \n",
    "    built on a 4nm process. The device features Samsung's largest battery in an S-series phone \n",
    "    and includes an integrated S Pen for productivity.\"\"\",\n",
    "    \n",
    "    \"\"\"The Galaxy S24 Ultra offers exceptional battery life with its 5,000 mAh battery, providing \n",
    "    up to 28 hours of video playback. It supports 45W fast charging, 15W wireless charging, and \n",
    "    4.5W reverse wireless charging for accessories.\"\"\",\n",
    "    \n",
    "    \"\"\"Samsung Galaxy S24 Ultra features a quad camera system with a 200MP main sensor, 12MP ultra-wide, \n",
    "    10MP 3x telephoto, and 50MP 5x telephoto. The 200MP sensor uses pixel binning to produce 12MP images \n",
    "    with exceptional detail. It can record 8K video at 30fps.\"\"\",\n",
    "    \n",
    "    # Google Pixel 8 Pro information\n",
    "    \"\"\"Google Pixel 8 Pro uses Google's custom Tensor G3 chip, optimized for AI and machine learning \n",
    "    tasks. The device features a 6.7-inch LTPO OLED display with adaptive refresh rate up to 120Hz. \n",
    "    Google emphasizes computational photography over raw hardware specs.\"\"\",\n",
    "    \n",
    "    \"\"\"The Pixel 8 Pro has a 5,050 mAh battery with impressive battery life, achieving up to 24 hours \n",
    "    of mixed use. It supports 30W fast charging and 23W wireless charging. The Extreme Battery Saver \n",
    "    mode can extend battery life up to 72 hours.\"\"\",\n",
    "    \n",
    "    \"\"\"Pixel 8 Pro camera system includes a 50MP main sensor, 48MP ultra-wide, and 48MP telephoto with \n",
    "    5x optical zoom. Google's computational photography features include Magic Eraser, Photo Unblur, \n",
    "    and Night Sight. The camera can capture 4K video at 60fps.\"\"\",\n",
    "    \n",
    "    # Comparative information\n",
    "    \"\"\"When comparing flagship smartphones in 2024, key factors include processor efficiency, camera \n",
    "    versatility, battery capacity, and software ecosystem. The iPhone 15 Pro excels in ecosystem \n",
    "    integration, the Galaxy S24 Ultra leads in raw specifications, and the Pixel 8 Pro stands out \n",
    "    for AI-powered features.\"\"\",\n",
    "    \n",
    "    \"\"\"Battery life in modern smartphones depends not just on battery capacity (mAh) but also on \n",
    "    processor efficiency, display technology, and software optimization. A phone with a smaller \n",
    "    battery but more efficient processor can often outlast one with a larger battery.\"\"\",\n",
    "    \n",
    "    \"\"\"Camera quality in smartphones is determined by multiple factors: sensor size, pixel count, \n",
    "    lens quality, optical image stabilization, and computational photography algorithms. Higher \n",
    "    megapixels don't always mean better photos - sensor size and processing capabilities matter more.\"\"\",\n",
    "]\n",
    "\n",
    "print(f\"Created knowledge base with {len(sample_documents)} documents\")\n",
    "print(f\"Total characters: {sum(len(doc) for doc in sample_documents):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558282b",
   "metadata": {},
   "source": [
    "## 3. Initialize Embedding Model and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e35c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "print(\"✓ Embedding model initialized\")\n",
    "print(f\"  Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"  Embedding dimension: 384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter for chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Convert documents to LangChain Document objects and split\n",
    "docs = [Document(page_content=doc) for doc in sample_documents]\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"✓ Documents split into {len(split_docs)} chunks\")\n",
    "print(f\"  Average chunk size: {sum(len(doc.page_content) for doc in split_docs) // len(split_docs)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ab569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"smartphone_knowledge\"\n",
    ")\n",
    "\n",
    "print(\"✓ Vector store created and indexed\")\n",
    "print(f\"  Total chunks indexed: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6647e44",
   "metadata": {},
   "source": [
    "## 4. Initialize LLM for Query Processing and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356315f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=2000\n",
    ")\n",
    "\n",
    "print(\"✓ LLM initialized\")\n",
    "print(f\"  Model: gpt-4o-mini\")\n",
    "print(f\"  Temperature: 0.3 (focused, consistent responses)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ae2c7",
   "metadata": {},
   "source": [
    "## 5. Baseline: Naive Single-Query RAG\n",
    "\n",
    "First, let's implement a baseline naive RAG system that performs a single retrieval for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_rag(query: str, k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Baseline naive RAG: single-query retrieval and generation.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing answer and metadata\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    # Prepare context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Create prompt for generation\n",
    "    prompt = f\"\"\"Using the following context, please answer the question. If the context doesn't \n",
    "contain enough information to answer fully, acknowledge this in your response.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": response.content,\n",
    "        \"retrieved_docs\": [doc.page_content for doc in retrieved_docs],\n",
    "        \"num_docs\": len(retrieved_docs)\n",
    "    }\n",
    "\n",
    "print(\"✓ Naive RAG function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799e0f6",
   "metadata": {},
   "source": [
    "## 6. Sub-Query Decomposition Implementation\n",
    "\n",
    "Now let's implement the core query decomposition logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233524f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template for query decomposition\n",
    "DECOMPOSITION_TEMPLATE = \"\"\"You are an expert at breaking down complex questions into simpler, \n",
    "independent sub-questions.\n",
    "\n",
    "Your task is to analyze the given question and decompose it into a set of simpler sub-questions that, \n",
    "when answered together, would provide a comprehensive answer to the original question.\n",
    "\n",
    "Guidelines:\n",
    "1. Each sub-question should be independent and answerable on its own\n",
    "2. Sub-questions should be specific and focused\n",
    "3. Cover all aspects of the original question\n",
    "4. Aim for 2-5 sub-questions depending on complexity\n",
    "5. Return ONLY a valid JSON array of sub-questions, nothing else\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Sub-questions (as JSON array):\"\"\"\n",
    "\n",
    "decomposition_prompt = PromptTemplate(\n",
    "    template=DECOMPOSITION_TEMPLATE,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "print(\"✓ Decomposition prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd443179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Decompose a complex query into simpler sub-queries.\n",
    "    \n",
    "    Args:\n",
    "        query: Complex user question\n",
    "    \n",
    "    Returns:\n",
    "        List of sub-questions\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    prompt = decomposition_prompt.format(question=query)\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse JSON response\n",
    "    try:\n",
    "        # Extract JSON from response (handle markdown code blocks if present)\n",
    "        response_text = response.content.strip()\n",
    "        if \"```json\" in response_text:\n",
    "            response_text = response_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in response_text:\n",
    "            response_text = response_text.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        sub_queries = json.loads(response_text)\n",
    "        \n",
    "        # Validate that we got a list\n",
    "        if not isinstance(sub_queries, list):\n",
    "            raise ValueError(\"Response is not a list\")\n",
    "        \n",
    "        return sub_queries\n",
    "    \n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Warning: Failed to parse sub-queries: {e}\")\n",
    "        print(f\"Response was: {response.content}\")\n",
    "        # Fallback: return original query\n",
    "        return [query]\n",
    "\n",
    "print(\"✓ Query decomposition function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516e35d",
   "metadata": {},
   "source": [
    "## 7. Multi-Query Retrieval Pipeline\n",
    "\n",
    "Implement parallel retrieval for multiple sub-queries with result aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f94e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_for_subqueries(\n",
    "    sub_queries: List[str], \n",
    "    k: int = 3\n",
    ") -> Dict[str, List[Document]]:\n",
    "    \"\"\"\n",
    "    Retrieve documents for each sub-query independently.\n",
    "    \n",
    "    Args:\n",
    "        sub_queries: List of sub-questions\n",
    "        k: Number of documents to retrieve per sub-query\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping each sub-query to its retrieved documents\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for sub_query in sub_queries:\n",
    "        # Retrieve documents for this sub-query\n",
    "        docs = vectorstore.similarity_search(sub_query, k=k)\n",
    "        results[sub_query] = docs\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Multi-query retrieval function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85405956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_documents(doc_dict: Dict[str, List[Document]]) -> List[Tuple[str, Document]]:\n",
    "    \"\"\"\n",
    "    Deduplicate retrieved documents while maintaining sub-query associations.\n",
    "    \n",
    "    Args:\n",
    "        doc_dict: Dictionary mapping sub-queries to retrieved documents\n",
    "    \n",
    "    Returns:\n",
    "        List of (sub_query, document) tuples with duplicates removed\n",
    "    \"\"\"\n",
    "    seen_content = set()\n",
    "    deduplicated = []\n",
    "    \n",
    "    for sub_query, docs in doc_dict.items():\n",
    "        for doc in docs:\n",
    "            # Use content as deduplication key\n",
    "            content_key = doc.page_content.strip()\n",
    "            \n",
    "            if content_key not in seen_content:\n",
    "                seen_content.add(content_key)\n",
    "                deduplicated.append((sub_query, doc))\n",
    "    \n",
    "    return deduplicated\n",
    "\n",
    "print(\"✓ Document deduplication function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b0bb6",
   "metadata": {},
   "source": [
    "## 8. Context Synthesis and Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_answer(\n",
    "    original_query: str,\n",
    "    sub_queries: List[str],\n",
    "    retrieved_docs: List[Tuple[str, Document]]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Synthesize final answer from sub-query contexts.\n",
    "    \n",
    "    Args:\n",
    "        original_query: Original complex question\n",
    "        sub_queries: List of sub-questions\n",
    "        retrieved_docs: List of (sub_query, document) tuples\n",
    "    \n",
    "    Returns:\n",
    "        Final synthesized answer\n",
    "    \"\"\"\n",
    "    # Organize context by sub-query\n",
    "    context_by_subquery = defaultdict(list)\n",
    "    for sub_query, doc in retrieved_docs:\n",
    "        context_by_subquery[sub_query].append(doc.page_content)\n",
    "    \n",
    "    # Build structured context string\n",
    "    context_sections = []\n",
    "    for i, sub_query in enumerate(sub_queries, 1):\n",
    "        if sub_query in context_by_subquery:\n",
    "            docs_text = \"\\n\".join(context_by_subquery[sub_query])\n",
    "            context_sections.append(f\"\"\"Sub-question {i}: {sub_query}\n",
    "Relevant Information:\n",
    "{docs_text}\"\"\")\n",
    "    \n",
    "    full_context = \"\\n\\n\" + \"=\"*80 + \"\\n\\n\".join(context_sections)\n",
    "    \n",
    "    # Create synthesis prompt\n",
    "    synthesis_prompt = f\"\"\"You are answering a complex question that has been broken down into \n",
    "sub-questions. Below you will find each sub-question along with relevant information retrieved \n",
    "for that sub-question.\n",
    "\n",
    "Your task is to synthesize a comprehensive answer to the original question by integrating \n",
    "information from all sub-questions.\n",
    "\n",
    "Guidelines:\n",
    "1. Provide a complete, well-structured answer to the original question\n",
    "2. Integrate information from all sub-questions cohesively\n",
    "3. If information is missing or contradictory, acknowledge this\n",
    "4. Be specific and cite key details from the context\n",
    "\n",
    "Original Question: {original_query}\n",
    "\n",
    "Sub-questions and Retrieved Information:\n",
    "{full_context}\n",
    "\n",
    "Comprehensive Answer:\"\"\"\n",
    "    \n",
    "    # Generate final answer\n",
    "    response = llm.invoke(synthesis_prompt)\n",
    "    return response.content\n",
    "\n",
    "print(\"✓ Answer synthesis function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce132f19",
   "metadata": {},
   "source": [
    "## 9. Complete Multi-Query RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_rag(\n",
    "    query: str,\n",
    "    k_per_subquery: int = 3,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete multi-query RAG pipeline with sub-query decomposition.\n",
    "    \n",
    "    Args:\n",
    "        query: Complex user question\n",
    "        k_per_subquery: Number of documents to retrieve per sub-query\n",
    "        verbose: Whether to print intermediate steps\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing answer and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"MULTI-QUERY RAG PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nOriginal Query: {query}\\n\")\n",
    "    \n",
    "    # Step 1: Decompose query into sub-queries\n",
    "    if verbose:\n",
    "        print(\"Step 1: Decomposing query into sub-questions...\")\n",
    "    \n",
    "    sub_queries = decompose_query(query)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nGenerated {len(sub_queries)} sub-questions:\")\n",
    "        for i, sq in enumerate(sub_queries, 1):\n",
    "            print(f\"  {i}. {sq}\")\n",
    "        print()\n",
    "    \n",
    "    # Step 2: Retrieve documents for each sub-query\n",
    "    if verbose:\n",
    "        print(\"Step 2: Retrieving documents for each sub-question...\")\n",
    "    \n",
    "    retrieved_dict = retrieve_for_subqueries(sub_queries, k=k_per_subquery)\n",
    "    \n",
    "    if verbose:\n",
    "        total_before = sum(len(docs) for docs in retrieved_dict.values())\n",
    "        print(f\"  Retrieved {total_before} documents (before deduplication)\\n\")\n",
    "    \n",
    "    # Step 3: Deduplicate documents\n",
    "    if verbose:\n",
    "        print(\"Step 3: Deduplicating retrieved documents...\")\n",
    "    \n",
    "    deduplicated_docs = deduplicate_documents(retrieved_dict)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  {len(deduplicated_docs)} unique documents after deduplication\\n\")\n",
    "    \n",
    "    # Step 4: Synthesize final answer\n",
    "    if verbose:\n",
    "        print(\"Step 4: Synthesizing comprehensive answer...\\n\")\n",
    "    \n",
    "    answer = synthesize_answer(query, sub_queries, deduplicated_docs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"FINAL ANSWER\")\n",
    "        print(\"=\"*80)\n",
    "        print(answer)\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"sub_queries\": sub_queries,\n",
    "        \"answer\": answer,\n",
    "        \"num_subqueries\": len(sub_queries),\n",
    "        \"num_unique_docs\": len(deduplicated_docs),\n",
    "        \"retrieved_docs\": [(sq, doc.page_content) for sq, doc in deduplicated_docs]\n",
    "    }\n",
    "\n",
    "print(\"✓ Complete multi-query RAG pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb0557",
   "metadata": {},
   "source": [
    "## 10. Comparative Evaluation: Naive vs. Multi-Query RAG\n",
    "\n",
    "Let's test both approaches on complex queries that require multi-hop reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f960cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries with increasing complexity\n",
    "test_queries = [\n",
    "    \"What are the differences in battery life and camera quality between the iPhone 15 Pro and Samsung Galaxy S24 Ultra?\",\n",
    "    \"Compare the processing power, charging capabilities, and photography features across iPhone 15 Pro, Galaxy S24 Ultra, and Pixel 8 Pro\",\n",
    "    \"Which flagship phone would be best for a photographer who needs long battery life and fast charging?\"\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(test_queries)} test queries for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6ea36",
   "metadata": {},
   "source": [
    "### Test Query 1: Two-Aspect Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c88496",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = test_queries[0]\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST QUERY 1\")\n",
    "print(\"#\"*80)\n",
    "print(f\"Query: {query_1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive RAG approach\n",
    "print(\"\\n--- NAIVE RAG APPROACH ---\\n\")\n",
    "naive_result_1 = naive_rag(query_1, k=5)\n",
    "\n",
    "print(f\"Retrieved {naive_result_1['num_docs']} documents\\n\")\n",
    "print(\"Answer:\")\n",
    "print(naive_result_1['answer'])\n",
    "print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbafdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Query RAG approach\n",
    "print(\"\\n--- MULTI-QUERY RAG APPROACH ---\\n\")\n",
    "multi_result_1 = multi_query_rag(query_1, k_per_subquery=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a373ef",
   "metadata": {},
   "source": [
    "### Test Query 2: Multi-Device, Multi-Aspect Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = test_queries[1]\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST QUERY 2\")\n",
    "print(\"#\"*80)\n",
    "print(f\"Query: {query_2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive RAG approach\n",
    "print(\"\\n--- NAIVE RAG APPROACH ---\\n\")\n",
    "naive_result_2 = naive_rag(query_2, k=5)\n",
    "\n",
    "print(f\"Retrieved {naive_result_2['num_docs']} documents\\n\")\n",
    "print(\"Answer:\")\n",
    "print(naive_result_2['answer'])\n",
    "print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Query RAG approach\n",
    "print(\"\\n--- MULTI-QUERY RAG APPROACH ---\\n\")\n",
    "multi_result_2 = multi_query_rag(query_2, k_per_subquery=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6d6d9",
   "metadata": {},
   "source": [
    "### Test Query 3: Recommendation with Multiple Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3 = test_queries[2]\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST QUERY 3\")\n",
    "print(\"#\"*80)\n",
    "print(f\"Query: {query_3}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9890d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive RAG approach\n",
    "print(\"\\n--- NAIVE RAG APPROACH ---\\n\")\n",
    "naive_result_3 = naive_rag(query_3, k=5)\n",
    "\n",
    "print(f\"Retrieved {naive_result_3['num_docs']} documents\\n\")\n",
    "print(\"Answer:\")\n",
    "print(naive_result_3['answer'])\n",
    "print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcefa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Query RAG approach\n",
    "print(\"\\n--- MULTI-QUERY RAG APPROACH ---\\n\")\n",
    "multi_result_3 = multi_query_rag(query_3, k_per_subquery=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c8861d",
   "metadata": {},
   "source": [
    "## 11. Advanced Technique: Diverse Multi-Query Rewriting (DMQR-RAG)\n",
    "\n",
    "DMQR-RAG generates diverse query variations at different information granularity levels to improve retrieval coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90cd1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DMQR prompt template\n",
    "DMQR_TEMPLATE = \"\"\"You are an expert at generating diverse query variations to improve information retrieval.\n",
    "\n",
    "Your task is to rewrite the given question in multiple ways, each focusing on a different aspect or \n",
    "granularity level:\n",
    "\n",
    "1. HIGH-LEVEL: A broad, conceptual version of the question\n",
    "2. SPECIFIC: A detailed, technical version with specific terms\n",
    "3. ALTERNATIVE PHRASING: Same meaning, completely different wording\n",
    "4. DOMAIN-SPECIFIC: Using domain-specific terminology and concepts\n",
    "\n",
    "Each variation should capture the same underlying information need but from a different angle.\n",
    "\n",
    "Return ONLY a valid JSON array with exactly 4 query variations, nothing else.\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Diverse Query Variations (as JSON array):\"\"\"\n",
    "\n",
    "dmqr_prompt = PromptTemplate(\n",
    "    template=DMQR_TEMPLATE,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "print(\"✓ DMQR prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5483de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmqr_generate_queries(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate diverse multi-query rewrites using DMQR approach.\n",
    "    \n",
    "    Args:\n",
    "        query: Original user question\n",
    "    \n",
    "    Returns:\n",
    "        List of diverse query variations\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    prompt = dmqr_prompt.format(question=query)\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse JSON response\n",
    "    try:\n",
    "        response_text = response.content.strip()\n",
    "        if \"```json\" in response_text:\n",
    "            response_text = response_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in response_text:\n",
    "            response_text = response_text.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        query_variations = json.loads(response_text)\n",
    "        \n",
    "        if not isinstance(query_variations, list):\n",
    "            raise ValueError(\"Response is not a list\")\n",
    "        \n",
    "        # Include original query as well\n",
    "        return [query] + query_variations\n",
    "    \n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Warning: Failed to parse DMQR queries: {e}\")\n",
    "        print(f\"Response was: {response.content}\")\n",
    "        return [query]\n",
    "\n",
    "print(\"✓ DMQR query generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cee927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmqr_rag(\n",
    "    query: str,\n",
    "    k_per_variation: int = 2,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    DMQR-RAG: Diverse Multi-Query Rewriting for RAG.\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        k_per_variation: Number of documents to retrieve per variation\n",
    "        verbose: Whether to print intermediate steps\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing answer and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"DMQR-RAG PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nOriginal Query: {query}\\n\")\n",
    "    \n",
    "    # Generate diverse query variations\n",
    "    if verbose:\n",
    "        print(\"Step 1: Generating diverse query variations...\")\n",
    "    \n",
    "    query_variations = dmqr_generate_queries(query)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nGenerated {len(query_variations)} query variations:\")\n",
    "        for i, qv in enumerate(query_variations, 1):\n",
    "            print(f\"  {i}. {qv}\")\n",
    "        print()\n",
    "    \n",
    "    # Retrieve documents for each variation\n",
    "    if verbose:\n",
    "        print(\"Step 2: Retrieving documents for each variation...\")\n",
    "    \n",
    "    all_docs = []\n",
    "    seen_content = set()\n",
    "    \n",
    "    for variation in query_variations:\n",
    "        docs = vectorstore.similarity_search(variation, k=k_per_variation)\n",
    "        \n",
    "        # Deduplicate on the fly\n",
    "        for doc in docs:\n",
    "            content_key = doc.page_content.strip()\n",
    "            if content_key not in seen_content:\n",
    "                seen_content.add(content_key)\n",
    "                all_docs.append(doc)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Retrieved {len(all_docs)} unique documents\\n\")\n",
    "    \n",
    "    # Prepare context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in all_docs])\n",
    "    \n",
    "    # Generate answer\n",
    "    if verbose:\n",
    "        print(\"Step 3: Generating comprehensive answer...\\n\")\n",
    "    \n",
    "    prompt = f\"\"\"Using the following context, please answer the question comprehensively.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\"FINAL ANSWER\")\n",
    "        print(\"=\"*80)\n",
    "        print(response.content)\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"query_variations\": query_variations,\n",
    "        \"answer\": response.content,\n",
    "        \"num_variations\": len(query_variations),\n",
    "        \"num_unique_docs\": len(all_docs)\n",
    "    }\n",
    "\n",
    "print(\"✓ DMQR-RAG pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e5115",
   "metadata": {},
   "source": [
    "### Test DMQR-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DMQR-RAG on a complex query\n",
    "test_query = \"What makes a smartphone camera system high quality?\"\n",
    "\n",
    "dmqr_result = dmqr_rag(test_query, k_per_variation=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b3c01",
   "metadata": {},
   "source": [
    "## 12. Analysis and Insights\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "**1. Retrieval Coverage:**\n",
    "- **Naive RAG**: Performs a single retrieval pass with k=5 documents, which may miss information if the query is complex or poorly phrased\n",
    "- **Multi-Query RAG**: Decomposes complex queries into sub-questions, retrieving k documents per sub-question, then deduplicating. This significantly increases the likelihood of finding all relevant information\n",
    "- **DMQR-RAG**: Generates diverse query variations at different granularity levels, casting a wider net to capture documents that might be missed by a single query formulation\n",
    "\n",
    "**2. Answer Quality:**\n",
    "- **Naive RAG**: Can provide good answers for simple queries, but struggles with multi-faceted questions or queries requiring information synthesis from multiple documents\n",
    "- **Multi-Query RAG**: Excels at complex queries by ensuring all aspects are addressed. The structured context organization helps the LLM generate more comprehensive answers\n",
    "- **DMQR-RAG**: Particularly effective when the knowledge base uses varied terminology or when queries can be interpreted at different abstraction levels\n",
    "\n",
    "**3. Trade-offs:**\n",
    "- **Latency**: Multi-query approaches require additional LLM calls (for decomposition/rewriting) and multiple retrieval operations, increasing response time\n",
    "- **Cost**: More LLM API calls mean higher costs, especially with proprietary models\n",
    "- **Complexity**: Increased system complexity requires more sophisticated error handling and monitoring\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Use Naive RAG when:**\n",
    "- Queries are simple and well-defined\n",
    "- Latency is critical\n",
    "- Cost optimization is a priority\n",
    "- Knowledge base is small and well-structured\n",
    "\n",
    "**Use Multi-Query RAG when:**\n",
    "- Queries are inherently complex with multiple facets\n",
    "- Answer completeness is more important than speed\n",
    "- Knowledge base is large with information spread across documents\n",
    "- Application requires multi-hop reasoning\n",
    "\n",
    "**Use DMQR-RAG when:**\n",
    "- Knowledge base uses inconsistent or varied terminology\n",
    "- Queries can be interpreted at different abstraction levels\n",
    "- Maximum retrieval recall is critical\n",
    "- Domain expertise varies among users (some technical, some not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f766b8",
   "metadata": {},
   "source": [
    "## 13. Production Considerations\n",
    "\n",
    "### Optimization Strategies\n",
    "\n",
    "1. **Adaptive Query Decomposition**: Not all queries need decomposition. Implement a classifier to detect query complexity and route simple queries to naive RAG\n",
    "\n",
    "2. **Parallel Processing**: Execute sub-query retrievals in parallel using async/await or threading to reduce latency\n",
    "\n",
    "3. **Caching**: Cache decomposition results and embeddings for frequently asked queries\n",
    "\n",
    "4. **Smart Deduplication**: Use semantic similarity for deduplication instead of exact content matching to catch near-duplicates\n",
    "\n",
    "5. **Result Ranking**: Implement scoring mechanisms (e.g., Reciprocal Rank Fusion) to prioritize the most relevant documents across sub-queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple query complexity classifier\n",
    "def classify_query_complexity(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify query complexity to determine which RAG approach to use.\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "    \n",
    "    Returns:\n",
    "        'simple' or 'complex'\n",
    "    \"\"\"\n",
    "    complexity_indicators = [\n",
    "        'compare', 'difference', 'versus', 'vs', 'and',\n",
    "        'both', 'all', 'multiple', 'various', 'different'\n",
    "    ]\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Check for complexity indicators\n",
    "    indicator_count = sum(1 for indicator in complexity_indicators if indicator in query_lower)\n",
    "    \n",
    "    # Check query length\n",
    "    word_count = len(query.split())\n",
    "    \n",
    "    # Simple heuristic: complex if multiple indicators or very long query\n",
    "    if indicator_count >= 2 or word_count > 15:\n",
    "        return 'complex'\n",
    "    else:\n",
    "        return 'simple'\n",
    "\n",
    "# Test the classifier\n",
    "test_queries_classify = [\n",
    "    \"What is the battery life of iPhone 15 Pro?\",\n",
    "    \"Compare battery life and camera quality between iPhone and Samsung\",\n",
    "    \"Which phone has the best camera?\"\n",
    "]\n",
    "\n",
    "print(\"Query Complexity Classification:\")\n",
    "print(\"=\"*80)\n",
    "for tq in test_queries_classify:\n",
    "    classification = classify_query_complexity(tq)\n",
    "    print(f\"Query: {tq}\")\n",
    "    print(f\"Classification: {classification}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692171b",
   "metadata": {},
   "source": [
    "## 14. Key Takeaways\n",
    "\n",
    "1. **Query decomposition is essential for complex queries**: Breaking down multi-faceted questions into simpler sub-questions dramatically improves retrieval quality and answer comprehensiveness\n",
    "\n",
    "2. **Sub-queries should be independent**: Each sub-question should be answerable on its own to enable parallel retrieval and avoid sequential dependencies\n",
    "\n",
    "3. **Deduplication is critical**: Multiple sub-queries will often retrieve overlapping documents, so efficient deduplication prevents redundant context and token waste\n",
    "\n",
    "4. **Context organization matters**: Structuring retrieved contexts by sub-query helps the LLM understand which information addresses which aspect of the original question\n",
    "\n",
    "5. **Diverse query rewriting (DMQR) expands coverage**: Generating queries at different granularity levels captures documents that single-perspective queries might miss\n",
    "\n",
    "6. **Trade-offs exist**: Multi-query approaches improve quality but increase latency and cost - choose based on your application's requirements\n",
    "\n",
    "7. **Adaptive routing is key for production**: Not all queries need complex decomposition - simple queries should be handled efficiently with naive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042f28d",
   "metadata": {},
   "source": [
    "## 15. Next Steps and Further Exploration\n",
    "\n",
    "To extend this demo:\n",
    "\n",
    "1. **Implement dependency detection**: Analyze sub-queries to identify dependencies and execute them sequentially when needed\n",
    "\n",
    "2. **Add Reciprocal Rank Fusion (RRF)**: Implement sophisticated ranking algorithms to score and prioritize documents across sub-queries\n",
    "\n",
    "3. **Experiment with different LLMs**: Test decomposition quality with different models (GPT-4, Claude, open-source models)\n",
    "\n",
    "4. **Build evaluation metrics**: Implement automated evaluation to measure retrieval recall, precision, and answer quality\n",
    "\n",
    "5. **Integrate with hybrid search**: Combine multi-query decomposition with hybrid search (dense + sparse vectors) for even better retrieval\n",
    "\n",
    "6. **Add query intent classification**: Build a classifier to route queries to the most appropriate decomposition strategy\n",
    "\n",
    "---\n",
    "\n",
    "**End of Demo #2**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
