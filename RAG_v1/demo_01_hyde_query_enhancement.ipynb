{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f7efdd",
   "metadata": {},
   "source": [
    "# Demo #1: Query Enhancement with HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "## üéØ Workshop Objectives\n",
    "\n",
    "In this notebook, we'll explore **Hypothetical Document Embeddings (HyDE)**, an advanced RAG technique that bridges the semantic gap between user queries and documents. We'll learn how to:\n",
    "\n",
    "1. Understand the **query-to-document asymmetry problem**\n",
    "2. Implement a **baseline Naive RAG** system\n",
    "3. Build an **advanced HyDE-enhanced RAG** pipeline\n",
    "4. Compare performance and analyze when HyDE excels\n",
    "5. Explore advanced HyDE variations\n",
    "\n",
    "## üìö Core Concepts\n",
    "\n",
    "### The Query-Document Asymmetry Problem\n",
    "\n",
    "Traditional RAG systems face a fundamental challenge:\n",
    "- **User queries** are typically short, keyword-focused, and question-like\n",
    "- **Documents** are long, verbose, and declarative\n",
    "\n",
    "When we embed both into the same vector space, the semantic mismatch can lead to suboptimal retrieval results.\n",
    "\n",
    "### HyDE Solution\n",
    "\n",
    "HyDE takes a clever approach:\n",
    "1. Instead of embedding the query directly, we ask an LLM to generate a **hypothetical ideal answer**\n",
    "2. We embed this rich, document-like answer\n",
    "3. We search using this embedding (answer-to-answer similarity vs. query-to-document)\n",
    "\n",
    "This transforms the search from \"query‚Üídocument\" to \"answer‚Üíanswer\", significantly improving semantic alignment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba1632",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's install and import all required libraries for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running for the first time)\n",
    "# !pip install langchain langchain-openai langchain-community chromadb sentence-transformers openai tiktoken\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d85dad",
   "metadata": {},
   "source": [
    "### Configure API Keys and LLM\n",
    "\n",
    "**Important:** Set your OpenAI API key as an environment variable or directly in the code (for demo purposes only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "# Option 1: Set as environment variable (recommended)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Option 2: Load from a .env file\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found in environment variables!\")\n",
    "    print(\"Please set it before proceeding.\")\n",
    "else:\n",
    "    print(\"‚úÖ API key configured successfully!\")\n",
    "\n",
    "# Initialize the LLM for query generation and answer generation\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1372fe",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion and Knowledge Base Creation\n",
    "\n",
    "Let's create a sample knowledge base about advanced RAG techniques. In a real scenario, you would load this from PDFs, web scraping, or databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef160c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample knowledge base: Technical documentation about RAG systems\n",
    "sample_documents = [\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation (RAG) is an AI framework that combines the power of large language models \n",
    "    with external knowledge retrieval. The core principle is to retrieve relevant information from a knowledge \n",
    "    base before generating a response. This approach significantly reduces hallucinations and provides more \n",
    "    accurate, up-to-date information. RAG systems typically consist of three main components: a retriever \n",
    "    that finds relevant documents, an embedder that converts text to vectors, and a generator that produces \n",
    "    the final answer based on retrieved context.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Vector embeddings are numerical representations of text that capture semantic meaning. In RAG systems, \n",
    "    both documents and queries are converted into high-dimensional vectors using embedding models like \n",
    "    sentence-transformers or OpenAI's ada-002. The similarity between vectors is typically measured using \n",
    "    cosine similarity or Euclidean distance. Embeddings enable semantic search, where documents can be \n",
    "    retrieved based on meaning rather than exact keyword matches. The quality of embeddings directly \n",
    "    impacts retrieval performance.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Chunking strategies are critical for RAG performance. Documents must be split into smaller segments \n",
    "    that balance context and precision. Fixed-size chunking divides text into equal-length segments, \n",
    "    while recursive character splitting respects natural boundaries like paragraphs and sentences. \n",
    "    Smaller chunks (200-500 tokens) provide precise retrieval but may lack context. Larger chunks \n",
    "    (1000-2000 tokens) preserve context but create noisy embeddings. The optimal chunk size depends \n",
    "    on the document structure and query complexity.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Hypothetical Document Embeddings (HyDE) is an advanced technique that addresses the asymmetry between \n",
    "    queries and documents. Instead of embedding the user's query directly, HyDE first uses an LLM to \n",
    "    generate a hypothetical ideal answer to the query. This generated answer is then embedded and used \n",
    "    for retrieval. Since the hypothetical answer is document-like (verbose, declarative), it better \n",
    "    matches the embedding space of the actual documents, leading to improved retrieval accuracy.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Cross-encoder re-rankers are models that take both a query and a document as input and output a \n",
    "    relevance score. Unlike bi-encoders which create separate embeddings, cross-encoders perform joint \n",
    "    encoding with attention across both texts. This makes them more accurate but slower than bi-encoders. \n",
    "    In a two-stage retrieval pipeline, fast bi-encoders retrieve a broad set of candidates (high recall), \n",
    "    and slower cross-encoders re-rank them for precision. Popular models include bge-rerank-large and \n",
    "    Cohere's rerank endpoint.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Context window optimization is essential for effective RAG. LLMs have token limits (e.g., 4k, 8k, \n",
    "    128k tokens) that constrain how much context can be provided. The 'lost in the middle' problem \n",
    "    shows that LLMs pay more attention to information at the beginning and end of the context. \n",
    "    Strategies include strategic reordering (placing most relevant docs at start/end), extractive \n",
    "    compression (filtering sentences by relevance), and abstractive summarization. Token counting \n",
    "    and dynamic truncation ensure context fits within limits.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Hybrid search combines dense vector search with sparse keyword search (BM25). Dense vectors capture \n",
    "    semantic meaning and handle synonyms well, but struggle with exact matches like acronyms or proper \n",
    "    names. BM25 excels at keyword matching but lacks semantic understanding. Hybrid search runs both \n",
    "    methods in parallel and fuses results using weighted fusion (combining scores) or Reciprocal Rank \n",
    "    Fusion (combining ranks). This approach leverages complementary strengths, improving retrieval \n",
    "    robustness across diverse query types.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Graph-based retrieval (GraphRAG) models knowledge as nodes and relationships rather than text chunks. \n",
    "    Entities are extracted from documents and connected via relationships to form a knowledge graph. \n",
    "    At query time, the system can traverse the graph to perform multi-hop reasoning. For example, to \n",
    "    answer 'Which movies directed by Christopher Nolan starred Michael Caine?', the system finds Nolan's \n",
    "    node, follows 'DIRECTED' edges to movies, then checks for 'STARRED_IN' edges to Caine. GraphRAG \n",
    "    enables complex relational queries that are difficult for vector search alone.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Create Document objects\n",
    "documents = [Document(page_content=doc.strip(), metadata={\"source\": f\"doc_{i}\"}) \n",
    "             for i, doc in enumerate(sample_documents)]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} sample documents\")\n",
    "print(f\"üìä Average document length: {sum(len(d.page_content) for d in documents) / len(documents):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e823705",
   "metadata": {},
   "source": [
    "### Chunk Documents\n",
    "\n",
    "We'll use recursive character splitting to create semantically coherent chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d46078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter with semantic-aware parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,  # Moderate chunk size balancing precision and context\n",
    "    chunk_overlap=50,  # Overlap to maintain continuity\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Prioritize paragraph/sentence boundaries\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "print(f\"üìä Average chunk length: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} characters\")\n",
    "print(f\"\\nüìÑ Sample chunk:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(chunks[0].page_content[:300] + \"...\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc77ea6",
   "metadata": {},
   "source": [
    "### Create Embeddings and Vector Store\n",
    "\n",
    "We'll use HuggingFace's sentence-transformers as our bi-encoder for creating dense vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae4abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model (using a lightweight, high-quality model)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if GPU available\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Normalize for cosine similarity\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded: all-MiniLM-L6-v2\")\n",
    "print(\"üìä Embedding dimension: 384\")\n",
    "\n",
    "# Create vector store with ChromaDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"rag_knowledge_base\",\n",
    "    persist_directory=None  # In-memory for this demo\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store created with {len(chunks)} document chunks\")\n",
    "print(f\"üíæ Vector store type: {type(vectorstore).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bbf9b6",
   "metadata": {},
   "source": [
    "## 3. Baseline Naive RAG Implementation\n",
    "\n",
    "Now let's implement a traditional RAG system that embeds queries directly and retrieves based on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ddd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_rag_retrieval(query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Perform traditional RAG retrieval by directly embedding the user query.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        k: Number of top documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of (Document, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Directly retrieve using the query embedding\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_answer(query: str, context_docs: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the LLM with retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        context_docs: Retrieved relevant documents\n",
    "        \n",
    "    Returns:\n",
    "        Generated answer string\n",
    "    \"\"\"\n",
    "    # Prepare context from retrieved documents\n",
    "    context = \"\\n\\n\".join([f\"[Doc {i+1}]: {doc.page_content}\" \n",
    "                           for i, doc in enumerate(context_docs)])\n",
    "    \n",
    "    # Create prompt template\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI assistant. Use the provided context to answer the user's question accurately and concisely.\"),\n",
    "        (\"human\", \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Generate response\n",
    "    messages = prompt_template.format_messages(context=context, question=query)\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Test the baseline RAG\n",
    "print(\"=\"*80)\n",
    "print(\"üîç BASELINE NAIVE RAG TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_query = \"What are the main challenges with chunking strategies in RAG systems?\"\n",
    "print(f\"\\nüìù Query: {test_query}\\n\")\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retrieved_docs = naive_rag_retrieval(test_query, k=3)\n",
    "\n",
    "print(\"üìö Retrieved Documents:\")\n",
    "for i, (doc, score) in enumerate(retrieved_docs):\n",
    "    print(f\"\\n[{i+1}] Similarity Score: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content[:150]}...\")\n",
    "\n",
    "# Generate answer\n",
    "answer = generate_answer(test_query, [doc for doc, _ in retrieved_docs])\n",
    "print(f\"\\nüí° Generated Answer:\")\n",
    "print(f\"{answer}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01345a",
   "metadata": {},
   "source": [
    "## 4. HyDE Enhancement Implementation\n",
    "\n",
    "Now let's implement the HyDE approach: generate a hypothetical answer first, then use it for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faea80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_document(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a hypothetical ideal answer to the query using an LLM.\n",
    "    This answer will be embedded and used for retrieval.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        \n",
    "    Returns:\n",
    "        Hypothetical document (ideal answer) as a string\n",
    "    \"\"\"\n",
    "    # Create HyDE prompt template\n",
    "    hyde_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert technical writer. Given a question, write a comprehensive, \n",
    "detailed passage that would contain the perfect answer to that question. \n",
    "Write as if you are authoring a technical document or research paper.\n",
    "Be specific, detailed, and authoritative.\"\"\"),\n",
    "        (\"human\", \"Question: {query}\\n\\nPassage:\")\n",
    "    ])\n",
    "    \n",
    "    # Generate hypothetical document\n",
    "    messages = hyde_prompt.format_messages(query=query)\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "\n",
    "def hyde_retrieval(query: str, k: int = 3) -> Tuple[str, List[Tuple[Document, float]]]:\n",
    "    \"\"\"\n",
    "    Perform HyDE-enhanced retrieval:\n",
    "    1. Generate hypothetical document\n",
    "    2. Embed the hypothetical document\n",
    "    3. Retrieve using the hypothetical document embedding\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        k: Number of top documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (hypothetical_doc, retrieved_results)\n",
    "    \"\"\"\n",
    "    # Step 1: Generate hypothetical ideal answer\n",
    "    hypothetical_doc = generate_hypothetical_document(query)\n",
    "    \n",
    "    # Step 2 & 3: Use hypothetical document for retrieval\n",
    "    # The vectorstore will automatically embed the hypothetical doc\n",
    "    results = vectorstore.similarity_search_with_score(hypothetical_doc, k=k)\n",
    "    \n",
    "    return hypothetical_doc, results\n",
    "\n",
    "\n",
    "# Test HyDE retrieval\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ HyDE-ENHANCED RAG TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_query = \"What are the main challenges with chunking strategies in RAG systems?\"\n",
    "print(f\"\\nüìù Query: {test_query}\\n\")\n",
    "\n",
    "# Generate hypothetical document and retrieve\n",
    "hypothetical_doc, retrieved_docs_hyde = hyde_retrieval(test_query, k=3)\n",
    "\n",
    "print(\"ü§ñ Hypothetical Document Generated:\")\n",
    "print(f\"{'-'*70}\")\n",
    "print(f\"{hypothetical_doc}\")\n",
    "print(f\"{'-'*70}\\n\")\n",
    "\n",
    "print(\"üìö Retrieved Documents (using HyDE):\")\n",
    "for i, (doc, score) in enumerate(retrieved_docs_hyde):\n",
    "    print(f\"\\n[{i+1}] Similarity Score: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content[:150]}...\")\n",
    "\n",
    "# Generate final answer\n",
    "answer_hyde = generate_answer(test_query, [doc for doc, _ in retrieved_docs_hyde])\n",
    "print(f\"\\nüí° Generated Answer (HyDE):\")\n",
    "print(f\"{answer_hyde}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8edec",
   "metadata": {},
   "source": [
    "## 5. Comparative Evaluation\n",
    "\n",
    "Let's systematically compare Baseline RAG vs. HyDE across multiple test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define diverse test queries\n",
    "test_queries = [\n",
    "    \"How does semantic search differ from keyword search?\",\n",
    "    \"What is the lost in the middle problem?\",\n",
    "    \"Explain the architecture of hybrid search systems\",\n",
    "    \"Why are cross-encoder models slower but more accurate?\",\n",
    "    \"What role do knowledge graphs play in RAG?\"\n",
    "]\n",
    "\n",
    "def evaluate_retrieval_quality(results: List[Tuple[Document, float]]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate basic retrieval quality metrics.\"\"\"\n",
    "    scores = [score for _, score in results]\n",
    "    return {\n",
    "        \"avg_similarity\": np.mean(scores),\n",
    "        \"max_similarity\": np.max(scores),\n",
    "        \"min_similarity\": np.min(scores),\n",
    "        \"score_variance\": np.var(scores)\n",
    "    }\n",
    "\n",
    "\n",
    "# Run comparative evaluation\n",
    "print(\"=\"*80)\n",
    "print(\"üìä COMPARATIVE EVALUATION: BASELINE vs. HyDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Baseline retrieval\n",
    "    baseline_results = naive_rag_retrieval(query, k=3)\n",
    "    baseline_metrics = evaluate_retrieval_quality(baseline_results)\n",
    "    \n",
    "    # HyDE retrieval\n",
    "    hyde_doc, hyde_results = hyde_retrieval(query, k=3)\n",
    "    hyde_metrics = evaluate_retrieval_quality(hyde_results)\n",
    "    \n",
    "    # Compare\n",
    "    print(f\"\\nüìà Retrieval Quality Metrics:\")\n",
    "    print(f\"{'Metric':<20} {'Baseline':>12} {'HyDE':>12} {'Improvement':>12}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    for metric in baseline_metrics:\n",
    "        baseline_val = baseline_metrics[metric]\n",
    "        hyde_val = hyde_metrics[metric]\n",
    "        improvement = ((hyde_val - baseline_val) / baseline_val * 100) if baseline_val != 0 else 0\n",
    "        \n",
    "        print(f\"{metric:<20} {baseline_val:>12.4f} {hyde_val:>12.4f} {improvement:>11.1f}%\")\n",
    "    \n",
    "    # Store for summary\n",
    "    comparison_results.append({\n",
    "        \"query\": query,\n",
    "        \"baseline\": baseline_metrics,\n",
    "        \"hyde\": hyde_metrics\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìä SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Calculate overall improvement\n",
    "avg_baseline_sim = np.mean([r[\"baseline\"][\"avg_similarity\"] for r in comparison_results])\n",
    "avg_hyde_sim = np.mean([r[\"hyde\"][\"avg_similarity\"] for r in comparison_results])\n",
    "overall_improvement = ((avg_hyde_sim - avg_baseline_sim) / avg_baseline_sim * 100)\n",
    "\n",
    "print(f\"\\nAverage Similarity Score:\")\n",
    "print(f\"  Baseline: {avg_baseline_sim:.4f}\")\n",
    "print(f\"  HyDE:     {avg_hyde_sim:.4f}\")\n",
    "print(f\"  Improvement: {overall_improvement:+.1f}%\")\n",
    "\n",
    "print(f\"\\n‚ú® HyDE shows {'superior' if overall_improvement > 0 else 'inferior'} performance overall!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed9632",
   "metadata": {},
   "source": [
    "### When Does HyDE Excel?\n",
    "\n",
    "Let's analyze specific cases where HyDE significantly outperforms baseline retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82141807",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ ANALYZING HYDE ADVANTAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test cases where HyDE should excel\n",
    "edge_cases = [\n",
    "    {\n",
    "        \"query\": \"Why?\",  # Very short, ambiguous query\n",
    "        \"context\": \"Short, vague queries\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"cross encoder vs bi encoder performance\",  # Keyword-heavy\n",
    "        \"context\": \"Keyword-style queries\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the problem with putting important info in the middle of long contexts?\",  # Conversational\n",
    "        \"context\": \"Conversational, verbose queries\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for case in edge_cases:\n",
    "    query = case[\"query\"]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìù Query: '{query}'\")\n",
    "    print(f\"üìç Context: {case['context']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_results = naive_rag_retrieval(query, k=2)\n",
    "    print(f\"\\nüîµ Baseline Top Result:\")\n",
    "    print(f\"  Score: {baseline_results[0][1]:.4f}\")\n",
    "    print(f\"  Content: {baseline_results[0][0].page_content[:100]}...\")\n",
    "    \n",
    "    # HyDE\n",
    "    hyde_doc, hyde_results = hyde_retrieval(query, k=2)\n",
    "    print(f\"\\nüü¢ HyDE Top Result:\")\n",
    "    print(f\"  Score: {hyde_results[0][1]:.4f}\")\n",
    "    print(f\"  Content: {hyde_results[0][0].page_content[:100]}...\")\n",
    "    \n",
    "    improvement = ((hyde_results[0][1] - baseline_results[0][1]) / baseline_results[0][1] * 100)\n",
    "    print(f\"\\nüìä Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "HyDE typically excels in these scenarios:\n",
    "1. ‚úÖ Short, ambiguous queries ‚Üí HyDE expands them into detailed passages\n",
    "2. ‚úÖ Keyword-heavy queries ‚Üí HyDE transforms into natural, semantic text\n",
    "3. ‚úÖ Conceptual questions ‚Üí HyDE bridges the semantic gap better\n",
    "4. ‚úÖ Domain-specific terminology ‚Üí LLM can use appropriate technical language\n",
    "\n",
    "HyDE may struggle when:\n",
    "1. ‚ùå Query already contains perfect keywords from documents\n",
    "2. ‚ùå LLM's knowledge is outdated or incorrect for the domain\n",
    "3. ‚ùå Additional latency is unacceptable (HyDE adds one extra LLM call)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a97158",
   "metadata": {},
   "source": [
    "## 6. Advanced HyDE Variations\n",
    "\n",
    "Let's explore more sophisticated HyDE techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47d3da",
   "metadata": {},
   "source": [
    "### Multi-Perspective HyDE\n",
    "\n",
    "Generate multiple hypothetical documents from different perspectives to increase retrieval robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_perspective_hypotheticals(query: str, num_perspectives: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate multiple hypothetical documents from different perspectives.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        num_perspectives: Number of different hypothetical documents to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of hypothetical documents\n",
    "    \"\"\"\n",
    "    perspectives = [\n",
    "        \"Write a technical, detailed explanation\",\n",
    "        \"Write a concise, beginner-friendly summary\",\n",
    "        \"Write from a practical, implementation-focused perspective\"\n",
    "    ]\n",
    "    \n",
    "    hypotheticals = []\n",
    "    \n",
    "    for i in range(num_perspectives):\n",
    "        perspective = perspectives[i] if i < len(perspectives) else perspectives[0]\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"You are an expert technical writer. {perspective} that would \n",
    "answer the following question. Be specific and authoritative.\"\"\"),\n",
    "            (\"human\", \"Question: {query}\\n\\nPassage:\")\n",
    "        ])\n",
    "        \n",
    "        messages = prompt.format_messages(query=query)\n",
    "        response = llm.invoke(messages)\n",
    "        hypotheticals.append(response.content)\n",
    "    \n",
    "    return hypotheticals\n",
    "\n",
    "\n",
    "def multi_perspective_hyde_retrieval(query: str, k: int = 3, num_perspectives: int = 3) -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Perform retrieval using multiple hypothetical documents and merge results.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        k: Number of documents to retrieve per perspective\n",
    "        num_perspectives: Number of hypothetical documents to generate\n",
    "        \n",
    "    Returns:\n",
    "        Merged and deduplicated list of retrieved documents with scores\n",
    "    \"\"\"\n",
    "    # Generate multiple hypothetical documents\n",
    "    hypotheticals = generate_multi_perspective_hypotheticals(query, num_perspectives)\n",
    "    \n",
    "    # Retrieve using each hypothetical document\n",
    "    all_results = {}  # Use dict to deduplicate by document ID\n",
    "    \n",
    "    for hyde_doc in hypotheticals:\n",
    "        results = vectorstore.similarity_search_with_score(hyde_doc, k=k)\n",
    "        \n",
    "        for doc, score in results:\n",
    "            doc_id = doc.page_content  # Use content as ID\n",
    "            # Keep the best (lowest) score for each unique document\n",
    "            if doc_id not in all_results or score < all_results[doc_id][1]:\n",
    "                all_results[doc_id] = (doc, score)\n",
    "    \n",
    "    # Sort by score and return top k\n",
    "    sorted_results = sorted(all_results.values(), key=lambda x: x[1])\n",
    "    return sorted_results[:k]\n",
    "\n",
    "\n",
    "# Test multi-perspective HyDE\n",
    "print(\"=\"*80)\n",
    "print(\"üîç MULTI-PERSPECTIVE HyDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_query = \"What are embedding models and why are they important?\"\n",
    "\n",
    "print(f\"\\nüìù Query: {test_query}\\n\")\n",
    "\n",
    "# Generate hypothetical documents\n",
    "hypotheticals = generate_multi_perspective_hypotheticals(test_query, 3)\n",
    "\n",
    "print(\"ü§ñ Generated Hypothetical Documents:\")\n",
    "for i, hyp in enumerate(hypotheticals, 1):\n",
    "    print(f\"\\n[Perspective {i}]\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"{hyp[:200]}...\")\n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "# Retrieve using multi-perspective approach\n",
    "multi_results = multi_perspective_hyde_retrieval(test_query, k=3, num_perspectives=3)\n",
    "\n",
    "print(f\"\\nüìö Final Merged Results (Top 3):\")\n",
    "for i, (doc, score) in enumerate(multi_results, 1):\n",
    "    print(f\"\\n[{i}] Score: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° Multi-Perspective Advantages:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "‚úÖ Increased Retrieval Robustness: Multiple views increase chance of finding relevant docs\n",
    "‚úÖ Better Coverage: Different perspectives may retrieve complementary information\n",
    "‚úÖ Reduced False Negatives: If one perspective fails, others may succeed\n",
    "‚ùå Higher Latency: Requires multiple LLM calls for generation\n",
    "‚ùå More Complex: Result fusion and deduplication needed\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246bbbaf",
   "metadata": {},
   "source": [
    "### Domain-Specific HyDE Prompt Engineering\n",
    "\n",
    "Customize HyDE prompts for different domains to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-specific prompt templates\n",
    "DOMAIN_PROMPTS = {\n",
    "    \"technical\": \"\"\"You are a senior software engineer and technical architect. \n",
    "Write a detailed technical documentation passage that answers the question below. \n",
    "Include specific terminology, architectural patterns, and implementation details.\"\"\",\n",
    "    \n",
    "    \"scientific\": \"\"\"You are a research scientist writing for a peer-reviewed journal. \n",
    "Write an academic passage with precise definitions, citations to methodologies, \n",
    "and evidence-based explanations that addresses the question below.\"\"\",\n",
    "    \n",
    "    \"business\": \"\"\"You are a business analyst and consultant. Write a clear, \n",
    "actionable passage that answers the question from a business perspective, \n",
    "including ROI considerations, strategic implications, and practical applications.\"\"\",\n",
    "    \n",
    "    \"tutorial\": \"\"\"You are an experienced educator and technical trainer. \n",
    "Write a step-by-step, beginner-friendly passage that explains the answer \n",
    "to the question below with examples and clear explanations.\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "def domain_aware_hyde(query: str, domain: str = \"technical\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a hypothetical document tailored to a specific domain.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        domain: Domain type ('technical', 'scientific', 'business', 'tutorial')\n",
    "        \n",
    "    Returns:\n",
    "        Domain-specific hypothetical document\n",
    "    \"\"\"\n",
    "    system_prompt = DOMAIN_PROMPTS.get(domain, DOMAIN_PROMPTS[\"technical\"])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Question: {query}\\n\\nPassage:\")\n",
    "    ])\n",
    "    \n",
    "    messages = prompt.format_messages(query=query)\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Demonstrate domain-specific HyDE\n",
    "print(\"=\"*80)\n",
    "print(\"üé® DOMAIN-SPECIFIC HyDE EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_query = \"What are the benefits of using hybrid search in RAG systems?\"\n",
    "\n",
    "domains = [\"technical\", \"business\", \"tutorial\"]\n",
    "\n",
    "for domain in domains:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üè∑Ô∏è  Domain: {domain.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    hyde_doc = domain_aware_hyde(test_query, domain)\n",
    "    print(f\"\\n{hyde_doc[:300]}...\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üí° Domain-Specific Prompt Engineering Benefits:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "‚úÖ Better Semantic Alignment: Matches the writing style of your knowledge base\n",
    "‚úÖ Improved Terminology: Uses domain-appropriate vocabulary\n",
    "‚úÖ Enhanced Relevance: LLM generates content that better matches document style\n",
    "‚úÖ Flexibility: Can adapt to different sections of a heterogeneous knowledge base\n",
    "\n",
    "Best Practice: Analyze your knowledge base's writing style and adapt prompts accordingly!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337b254",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Conclusions\n",
    "\n",
    "Let's summarize our findings and provide actionable recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13bc240",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä FINAL ANALYSIS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "## Key Findings\n",
    "\n",
    "### 1. Performance Comparison\n",
    "\n",
    "Baseline RAG:\n",
    "- ‚úÖ Fast: Direct query embedding (no extra LLM call)\n",
    "- ‚úÖ Simple: Straightforward implementation\n",
    "- ‚ùå Query-Document Mismatch: Struggles with semantic gaps\n",
    "- ‚ùå Keyword-Dependent: Poor on vague or conversational queries\n",
    "\n",
    "HyDE-Enhanced RAG:\n",
    "- ‚úÖ Better Semantic Alignment: Transforms query to document-like text\n",
    "- ‚úÖ Improved Retrieval Quality: Higher similarity scores on average\n",
    "- ‚úÖ Robust to Query Variations: Handles short, vague queries better\n",
    "- ‚ùå Additional Latency: +1 LLM call (~200-500ms overhead)\n",
    "- ‚ùå Token Costs: Extra tokens for hypothetical document generation\n",
    "\n",
    "### 2. When to Use HyDE\n",
    "\n",
    "‚úÖ RECOMMENDED FOR:\n",
    "- Complex, conceptual queries requiring semantic understanding\n",
    "- Short or ambiguous user inputs that need expansion\n",
    "- Domain-specific knowledge bases with technical terminology\n",
    "- Systems where accuracy is more critical than speed\n",
    "- B2B applications with sophisticated users\n",
    "\n",
    "‚ùå NOT RECOMMENDED FOR:\n",
    "- Simple keyword-based lookups (e.g., \"Find document about X\")\n",
    "- Real-time systems with strict latency requirements (<100ms)\n",
    "- Cost-sensitive applications with high query volumes\n",
    "- Knowledge bases where LLM may lack domain expertise\n",
    "\n",
    "### 3. Optimization Strategies\n",
    "\n",
    "üîß Hybrid Approach:\n",
    "- Use query classification to route simple queries to baseline\n",
    "- Apply HyDE only for complex queries detected by heuristics\n",
    "- Example heuristic: Query length < 5 words ‚Üí baseline, else ‚Üí HyDE\n",
    "\n",
    "üîß Caching:\n",
    "- Cache hypothetical documents for frequently asked questions\n",
    "- Reduce redundant LLM calls for similar queries\n",
    "\n",
    "üîß Async Processing:\n",
    "- Generate hypothetical document while performing baseline retrieval\n",
    "- Combine results from both approaches for best coverage\n",
    "\n",
    "### 4. Trade-off Analysis\n",
    "\n",
    "| Dimension       | Baseline | HyDE | Winner |\n",
    "|----------------|----------|------|--------|\n",
    "| Speed          | ‚ö°‚ö°‚ö°    | ‚ö°‚ö°   | Baseline |\n",
    "| Accuracy       | ‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê  | HyDE |\n",
    "| Cost           | üí∞       | üí∞üí∞  | Baseline |\n",
    "| Simplicity     | ‚úÖ‚úÖ‚úÖ    | ‚úÖ‚úÖ   | Baseline |\n",
    "| Robustness     | ‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê  | HyDE |\n",
    "\n",
    "### 5. Production Best Practices\n",
    "\n",
    "1Ô∏è‚É£ Start Simple: Implement baseline first, measure performance\n",
    "2Ô∏è‚É£ A/B Test: Compare baseline vs HyDE on representative queries\n",
    "3Ô∏è‚É£ Monitor Metrics: Track latency, cost, and user satisfaction\n",
    "4Ô∏è‚É£ Iterate: Fine-tune prompts based on failure analysis\n",
    "5Ô∏è‚É£ Consider Hybrid: Use query classification for intelligent routing\n",
    "\n",
    "### 6. Next Steps in Your RAG Journey\n",
    "\n",
    "After mastering HyDE, explore:\n",
    "- üîÑ Multi-Query Decomposition (Demo #2)\n",
    "- üîç Hybrid Search with BM25 (Demo #3)\n",
    "- üèóÔ∏è Hierarchical Retrieval (Demo #4)\n",
    "- üéØ Cross-Encoder Re-ranking (Demo #5)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41fcd1a",
   "metadata": {},
   "source": [
    "## üéì Summary and Key Takeaways\n",
    "\n",
    "**Congratulations!** You've successfully implemented HyDE-enhanced RAG and compared it against baseline RAG.\n",
    "\n",
    "### What You Learned:\n",
    "1. ‚úÖ The **query-document asymmetry problem** and why it matters\n",
    "2. ‚úÖ How to implement **baseline Naive RAG** with direct query embedding\n",
    "3. ‚úÖ How to build **HyDE-enhanced RAG** with hypothetical document generation\n",
    "4. ‚úÖ Comparative evaluation methodologies for RAG systems\n",
    "5. ‚úÖ Advanced HyDE variations: multi-perspective and domain-specific approaches\n",
    "\n",
    "### Key Insights:\n",
    "- HyDE transforms retrieval from query‚Üídocument to answer‚Üíanswer similarity\n",
    "- The technique adds latency but significantly improves semantic alignment\n",
    "- Domain-specific prompt engineering can further enhance HyDE performance\n",
    "- Multi-perspective HyDE increases robustness at the cost of complexity\n",
    "\n",
    "### Production Considerations:\n",
    "- Measure the latency-accuracy trade-off for your specific use case\n",
    "- Consider hybrid approaches that route queries intelligently\n",
    "- Monitor token costs and implement caching strategies\n",
    "- Continuously evaluate and iterate on prompt templates\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References and Further Reading\n",
    "\n",
    "1. **HyDE Paper**: \"Precise Zero-Shot Dense Retrieval without Relevance Labels\" (Gao et al., 2022)\n",
    "2. **LangChain Documentation**: https://python.langchain.com/docs/\n",
    "3. **ChromaDB**: https://docs.trychroma.com/\n",
    "4. **Sentence Transformers**: https://www.sbert.net/\n",
    "\n",
    "---\n",
    "\n",
    "**Next Demo**: Multi-Query and Sub-Query Decomposition (Demo #2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
