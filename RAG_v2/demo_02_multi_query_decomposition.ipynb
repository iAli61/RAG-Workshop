{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f8ad10",
   "metadata": {},
   "source": [
    "# Demo #2: Multi-Query Decomposition - Complex Query Handling\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo demonstrates how **Sub-Query Decomposition** enables comprehensive answers to complex, multi-faceted questions by breaking them into simpler sub-queries and aggregating the results.\n",
    "\n",
    "### Core Concepts:\n",
    "- **Sub-Query Decomposition**: Breaking complex queries into manageable pieces\n",
    "- **Parallel Retrieval**: Executing multiple searches independently\n",
    "- **Context Aggregation**: Synthesizing information from multiple sources\n",
    "- **Multi-Hop Reasoning**: Answering questions requiring multiple pieces of information\n",
    "\n",
    "### Why Sub-Query Decomposition Works:\n",
    "Complex questions often require information from multiple sources or perspectives:\n",
    "- \"Compare X and Y\" requires retrieving information about both X and Y separately\n",
    "- \"What are the advantages and disadvantages...\" needs retrieval focused on pros and cons\n",
    "- Single-query approaches may retrieve incomplete or biased information\n",
    "\n",
    "Sub-Query Decomposition solves this by:\n",
    "1. Using an LLM to break the complex query into focused sub-questions\n",
    "2. Retrieving relevant context for each sub-question independently\n",
    "3. Aggregating all retrieved information\n",
    "4. Synthesizing a comprehensive answer from the complete context\n",
    "\n",
    "### Demo Structure:\n",
    "1. Setup and data ingestion\n",
    "2. Test baseline single-query approach\n",
    "3. Implement sub-query decomposition\n",
    "4. Compare results and coverage\n",
    "5. Analyze multi-hop reasoning capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e5099",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8264e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell only once\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# LlamaIndex core components\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "# Azure OpenAI components\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9457c0f9",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0297bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "llm_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "\n",
    "# Validate configuration\n",
    "if not all([api_key, azure_endpoint, llm_deployment, embedding_deployment]):\n",
    "    raise ValueError(\"Missing required Azure OpenAI configuration. Check your .env file.\")\n",
    "\n",
    "print(\"âœ“ Azure OpenAI configuration loaded\")\n",
    "print(f\"  Endpoint: {azure_endpoint}\")\n",
    "print(f\"  LLM Deployment: {llm_deployment}\")\n",
    "print(f\"  Embedding Deployment: {embedding_deployment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756783dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI LLM\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=llm_deployment,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI Embedding Model\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=embedding_deployment,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"âœ“ Azure OpenAI models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55fd80",
   "metadata": {},
   "source": [
    "## 3. Load and Process Documents\n",
    "\n",
    "We'll use the same ML concepts knowledge base from Demo #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = Path(\"./data/ml_concepts\")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=str(data_dir),\n",
    "    required_exts=['.md']\n",
    ").load_data()\n",
    "\n",
    "print(f\"âœ“ Loaded {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    filename = Path(doc.metadata.get('file_name', 'unknown')).stem\n",
    "    print(f\"  - {filename} ({len(doc.text)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559748b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter and parse documents\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"âœ“ Created {len(nodes)} text chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    embed_model=azure_embed\n",
    ")\n",
    "\n",
    "print(\"âœ“ Vector index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dff05e",
   "metadata": {},
   "source": [
    "## 4. Baseline: Single-Query Approach\n",
    "\n",
    "First, let's test a complex query that requires comparing multiple algorithms using a standard query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline query engine\n",
    "baseline_engine = index.as_query_engine(\n",
    "    llm=azure_llm,\n",
    "    similarity_top_k=3  # Retrieve top 3 chunks\n",
    ")\n",
    "\n",
    "print(\"âœ“ Baseline query engine created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d95184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a complex, multi-faceted query\n",
    "complex_query = \"Compare the strengths and weaknesses of gradient boosting and random forests for classification tasks. When should I use each one?\"\n",
    "\n",
    "print(f\"Complex Query: {complex_query}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fde835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute baseline query\n",
    "baseline_response = baseline_engine.query(complex_query)\n",
    "\n",
    "print(\"\\nğŸ“Š BASELINE APPROACH (Single Query)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nAnswer:\\n{baseline_response.response}\")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"\\nRetrieved Chunks:\")\n",
    "for i, node in enumerate(baseline_response.source_nodes, 1):\n",
    "    print(f\"\\n[Chunk {i}] Score: {node.score:.4f} | Source: {Path(node.metadata.get('file_name', 'unknown')).stem}\")\n",
    "    print(f\"{node.text[:250]}...\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f8abc",
   "metadata": {},
   "source": [
    "## 5. Implement Sub-Query Decomposition\n",
    "\n",
    "Now let's use the `SubQuestionQueryEngine` to automatically decompose the complex query into sub-questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f655a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine tool\n",
    "# This wraps our index with metadata describing what it contains\n",
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=index.as_query_engine(similarity_top_k=3),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"ml_algorithms\",\n",
    "        description=\"Comprehensive knowledge base about machine learning algorithms including gradient boosting, random forests, neural networks, support vector machines, and k-means clustering. Contains information about how they work, their advantages, disadvantages, and use cases.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"âœ“ Query engine tool created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SubQuestionQueryEngine\n",
    "subquestion_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[query_engine_tool],\n",
    "    llm=azure_llm,\n",
    "    verbose=True,  # This will show us the sub-questions being generated\n",
    "    use_async=False\n",
    ")\n",
    "\n",
    "print(\"âœ“ Sub-question query engine created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1016312",
   "metadata": {},
   "source": [
    "## 6. Execute Sub-Query Decomposition\n",
    "\n",
    "Watch how the engine automatically breaks down our complex query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0abdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the same complex query with sub-question decomposition\n",
    "print(\"\\nğŸš€ SUB-QUESTION DECOMPOSITION APPROACH\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nOriginal Complex Query: {complex_query}\")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"\\nGenerating and executing sub-questions...\\n\")\n",
    "\n",
    "subquestion_response = subquestion_engine.query(complex_query)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"\\nâœ¨ SYNTHESIZED ANSWER:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n{subquestion_response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d89ab",
   "metadata": {},
   "source": [
    "## 7. Analyze Sub-Questions Generated\n",
    "\n",
    "Let's examine what sub-questions were automatically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display sub-questions from the response metadata\n",
    "print(\"\\nğŸ“‹ GENERATED SUB-QUESTIONS ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if hasattr(subquestion_response, 'metadata') and 'sub_qa' in subquestion_response.metadata:\n",
    "    sub_qa_pairs = subquestion_response.metadata['sub_qa']\n",
    "    \n",
    "    for i, (sub_q, sub_a) in enumerate(sub_qa_pairs, 1):\n",
    "        print(f\"\\n[Sub-Question {i}]\")\n",
    "        print(f\"Question: {sub_q.sub_q.sub_question}\")\n",
    "        print(f\"Tool Used: {sub_q.sub_q.tool_name}\")\n",
    "        print(f\"\\nAnswer: {sub_a.response[:300]}...\")\n",
    "        print(\"-\" * 100)\n",
    "else:\n",
    "    print(\"Sub-question metadata not available in this response format.\")\n",
    "    print(\"However, you should have seen the sub-questions in the verbose output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f230690",
   "metadata": {},
   "source": [
    "## 8. Side-by-Side Comparison\n",
    "\n",
    "Let's compare the coverage and quality of both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(baseline_resp, subq_resp, query):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"ğŸ“Š DETAILED COMPARISON: BASELINE vs. SUB-QUESTION DECOMPOSITION\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    print(f\"\\nOriginal Query: {query}\")\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "    \n",
    "    # Compare answer lengths\n",
    "    print(\"\\n1. ANSWER LENGTH COMPARISON\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Baseline answer: {len(baseline_resp.response)} characters\")\n",
    "    print(f\"Sub-question answer: {len(subq_resp.response)} characters\")\n",
    "    \n",
    "    # Compare source coverage\n",
    "    print(\"\\n2. SOURCE COVERAGE COMPARISON\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    baseline_sources = set([Path(n.metadata.get('file_name', 'unknown')).stem for n in baseline_resp.source_nodes])\n",
    "    print(f\"\\nBaseline sources ({len(baseline_sources)}): {baseline_sources}\")\n",
    "    print(f\"Baseline retrieved {len(baseline_resp.source_nodes)} chunks total\")\n",
    "    \n",
    "    # For sub-question engine, sources are distributed across sub-queries\n",
    "    subq_sources = set([Path(n.metadata.get('file_name', 'unknown')).stem for n in subq_resp.source_nodes])\n",
    "    print(f\"\\nSub-question sources ({len(subq_sources)}): {subq_sources}\")\n",
    "    print(f\"Sub-question retrieved {len(subq_resp.source_nodes)} chunks total\")\n",
    "    \n",
    "    # Identify unique sources\n",
    "    only_baseline = baseline_sources - subq_sources\n",
    "    only_subq = subq_sources - baseline_sources\n",
    "    shared = baseline_sources & subq_sources\n",
    "    \n",
    "    print(f\"\\nShared sources: {shared}\")\n",
    "    if only_baseline:\n",
    "        print(f\"Only in baseline: {only_baseline}\")\n",
    "    if only_subq:\n",
    "        print(f\"Only in sub-question: {only_subq}\")\n",
    "    \n",
    "    # Compare answers\n",
    "    print(\"\\n3. GENERATED ANSWERS\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"\\nğŸ“Š BASELINE ANSWER:\\n{baseline_resp.response}\")\n",
    "    print(f\"\\n{'-' * 100}\")\n",
    "    print(f\"\\nğŸš€ SUB-QUESTION ANSWER:\\n{subq_resp.response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Run comparison\n",
    "compare_responses(baseline_response, subquestion_response, complex_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3585c8",
   "metadata": {},
   "source": [
    "## 9. Test with Additional Complex Queries\n",
    "\n",
    "Let's test with more queries that require multi-hop reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional complex queries\n",
    "additional_queries = [\n",
    "    \"Which algorithms are better for high-dimensional data, and what are their computational trade-offs?\",\n",
    "    \"How do supervised learning algorithms differ in their approaches to finding decision boundaries, and which is most interpretable?\",\n",
    "    \"What are the key differences between algorithms that handle outliers well versus those that don't?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ”¬ ADDITIONAL COMPLEX QUERY TESTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, query in enumerate(additional_queries[:1], 1):  # Test just one to save time\n",
    "    print(f\"\\n\\n{'=' * 100}\")\n",
    "    print(f\"Test Query {i}: {query}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Baseline\n",
    "    print(\"\\nğŸ“Š Baseline Approach:\")\n",
    "    baseline_resp = baseline_engine.query(query)\n",
    "    print(f\"Sources: {[Path(n.metadata.get('file_name', 'unknown')).stem for n in baseline_resp.source_nodes]}\")\n",
    "    print(f\"Answer length: {len(baseline_resp.response)} characters\")\n",
    "    \n",
    "    # Sub-question\n",
    "    print(f\"\\nğŸš€ Sub-Question Approach:\")\n",
    "    print(\"(Watch for sub-questions being generated...)\\n\")\n",
    "    subq_resp = subquestion_engine.query(query)\n",
    "    print(f\"\\nSources: {[Path(n.metadata.get('file_name', 'unknown')).stem for n in subq_resp.source_nodes]}\")\n",
    "    print(f\"Answer length: {len(subq_resp.response)} characters\")\n",
    "    \n",
    "    print(f\"\\nâœ¨ Sub-Question Answer:\\n{subq_resp.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6547d11",
   "metadata": {},
   "source": [
    "## 10. Visualize the Data Flow\n",
    "\n",
    "Let's create a simple visualization of how sub-query decomposition works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    DATA FLOW: SUB-QUERY DECOMPOSITION                          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "BASELINE APPROACH (Single Query):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "  Complex Query\n",
    "       â”‚\n",
    "       â–¼\n",
    "  Embed Query\n",
    "       â”‚\n",
    "       â–¼\n",
    "  Vector Search â”€â”€â”€â”€â–º Retrieve Top-K Chunks (may miss important context)\n",
    "       â”‚\n",
    "       â–¼\n",
    "  LLM Generation\n",
    "       â”‚\n",
    "       â–¼\n",
    "  Final Answer (limited by single retrieval pass)\n",
    "\n",
    "\n",
    "SUB-QUESTION DECOMPOSITION APPROACH:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "  Complex Query\n",
    "       â”‚\n",
    "       â–¼\n",
    "  LLM Decomposes â”€â”€â”€â”€â–º Sub-Q1: \"What are strengths of gradient boosting?\"\n",
    "       â”‚              Sub-Q2: \"What are weaknesses of gradient boosting?\"\n",
    "       â”‚              Sub-Q3: \"What are strengths of random forests?\"\n",
    "       â”‚              Sub-Q4: \"What are weaknesses of random forests?\"\n",
    "       â”‚              Sub-Q5: \"When to use each algorithm?\"\n",
    "       â–¼\n",
    "  Parallel Retrieval:\n",
    "       â”œâ”€â”€â”€â”€â”€â–º Sub-Q1 â†’ Vector Search â†’ Chunks about GB strengths\n",
    "       â”œâ”€â”€â”€â”€â”€â–º Sub-Q2 â†’ Vector Search â†’ Chunks about GB weaknesses\n",
    "       â”œâ”€â”€â”€â”€â”€â–º Sub-Q3 â†’ Vector Search â†’ Chunks about RF strengths\n",
    "       â”œâ”€â”€â”€â”€â”€â–º Sub-Q4 â†’ Vector Search â†’ Chunks about RF weaknesses\n",
    "       â””â”€â”€â”€â”€â”€â–º Sub-Q5 â†’ Vector Search â†’ Chunks about use cases\n",
    "       â”‚\n",
    "       â–¼\n",
    "  Aggregate All Retrieved Contexts (comprehensive coverage)\n",
    "       â”‚\n",
    "       â–¼\n",
    "  LLM Synthesis\n",
    "       â”‚\n",
    "       â–¼\n",
    "  Final Answer (comprehensive, balanced, multi-faceted)\n",
    "\n",
    "\n",
    "KEY ADVANTAGES:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Each sub-question targets specific information\n",
    "âœ“ Multiple retrieval passes increase coverage\n",
    "âœ“ Balanced information from different sources\n",
    "âœ“ Better handling of comparative queries\n",
    "âœ“ Reduced chance of missing critical context\n",
    "\n",
    "TRADE-OFFS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ Higher latency (multiple LLM calls + multiple retrievals)\n",
    "â€¢ Increased token usage and cost\n",
    "â€¢ More complex pipeline to debug\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3299a61a",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways and Best Practices\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Complex queries benefit from decomposition**: Questions requiring comparisons, multi-faceted analysis, or information synthesis are better served by breaking them into sub-questions.\n",
    "\n",
    "2. **Increased coverage**: Sub-query decomposition retrieves from multiple focused searches, resulting in more comprehensive context.\n",
    "\n",
    "3. **Better balance**: For comparative queries (\"X vs Y\"), decomposition ensures both sides get equal attention in retrieval.\n",
    "\n",
    "4. **Automatic planning**: The LLM automatically determines what sub-questions are needed - no manual query engineering required.\n",
    "\n",
    "### When to Use Sub-Query Decomposition:\n",
    "\n",
    "âœ… **Use when:**\n",
    "- Query contains \"compare\", \"contrast\", \"differences\"\n",
    "- Question has multiple parts or facets\n",
    "- Requires synthesizing information from different sources\n",
    "- Multi-hop reasoning is needed\n",
    "- Comprehensive coverage is more important than speed\n",
    "\n",
    "âŒ **Avoid when:**\n",
    "- Simple, focused queries\n",
    "- Real-time latency requirements\n",
    "- Limited API budget\n",
    "- Single-source answers are sufficient\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Tool metadata matters**: Provide clear, descriptive metadata for query engine tools so the LLM can generate relevant sub-questions.\n",
    "\n",
    "2. **Use verbose mode during development**: See what sub-questions are being generated to debug and optimize.\n",
    "\n",
    "3. **Consider async execution**: Set `use_async=True` to parallelize sub-question retrieval (reduces latency).\n",
    "\n",
    "4. **Balance top-k per sub-query**: Lower top-k per sub-question (e.g., 2-3) since you're doing multiple retrievals.\n",
    "\n",
    "5. **Monitor costs**: Each sub-question is an additional LLM call and retrieval operation.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In Demo #3, we'll explore **Hybrid Search**, combining dense vector search with sparse keyword retrieval (BM25) to improve precision across different query types."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
