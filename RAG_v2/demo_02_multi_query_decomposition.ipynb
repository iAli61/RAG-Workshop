{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f8ad10",
   "metadata": {},
   "source": [
    "# Demo #2: Multi-Query Decomposition - Complex Query Handling\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo demonstrates how **Sub-Query Decomposition** enables comprehensive answers to complex, multi-faceted questions by breaking them into simpler sub-queries and aggregating the results.\n",
    "\n",
    "### Core Concepts:\n",
    "- **Sub-Query Decomposition**: Breaking complex queries into manageable pieces\n",
    "- **Parallel Retrieval**: Executing multiple searches independently\n",
    "- **Context Aggregation**: Synthesizing information from multiple sources\n",
    "- **Multi-Hop Reasoning**: Answering questions requiring multiple pieces of information\n",
    "\n",
    "### Why Sub-Query Decomposition Works:\n",
    "Complex questions often require information from multiple sources or perspectives:\n",
    "- \"Compare X and Y\" requires retrieving information about both X and Y separately\n",
    "- \"What are the advantages and disadvantages...\" needs retrieval focused on pros and cons\n",
    "- Single-query approaches may retrieve incomplete or biased information\n",
    "\n",
    "Sub-Query Decomposition solves this by:\n",
    "1. Using an LLM to break the complex query into focused sub-questions\n",
    "2. Retrieving relevant context for each sub-question independently\n",
    "3. Aggregating all retrieved information\n",
    "4. Synthesizing a comprehensive answer from the complete context\n",
    "\n",
    "### Demo Structure:\n",
    "1. Setup and data ingestion\n",
    "2. Test baseline single-query approach\n",
    "3. Implement sub-query decomposition\n",
    "4. Compare results and coverage\n",
    "5. Analyze multi-hop reasoning capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e5099",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8264e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell only once\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# LlamaIndex core components\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "# Azure OpenAI components\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9457c0f9",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0297bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "llm_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "\n",
    "# Validate configuration\n",
    "if not all([api_key, azure_endpoint, llm_deployment, embedding_deployment]):\n",
    "    raise ValueError(\"Missing required Azure OpenAI configuration. Check your .env file.\")\n",
    "\n",
    "print(\"✓ Azure OpenAI configuration loaded\")\n",
    "print(f\"  Endpoint: {azure_endpoint}\")\n",
    "print(f\"  LLM Deployment: {llm_deployment}\")\n",
    "print(f\"  Embedding Deployment: {embedding_deployment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756783dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI LLM\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=llm_deployment,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI Embedding Model\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=embedding_deployment,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"✓ Azure OpenAI models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55fd80",
   "metadata": {},
   "source": [
    "## 3. Load and Process Documents\n",
    "\n",
    "We'll use the same ML concepts knowledge base from Demo #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = Path(\"./data/ml_concepts\")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=str(data_dir),\n",
    "    required_exts=['.md']\n",
    ").load_data()\n",
    "\n",
    "print(f\"✓ Loaded {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    filename = Path(doc.metadata.get('file_name', 'unknown')).stem\n",
    "    print(f\"  - {filename} ({len(doc.text)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559748b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter and parse documents\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"✓ Created {len(nodes)} text chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    embed_model=azure_embed\n",
    ")\n",
    "\n",
    "print(\"✓ Vector index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dff05e",
   "metadata": {},
   "source": [
    "## 4. Baseline: Single-Query Approach\n",
    "\n",
    "First, let's test a complex query that requires comparing multiple algorithms using a standard query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline query engine\n",
    "baseline_engine = index.as_query_engine(\n",
    "    llm=azure_llm,\n",
    "    similarity_top_k=3  # Retrieve top 3 chunks\n",
    ")\n",
    "\n",
    "print(\"✓ Baseline query engine created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d95184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a complex, multi-faceted query\n",
    "complex_query = \"Compare the strengths and weaknesses of gradient boosting and random forests for classification tasks. When should I use each one?\"\n",
    "\n",
    "print(f\"Complex Query: {complex_query}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fde835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute baseline query\n",
    "baseline_response = baseline_engine.query(complex_query)\n",
    "\n",
    "print(\"\\n📊 BASELINE APPROACH (Single Query)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nAnswer:\\n{baseline_response.response}\")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"\\nRetrieved Chunks:\")\n",
    "for i, node in enumerate(baseline_response.source_nodes, 1):\n",
    "    print(f\"\\n[Chunk {i}] Score: {node.score:.4f} | Source: {Path(node.metadata.get('file_name', 'unknown')).stem}\")\n",
    "    print(f\"{node.text[:250]}...\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f8abc",
   "metadata": {},
   "source": [
    "## 5. Implement Sub-Query Decomposition\n",
    "\n",
    "Now let's use the `SubQuestionQueryEngine` to automatically decompose the complex query into sub-questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f655a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine tool\n",
    "# This wraps our index with metadata describing what it contains\n",
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=index.as_query_engine(similarity_top_k=3),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"ml_algorithms\",\n",
    "        description=\"Comprehensive knowledge base about machine learning algorithms including gradient boosting, random forests, neural networks, support vector machines, and k-means clustering. Contains information about how they work, their advantages, disadvantages, and use cases.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✓ Query engine tool created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SubQuestionQueryEngine\n",
    "subquestion_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[query_engine_tool],\n",
    "    llm=azure_llm,\n",
    "    verbose=True,  # This will show us the sub-questions being generated\n",
    "    use_async=False\n",
    ")\n",
    "\n",
    "print(\"✓ Sub-question query engine created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1016312",
   "metadata": {},
   "source": [
    "## 6. Execute Sub-Query Decomposition\n",
    "\n",
    "Watch how the engine automatically breaks down our complex query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0abdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the same complex query with sub-question decomposition\n",
    "print(\"\\n🚀 SUB-QUESTION DECOMPOSITION APPROACH\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nOriginal Complex Query: {complex_query}\")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"\\nGenerating and executing sub-questions...\\n\")\n",
    "\n",
    "subquestion_response = subquestion_engine.query(complex_query)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"\\n✨ SYNTHESIZED ANSWER:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n{subquestion_response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d89ab",
   "metadata": {},
   "source": [
    "## 7. Analyze Sub-Questions Generated\n",
    "\n",
    "Let's examine what sub-questions were automatically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display sub-questions from the response metadata\n",
    "print(\"\\n📋 GENERATED SUB-QUESTIONS ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if hasattr(subquestion_response, 'metadata') and 'sub_qa' in subquestion_response.metadata:\n",
    "    sub_qa_pairs = subquestion_response.metadata['sub_qa']\n",
    "    \n",
    "    for i, (sub_q, sub_a) in enumerate(sub_qa_pairs, 1):\n",
    "        print(f\"\\n[Sub-Question {i}]\")\n",
    "        print(f\"Question: {sub_q.sub_q.sub_question}\")\n",
    "        print(f\"Tool Used: {sub_q.sub_q.tool_name}\")\n",
    "        print(f\"\\nAnswer: {sub_a.response[:300]}...\")\n",
    "        print(\"-\" * 100)\n",
    "else:\n",
    "    print(\"Sub-question metadata not available in this response format.\")\n",
    "    print(\"However, you should have seen the sub-questions in the verbose output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f230690",
   "metadata": {},
   "source": [
    "## 8. Side-by-Side Comparison\n",
    "\n",
    "Let's compare the coverage and quality of both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(baseline_resp, subq_resp, query):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"📊 DETAILED COMPARISON: BASELINE vs. SUB-QUESTION DECOMPOSITION\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    print(f\"\\nOriginal Query: {query}\")\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "    \n",
    "    # Compare answer lengths\n",
    "    print(\"\\n1. ANSWER LENGTH COMPARISON\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Baseline answer: {len(baseline_resp.response)} characters\")\n",
    "    print(f\"Sub-question answer: {len(subq_resp.response)} characters\")\n",
    "    \n",
    "    # Compare source coverage\n",
    "    print(\"\\n2. SOURCE COVERAGE COMPARISON\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    baseline_sources = set([Path(n.metadata.get('file_name', 'unknown')).stem for n in baseline_resp.source_nodes])\n",
    "    print(f\"\\nBaseline sources ({len(baseline_sources)}): {baseline_sources}\")\n",
    "    print(f\"Baseline retrieved {len(baseline_resp.source_nodes)} chunks total\")\n",
    "    \n",
    "    # For sub-question engine, sources are distributed across sub-queries\n",
    "    subq_sources = set([Path(n.metadata.get('file_name', 'unknown')).stem for n in subq_resp.source_nodes])\n",
    "    print(f\"\\nSub-question sources ({len(subq_sources)}): {subq_sources}\")\n",
    "    print(f\"Sub-question retrieved {len(subq_resp.source_nodes)} chunks total\")\n",
    "    \n",
    "    # Identify unique sources\n",
    "    only_baseline = baseline_sources - subq_sources\n",
    "    only_subq = subq_sources - baseline_sources\n",
    "    shared = baseline_sources & subq_sources\n",
    "    \n",
    "    print(f\"\\nShared sources: {shared}\")\n",
    "    if only_baseline:\n",
    "        print(f\"Only in baseline: {only_baseline}\")\n",
    "    if only_subq:\n",
    "        print(f\"Only in sub-question: {only_subq}\")\n",
    "    \n",
    "    # Compare answers\n",
    "    print(\"\\n3. GENERATED ANSWERS\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"\\n📊 BASELINE ANSWER:\\n{baseline_resp.response}\")\n",
    "    print(f\"\\n{'-' * 100}\")\n",
    "    print(f\"\\n🚀 SUB-QUESTION ANSWER:\\n{subq_resp.response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Run comparison\n",
    "compare_responses(baseline_response, subquestion_response, complex_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3585c8",
   "metadata": {},
   "source": [
    "## 9. Test with Additional Complex Queries\n",
    "\n",
    "Let's test with more queries that require multi-hop reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional complex queries\n",
    "additional_queries = [\n",
    "    \"Which algorithms are better for high-dimensional data, and what are their computational trade-offs?\",\n",
    "    \"How do supervised learning algorithms differ in their approaches to finding decision boundaries, and which is most interpretable?\",\n",
    "    \"What are the key differences between algorithms that handle outliers well versus those that don't?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"🔬 ADDITIONAL COMPLEX QUERY TESTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, query in enumerate(additional_queries[:1], 1):  # Test just one to save time\n",
    "    print(f\"\\n\\n{'=' * 100}\")\n",
    "    print(f\"Test Query {i}: {query}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Baseline\n",
    "    print(\"\\n📊 Baseline Approach:\")\n",
    "    baseline_resp = baseline_engine.query(query)\n",
    "    print(f\"Sources: {[Path(n.metadata.get('file_name', 'unknown')).stem for n in baseline_resp.source_nodes]}\")\n",
    "    print(f\"Answer length: {len(baseline_resp.response)} characters\")\n",
    "    \n",
    "    # Sub-question\n",
    "    print(f\"\\n🚀 Sub-Question Approach:\")\n",
    "    print(\"(Watch for sub-questions being generated...)\\n\")\n",
    "    subq_resp = subquestion_engine.query(query)\n",
    "    print(f\"\\nSources: {[Path(n.metadata.get('file_name', 'unknown')).stem for n in subq_resp.source_nodes]}\")\n",
    "    print(f\"Answer length: {len(subq_resp.response)} characters\")\n",
    "    \n",
    "    print(f\"\\n✨ Sub-Question Answer:\\n{subq_resp.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6547d11",
   "metadata": {},
   "source": [
    "## 10. Visualize the Data Flow\n",
    "\n",
    "Let's create a simple visualization of how sub-query decomposition works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    DATA FLOW: SUB-QUERY DECOMPOSITION                          ║\n",
    "╚════════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "BASELINE APPROACH (Single Query):\n",
    "─────────────────────────────────\n",
    "\n",
    "  Complex Query\n",
    "       │\n",
    "       ▼\n",
    "  Embed Query\n",
    "       │\n",
    "       ▼\n",
    "  Vector Search ────► Retrieve Top-K Chunks (may miss important context)\n",
    "       │\n",
    "       ▼\n",
    "  LLM Generation\n",
    "       │\n",
    "       ▼\n",
    "  Final Answer (limited by single retrieval pass)\n",
    "\n",
    "\n",
    "SUB-QUESTION DECOMPOSITION APPROACH:\n",
    "────────────────────────────────────\n",
    "\n",
    "  Complex Query\n",
    "       │\n",
    "       ▼\n",
    "  LLM Decomposes ────► Sub-Q1: \"What are strengths of gradient boosting?\"\n",
    "       │              Sub-Q2: \"What are weaknesses of gradient boosting?\"\n",
    "       │              Sub-Q3: \"What are strengths of random forests?\"\n",
    "       │              Sub-Q4: \"What are weaknesses of random forests?\"\n",
    "       │              Sub-Q5: \"When to use each algorithm?\"\n",
    "       ▼\n",
    "  Parallel Retrieval:\n",
    "       ├─────► Sub-Q1 → Vector Search → Chunks about GB strengths\n",
    "       ├─────► Sub-Q2 → Vector Search → Chunks about GB weaknesses\n",
    "       ├─────► Sub-Q3 → Vector Search → Chunks about RF strengths\n",
    "       ├─────► Sub-Q4 → Vector Search → Chunks about RF weaknesses\n",
    "       └─────► Sub-Q5 → Vector Search → Chunks about use cases\n",
    "       │\n",
    "       ▼\n",
    "  Aggregate All Retrieved Contexts (comprehensive coverage)\n",
    "       │\n",
    "       ▼\n",
    "  LLM Synthesis\n",
    "       │\n",
    "       ▼\n",
    "  Final Answer (comprehensive, balanced, multi-faceted)\n",
    "\n",
    "\n",
    "KEY ADVANTAGES:\n",
    "───────────────\n",
    "✓ Each sub-question targets specific information\n",
    "✓ Multiple retrieval passes increase coverage\n",
    "✓ Balanced information from different sources\n",
    "✓ Better handling of comparative queries\n",
    "✓ Reduced chance of missing critical context\n",
    "\n",
    "TRADE-OFFS:\n",
    "───────────\n",
    "• Higher latency (multiple LLM calls + multiple retrievals)\n",
    "• Increased token usage and cost\n",
    "• More complex pipeline to debug\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3299a61a",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways and Best Practices\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Complex queries benefit from decomposition**: Questions requiring comparisons, multi-faceted analysis, or information synthesis are better served by breaking them into sub-questions.\n",
    "\n",
    "2. **Increased coverage**: Sub-query decomposition retrieves from multiple focused searches, resulting in more comprehensive context.\n",
    "\n",
    "3. **Better balance**: For comparative queries (\"X vs Y\"), decomposition ensures both sides get equal attention in retrieval.\n",
    "\n",
    "4. **Automatic planning**: The LLM automatically determines what sub-questions are needed - no manual query engineering required.\n",
    "\n",
    "### When to Use Sub-Query Decomposition:\n",
    "\n",
    "✅ **Use when:**\n",
    "- Query contains \"compare\", \"contrast\", \"differences\"\n",
    "- Question has multiple parts or facets\n",
    "- Requires synthesizing information from different sources\n",
    "- Multi-hop reasoning is needed\n",
    "- Comprehensive coverage is more important than speed\n",
    "\n",
    "❌ **Avoid when:**\n",
    "- Simple, focused queries\n",
    "- Real-time latency requirements\n",
    "- Limited API budget\n",
    "- Single-source answers are sufficient\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Tool metadata matters**: Provide clear, descriptive metadata for query engine tools so the LLM can generate relevant sub-questions.\n",
    "\n",
    "2. **Use verbose mode during development**: See what sub-questions are being generated to debug and optimize.\n",
    "\n",
    "3. **Consider async execution**: Set `use_async=True` to parallelize sub-question retrieval (reduces latency).\n",
    "\n",
    "4. **Balance top-k per sub-query**: Lower top-k per sub-question (e.g., 2-3) since you're doing multiple retrievals.\n",
    "\n",
    "5. **Monitor costs**: Each sub-question is an additional LLM call and retrieval operation.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In Demo #3, we'll explore **Hybrid Search**, combining dense vector search with sparse keyword retrieval (BM25) to improve precision across different query types."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
