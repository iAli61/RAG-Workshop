{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5f3ac5",
   "metadata": {},
   "source": [
    "# Demo #7: Corrective RAG (CRAG) - Self-Correcting Retrieval\n",
    "\n",
    "## Objective\n",
    "Implement a self-reflective system that evaluates retrieval quality and triggers corrective actions (web search fallback) when internal knowledge is insufficient.\n",
    "\n",
    "## Core Concepts\n",
    "- **Self-correction and retrieval evaluation**: System assesses its own retrieval quality\n",
    "- **Dynamic routing based on confidence scores**: High/Low/Ambiguous confidence triggers different paths\n",
    "- **Fallback to external knowledge sources**: Web search when internal knowledge fails\n",
    "- **Knowledge refinement**: Document grading and filtering\n",
    "\n",
    "## What is Corrective RAG (CRAG)?\n",
    "\n",
    "Traditional RAG assumes that retrieved documents are always relevant and useful. **CRAG challenges this assumption** by:\n",
    "\n",
    "1. **Evaluating** retrieval quality before generation\n",
    "2. **Correcting** by routing to alternative sources when needed\n",
    "3. **Refining** retrieved content through knowledge strip filtering\n",
    "\n",
    "### The Problem with Naive RAG\n",
    "```\n",
    "Query → Retrieve → Generate\n",
    "         ↓\n",
    "   (Assumes retrieval is good)\n",
    "```\n",
    "\n",
    "**Issues:**\n",
    "- ❌ No evaluation of retrieval quality\n",
    "- ❌ Generates answers even with poor/irrelevant context\n",
    "- ❌ No fallback when knowledge base lacks information\n",
    "- ❌ Leads to hallucinations or low-quality answers\n",
    "\n",
    "### CRAG Solution\n",
    "```\n",
    "Query → Retrieve → Evaluate Confidence\n",
    "                        ↓\n",
    "         ┌──────────────┼──────────────┐\n",
    "         ↓              ↓              ↓\n",
    "      HIGH           LOW          AMBIGUOUS\n",
    "         ↓              ↓              ↓\n",
    "   Use Internal   Web Search    Merge Both\n",
    "       Docs          Only         Sources\n",
    "         ↓              ↓              ↓\n",
    "         └──────────────┴──────────────┘\n",
    "                        ↓\n",
    "                  LLM Generation\n",
    "```\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Query\n",
    "  ↓\n",
    "Retrieve from Internal Knowledge Base\n",
    "  ↓\n",
    "Evaluator scores relevance (0-1)\n",
    "  ↓\n",
    "Decision:\n",
    "  • High (>0.7): Use internal docs\n",
    "  • Low (<0.4): Use web search\n",
    "  • Ambiguous (0.4-0.7): Merge both\n",
    "  ↓\n",
    "Optional: Knowledge strip filtering\n",
    "  ↓\n",
    "LLM Generation with refined context\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ad17e",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies and Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a53b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell if packages are not already installed\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify Azure OpenAI credentials\n",
    "required_vars = [\n",
    "    'AZURE_OPENAI_API_KEY',\n",
    "    'AZURE_OPENAI_ENDPOINT',\n",
    "    'AZURE_OPENAI_API_VERSION',\n",
    "    'AZURE_OPENAI_DEPLOYMENT_NAME',\n",
    "    'AZURE_OPENAI_EMBEDDING_DEPLOYMENT'\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "if missing_vars:\n",
    "    print(f\"❌ Missing environment variables: {', '.join(missing_vars)}\")\n",
    "else:\n",
    "    print(\"✅ All required environment variables are set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64bb771",
   "metadata": {},
   "source": [
    "## Step 1: Setup Limited Internal Knowledge Base\n",
    "\n",
    "We'll create a focused knowledge base on **machine learning concepts**. This will allow us to test:\n",
    "- **In-domain queries**: Questions about ML algorithms (should score HIGH)\n",
    "- **Out-of-domain queries**: Questions about unrelated topics (should score LOW)\n",
    "- **Ambiguous queries**: Questions partially covered (should score AMBIGUOUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Initialize Azure OpenAI\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    ")\n",
    "\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "\n",
    "print(\"✅ Azure OpenAI initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only ML concepts (limited knowledge base)\n",
    "documents = SimpleDirectoryReader('data/ml_concepts').load_data()\n",
    "\n",
    "print(f\"📚 Internal Knowledge Base: {len(documents)} documents\")\n",
    "print(\"\\nDocuments:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    filename = os.path.basename(doc.metadata.get('file_name', 'Unknown'))\n",
    "    print(f\"   {i}. {filename}\")\n",
    "\n",
    "print(\"\\n⚠️ This is a LIMITED knowledge base focused on ML concepts.\")\n",
    "print(\"   Queries outside this domain should trigger web search fallback.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "index = VectorStoreIndex(nodes, embed_model=azure_embed)\n",
    "\n",
    "print(f\"✅ Vector index created with {len(nodes)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c33c8",
   "metadata": {},
   "source": [
    "## Step 2: Implement Retrieval Evaluator\n",
    "\n",
    "The evaluator is the core of CRAG. It assesses whether retrieved documents are sufficient to answer the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317eb3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_relevance(query: str, retrieved_chunks: List[str], llm) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of retrieved chunks for a given query.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (confidence_score, reasoning)\n",
    "        - confidence_score: 0-1 scale\n",
    "        - reasoning: Explanation of the score\n",
    "    \"\"\"\n",
    "    # Combine chunks for evaluation\n",
    "    context = \"\\n\\n\".join([f\"Document {i+1}: {chunk[:300]}...\" for i, chunk in enumerate(retrieved_chunks[:5])])\n",
    "    \n",
    "    eval_prompt = f\"\"\"\n",
    "You are a retrieval quality evaluator. Assess whether the retrieved documents contain sufficient information to answer the query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Evaluate the retrieval quality and provide:\n",
    "1. A confidence score between 0 and 1:\n",
    "   - 0.0-0.4: LOW - Documents are irrelevant or insufficient\n",
    "   - 0.4-0.7: AMBIGUOUS - Partial information, may need supplementation\n",
    "   - 0.7-1.0: HIGH - Documents are highly relevant and sufficient\n",
    "\n",
    "2. Brief reasoning (1-2 sentences)\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "    \"confidence_score\": 0.85,\n",
    "    \"reasoning\": \"Your explanation here\"\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.complete(eval_prompt)\n",
    "    \n",
    "    try:\n",
    "        # Parse JSON response\n",
    "        # Extract JSON from response (handle cases where LLM adds extra text)\n",
    "        response_text = response.text.strip()\n",
    "        # Find JSON in response\n",
    "        json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            result = json.loads(json_match.group())\n",
    "            return result['confidence_score'], result['reasoning']\n",
    "        else:\n",
    "            # Fallback: try to extract score from text\n",
    "            score_match = re.search(r'(0\\.[0-9]+|1\\.0)', response_text)\n",
    "            if score_match:\n",
    "                return float(score_match.group()), response_text\n",
    "            return 0.5, \"Could not parse evaluation\"\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error parsing evaluation: {e}\")\n",
    "        return 0.5, \"Evaluation error\"\n",
    "\n",
    "print(\"✅ Retrieval evaluator function created\")\n",
    "print(\"   Thresholds: HIGH (>0.7), AMBIGUOUS (0.4-0.7), LOW (<0.4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3a75e",
   "metadata": {},
   "source": [
    "## Step 3: Implement Web Search Fallback\n",
    "\n",
    "For this demo, we'll use a **mock web search** that simulates external knowledge retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search_fallback(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Simulate web search results.\n",
    "    \n",
    "    In a production system, this would integrate with:\n",
    "    - Bing Search API (Azure)\n",
    "    - Google Search API\n",
    "    - Tavily Search API\n",
    "    - SerpAPI\n",
    "    \n",
    "    For demo purposes, we return mock results based on query keywords.\n",
    "    \"\"\"\n",
    "    print(f\"\\n🌐 Simulating web search for: {query}\")\n",
    "    \n",
    "    # Mock results based on query content\n",
    "    if \"climate\" in query.lower() or \"global warming\" in query.lower():\n",
    "        return [\n",
    "            \"[Web Source 1] Climate change refers to long-term shifts in temperatures and weather patterns. Since the 1800s, human activities have been the main driver of climate change, primarily due to burning fossil fuels.\",\n",
    "            \"[Web Source 2] The effects of climate change include rising sea levels, more frequent extreme weather events, and significant impacts on ecosystems and biodiversity.\",\n",
    "            \"[Web Source 3] The Paris Agreement is an international treaty on climate change, adopted in 2015, with the goal of limiting global warming to well below 2°C above pre-industrial levels.\"\n",
    "        ]\n",
    "    elif \"quantum computing\" in query.lower() or \"quantum\" in query.lower():\n",
    "        return [\n",
    "            \"[Web Source 1] Quantum computing leverages quantum mechanics principles like superposition and entanglement to process information in fundamentally different ways than classical computers.\",\n",
    "            \"[Web Source 2] Quantum computers use quantum bits (qubits) which can exist in multiple states simultaneously, enabling parallel processing of vast amounts of data.\",\n",
    "            \"[Web Source 3] Current applications of quantum computing include cryptography, drug discovery, optimization problems, and simulation of quantum systems.\"\n",
    "        ]\n",
    "    elif \"blockchain\" in query.lower() or \"cryptocurrency\" in query.lower():\n",
    "        return [\n",
    "            \"[Web Source 1] Blockchain is a distributed ledger technology that records transactions across multiple computers in a way that makes it difficult to alter retroactively.\",\n",
    "            \"[Web Source 2] Cryptocurrencies like Bitcoin use blockchain technology to enable peer-to-peer transactions without the need for a central authority.\",\n",
    "            \"[Web Source 3] Beyond cryptocurrencies, blockchain has applications in supply chain management, smart contracts, and digital identity verification.\"\n",
    "        ]\n",
    "    else:\n",
    "        # Generic fallback\n",
    "        return [\n",
    "            f\"[Web Source 1] Web search result related to: {query}. This is a simulated result from an external knowledge source.\",\n",
    "            f\"[Web Source 2] Additional information about {query} from web search. In a real system, this would come from Bing/Google API.\",\n",
    "            f\"[Web Source 3] Further context on {query} retrieved from external sources.\"\n",
    "        ]\n",
    "\n",
    "print(\"✅ Mock web search function created\")\n",
    "print(\"   In production, replace with Bing Search API or similar service.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61be9f",
   "metadata": {},
   "source": [
    "## Step 4: Build CRAG Query Engine\n",
    "\n",
    "Now we'll create the main CRAG system that ties everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectiveRAGEngine:\n",
    "    \"\"\"\n",
    "    Corrective RAG Query Engine with self-evaluation and dynamic routing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index, llm, embed_model, retriever_top_k=5):\n",
    "        self.index = index\n",
    "        self.llm = llm\n",
    "        self.embed_model = embed_model\n",
    "        self.retriever = index.as_retriever(similarity_top_k=retriever_top_k)\n",
    "        \n",
    "        # Confidence thresholds\n",
    "        self.HIGH_THRESHOLD = 0.7\n",
    "        self.LOW_THRESHOLD = 0.4\n",
    "    \n",
    "    def query(self, query_str: str, verbose: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Execute CRAG pipeline:\n",
    "        1. Retrieve from internal KB\n",
    "        2. Evaluate retrieval quality\n",
    "        3. Route based on confidence\n",
    "        4. Generate answer\n",
    "        \n",
    "        Returns:\n",
    "            Dict with keys: answer, confidence, route, reasoning\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"🔍 Query: {query_str}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Step 1: Retrieve from internal KB\n",
    "        if verbose:\n",
    "            print(\"📚 Step 1: Retrieving from internal knowledge base...\")\n",
    "        \n",
    "        retrieved_nodes = self.retriever.retrieve(query_str)\n",
    "        retrieved_texts = [node.node.text for node in retrieved_nodes]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Retrieved {len(retrieved_texts)} chunks from internal KB\")\n",
    "            for i, node in enumerate(retrieved_nodes[:3], 1):\n",
    "                filename = os.path.basename(node.node.metadata.get('file_name', 'Unknown'))\n",
    "                print(f\"   {i}. {filename} (score: {node.score:.4f})\")\n",
    "        \n",
    "        # Step 2: Evaluate retrieval quality\n",
    "        if verbose:\n",
    "            print(\"\\n🎯 Step 2: Evaluating retrieval quality...\")\n",
    "        \n",
    "        confidence, reasoning = evaluate_retrieval_relevance(\n",
    "            query_str, retrieved_texts, self.llm\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Confidence Score: {confidence:.2f}\")\n",
    "            print(f\"   Reasoning: {reasoning}\")\n",
    "        \n",
    "        # Step 3: Route based on confidence\n",
    "        if confidence >= self.HIGH_THRESHOLD:\n",
    "            route = \"INTERNAL\"\n",
    "            context_chunks = retrieved_texts\n",
    "            if verbose:\n",
    "                print(f\"\\n✅ Step 3: Route = {route} (confidence >= {self.HIGH_THRESHOLD})\")\n",
    "                print(\"   Using internal documents only.\")\n",
    "        \n",
    "        elif confidence < self.LOW_THRESHOLD:\n",
    "            route = \"WEB_SEARCH\"\n",
    "            web_results = web_search_fallback(query_str)\n",
    "            context_chunks = web_results\n",
    "            if verbose:\n",
    "                print(f\"\\n🌐 Step 3: Route = {route} (confidence < {self.LOW_THRESHOLD})\")\n",
    "                print(\"   Internal KB insufficient. Using web search results.\")\n",
    "                for i, result in enumerate(web_results, 1):\n",
    "                    print(f\"   {i}. {result[:80]}...\")\n",
    "        \n",
    "        else:\n",
    "            route = \"HYBRID\"\n",
    "            web_results = web_search_fallback(query_str)\n",
    "            context_chunks = retrieved_texts + web_results\n",
    "            if verbose:\n",
    "                print(f\"\\n🔀 Step 3: Route = {route} ({self.LOW_THRESHOLD} <= confidence < {self.HIGH_THRESHOLD})\")\n",
    "                print(\"   Merging internal KB and web search results.\")\n",
    "        \n",
    "        # Step 4: Generate answer\n",
    "        if verbose:\n",
    "            print(f\"\\n🤖 Step 4: Generating answer with {len(context_chunks)} context chunks...\")\n",
    "        \n",
    "        context = \"\\n\\n\".join([f\"[Source {i+1}] {chunk}\" for i, chunk in enumerate(context_chunks)])\n",
    "        \n",
    "        generation_prompt = f\"\"\"\n",
    "Answer the following question based on the provided context. Be accurate and cite sources.\n",
    "\n",
    "Question: {query_str}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(generation_prompt)\n",
    "        \n",
    "        return {\n",
    "            'answer': response.text,\n",
    "            'confidence': confidence,\n",
    "            'route': route,\n",
    "            'reasoning': reasoning,\n",
    "            'num_internal_chunks': len(retrieved_texts),\n",
    "            'num_web_chunks': len(web_search_fallback(query_str)) if route in ['WEB_SEARCH', 'HYBRID'] else 0\n",
    "        }\n",
    "\n",
    "# Initialize CRAG engine\n",
    "crag_engine = CorrectiveRAGEngine(\n",
    "    index=index,\n",
    "    llm=azure_llm,\n",
    "    embed_model=azure_embed,\n",
    "    retriever_top_k=5\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Corrective RAG Engine initialized\")\n",
    "print(\"   Ready to handle in-domain, out-of-domain, and ambiguous queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1cf942",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation Scenarios\n",
    "\n",
    "Let's test the three scenarios: In-domain, Out-of-domain, and Ambiguous queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ecdfa0",
   "metadata": {},
   "source": [
    "### Scenario 1: In-Domain Query (HIGH Confidence)\n",
    "\n",
    "This query is about neural networks, which is fully covered in our ML knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc9db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_domain_query = \"What is backpropagation in neural networks and how does it work?\"\n",
    "\n",
    "result_in_domain = crag_engine.query(in_domain_query, verbose=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"📝 FINAL ANSWER:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(result_in_domain['answer'])\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Route: {result_in_domain['route']}\")\n",
    "print(f\"Confidence: {result_in_domain['confidence']:.2f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b293b34",
   "metadata": {},
   "source": [
    "### Scenario 2: Out-of-Domain Query (LOW Confidence)\n",
    "\n",
    "This query is about climate change, which is NOT in our ML-focused knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b99983",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_domain_query = \"What are the main causes of climate change and global warming?\"\n",
    "\n",
    "result_out_domain = crag_engine.query(out_of_domain_query, verbose=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"📝 FINAL ANSWER:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(result_out_domain['answer'])\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Route: {result_out_domain['route']}\")\n",
    "print(f\"Confidence: {result_out_domain['confidence']:.2f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948c945",
   "metadata": {},
   "source": [
    "### Scenario 3: Ambiguous Query (AMBIGUOUS Confidence)\n",
    "\n",
    "This query mentions machine learning but asks about a specific application (quantum computing) not fully covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ccb3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_query = \"How can machine learning algorithms be used in quantum computing applications?\"\n",
    "\n",
    "result_ambiguous = crag_engine.query(ambiguous_query, verbose=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"📝 FINAL ANSWER:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(result_ambiguous['answer'])\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Route: {result_ambiguous['route']}\")\n",
    "print(f\"Confidence: {result_ambiguous['confidence']:.2f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442038f",
   "metadata": {},
   "source": [
    "## Step 6: Comparative Analysis\n",
    "\n",
    "Let's compare CRAG with a naive RAG system that doesn't evaluate or correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae019d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Naive RAG (no evaluation, no correction)\n",
    "naive_retriever = index.as_retriever(similarity_top_k=5)\n",
    "naive_engine = RetrieverQueryEngine(retriever=naive_retriever, llm=azure_llm)\n",
    "\n",
    "print(\"✅ Naive RAG engine created for comparison\")\n",
    "print(\"   This engine always uses internal KB, even when insufficient.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test naive RAG on out-of-domain query\n",
    "print(f\"\\n🔍 Testing NAIVE RAG on out-of-domain query:\\n{out_of_domain_query}\\n\")\n",
    "\n",
    "naive_response = naive_engine.query(out_of_domain_query)\n",
    "\n",
    "print(\"📝 Naive RAG Answer (No Correction):\")\n",
    "print(\"=\"*80)\n",
    "print(naive_response.response)\n",
    "print(\"=\"*80)\n",
    "print(\"\\n⚠️ Issues with Naive RAG:\")\n",
    "print(\"   - May hallucinate or provide irrelevant information\")\n",
    "print(\"   - No awareness that knowledge base lacks relevant information\")\n",
    "print(\"   - No fallback to external sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09294a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare side-by-side\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIDE-BY-SIDE COMPARISON: Out-of-Domain Query\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nQuery: {out_of_domain_query}\\n\")\n",
    "\n",
    "print(\"\\n[NAIVE RAG - No Evaluation]\")\n",
    "print(\"-\"*80)\n",
    "print(naive_response.response[:400] + \"...\")\n",
    "print(\"\\nRoute: INTERNAL ONLY (always)\")\n",
    "print(\"Confidence: N/A (no evaluation)\")\n",
    "\n",
    "print(\"\\n\\n[CORRECTIVE RAG - With Evaluation]\")\n",
    "print(\"-\"*80)\n",
    "print(result_out_domain['answer'][:400] + \"...\")\n",
    "print(f\"\\nRoute: {result_out_domain['route']}\")\n",
    "print(f\"Confidence: {result_out_domain['confidence']:.2f}\")\n",
    "print(f\"Reasoning: {result_out_domain['reasoning']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ CRAG provides accurate answers by recognizing knowledge gaps\")\n",
    "print(\"   and routing to appropriate external sources.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb6512",
   "metadata": {},
   "source": [
    "## Visualization: CRAG Decision Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3221c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Collect results\n",
    "scenarios = [\n",
    "    ('In-Domain\\n(Neural Networks)', result_in_domain),\n",
    "    ('Out-of-Domain\\n(Climate Change)', result_out_domain),\n",
    "    ('Ambiguous\\n(ML + Quantum)', result_ambiguous)\n",
    "]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Confidence Scores\n",
    "labels = [s[0] for s in scenarios]\n",
    "confidences = [s[1]['confidence'] for s in scenarios]\n",
    "colors = ['green' if c >= 0.7 else 'red' if c < 0.4 else 'orange' for c in confidences]\n",
    "\n",
    "bars = ax1.bar(labels, confidences, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax1.axhline(y=0.7, color='green', linestyle='--', linewidth=2, label='HIGH threshold (0.7)')\n",
    "ax1.axhline(y=0.4, color='red', linestyle='--', linewidth=2, label='LOW threshold (0.4)')\n",
    "ax1.set_ylabel('Confidence Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('CRAG Confidence Evaluation', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 1.0)\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, conf in zip(bars, confidences):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{conf:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Plot 2: Routing Decisions\n",
    "routes = [s[1]['route'] for s in scenarios]\n",
    "route_colors = {'INTERNAL': 'green', 'WEB_SEARCH': 'red', 'HYBRID': 'orange'}\n",
    "route_labels = {'INTERNAL': 'Internal KB Only', 'WEB_SEARCH': 'Web Search Only', 'HYBRID': 'Hybrid (Both)'}\n",
    "\n",
    "route_counts = {r: routes.count(r) for r in set(routes)}\n",
    "route_names = [route_labels[r] for r in route_counts.keys()]\n",
    "route_values = list(route_counts.values())\n",
    "route_colors_list = [route_colors[r] for r in route_counts.keys()]\n",
    "\n",
    "ax2.pie(route_values, labels=route_names, colors=route_colors_list, autopct='%1.0f%%',\n",
    "        startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "ax2.set_title('CRAG Routing Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Decision Summary:\")\n",
    "for label, result in scenarios:\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "    print(f\"  Route: {result['route']}\")\n",
    "    print(f\"  Reasoning: {result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6121100",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. **Self-Evaluation is Critical**\n",
    "- CRAG evaluates its own retrieval quality before generating\n",
    "- Prevents hallucinations and low-quality answers\n",
    "- Provides confidence scores for transparency\n",
    "\n",
    "### 2. **Dynamic Routing Improves Robustness**\n",
    "✅ **Three routing strategies:**\n",
    "- **HIGH confidence (>0.7)**: Use internal KB only (most efficient)\n",
    "- **LOW confidence (<0.4)**: Use web search only (knowledge gap detected)\n",
    "- **AMBIGUOUS (0.4-0.7)**: Merge both sources (hybrid approach)\n",
    "\n",
    "### 3. **Fallback Mechanisms are Essential**\n",
    "- No knowledge base is complete\n",
    "- External sources (web search, APIs) fill gaps\n",
    "- System remains useful even for out-of-domain queries\n",
    "\n",
    "### 4. **CRAG vs. Naive RAG**\n",
    "\n",
    "| Aspect | Naive RAG | Corrective RAG |\n",
    "|--------|-----------|----------------|\n",
    "| Retrieval Evaluation | ❌ None | ✅ LLM-based scoring |\n",
    "| Knowledge Gap Detection | ❌ No | ✅ Yes |\n",
    "| Fallback Mechanism | ❌ No | ✅ Web search |\n",
    "| Out-of-Domain Handling | ❌ Poor | ✅ Good |\n",
    "| Hallucination Risk | ⚠️ Higher | ✅ Lower |\n",
    "| Cost | 💰 Lower | 💰💰 Higher (extra LLM call) |\n",
    "\n",
    "### 5. **When to Use CRAG**\n",
    "\n",
    "✅ **Use CRAG when:**\n",
    "- Knowledge base is limited or specialized\n",
    "- Query distribution is unpredictable\n",
    "- Answer accuracy is critical (e.g., medical, legal, financial)\n",
    "- You have access to reliable external sources\n",
    "- Cost of wrong answer > cost of extra evaluation\n",
    "\n",
    "❌ **Skip CRAG when:**\n",
    "- Knowledge base is comprehensive and well-curated\n",
    "- All queries are guaranteed to be in-domain\n",
    "- Cost/latency is critical constraint\n",
    "- No reliable external sources available\n",
    "\n",
    "### 6. **Implementation Considerations**\n",
    "\n",
    "**Threshold Tuning:**\n",
    "- Adjust HIGH/LOW thresholds based on your tolerance for:\n",
    "  - False positives (using internal KB when insufficient)\n",
    "  - False negatives (triggering web search unnecessarily)\n",
    "- Recommended starting point: HIGH=0.7, LOW=0.4\n",
    "- Monitor and adjust based on real-world performance\n",
    "\n",
    "**Evaluator Design:**\n",
    "- LLM-based evaluation is flexible but adds latency/cost\n",
    "- Alternative: Train a lightweight classifier (faster, cheaper)\n",
    "- Hybrid: Use heuristics (chunk scores, keyword matching) + LLM fallback\n",
    "\n",
    "**Web Search Integration:**\n",
    "- Production: Use Bing Search API, Google Search API, or Tavily\n",
    "- Parse and chunk web results same as internal docs\n",
    "- Consider freshness (web results may be more up-to-date)\n",
    "- Add source citations to answers\n",
    "\n",
    "### 7. **Advanced Extensions**\n",
    "\n",
    "**Knowledge Strip Filtering:**\n",
    "- Further refine by scoring individual sentences\n",
    "- Remove low-relevance sentences before generation\n",
    "- Reduces token count and improves precision\n",
    "\n",
    "**Multi-Hop CRAG:**\n",
    "- Iteratively retrieve and evaluate\n",
    "- Refine query based on evaluation results\n",
    "- Enable complex reasoning over multiple sources\n",
    "\n",
    "**Confidence Calibration:**\n",
    "- Log confidence scores and actual answer quality\n",
    "- Fine-tune thresholds over time\n",
    "- Improve evaluator with domain-specific examples\n",
    "\n",
    "## Architecture Diagram\n",
    "\n",
    "```\n",
    "                    User Query\n",
    "                        ↓\n",
    "        ┌───────────────────────────────┐\n",
    "        │  Retrieve from Internal KB    │\n",
    "        │  (Top-K chunks)               │\n",
    "        └───────────┬───────────────────┘\n",
    "                    ↓\n",
    "        ┌───────────────────────────────┐\n",
    "        │  LLM Evaluator                │\n",
    "        │  (Score: 0-1)                 │\n",
    "        └───────────┬───────────────────┘\n",
    "                    ↓\n",
    "         ┌──────────┼──────────┐\n",
    "         ↓          ↓          ↓\n",
    "     Score ≥ 0.7  0.4-0.7   < 0.4\n",
    "         ↓          ↓          ↓\n",
    "    ┌────────┐ ┌────────┐ ┌────────┐\n",
    "    │Internal│ │ Hybrid │ │  Web   │\n",
    "    │   KB   │ │ (Both) │ │ Search │\n",
    "    └────┬───┘ └────┬───┘ └────┬───┘\n",
    "         └──────────┼──────────┘\n",
    "                    ↓\n",
    "        ┌───────────────────────────────┐\n",
    "        │  Optional: Knowledge Strip    │\n",
    "        │  Filtering (sentence-level)   │\n",
    "        └───────────┬───────────────────┘\n",
    "                    ↓\n",
    "        ┌───────────────────────────────┐\n",
    "        │  LLM Generation               │\n",
    "        │  (High-quality context)       │\n",
    "        └───────────┬───────────────────┘\n",
    "                    ↓\n",
    "              Final Answer\n",
    "          (with source citations)\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To enhance your CRAG system:\n",
    "1. **Integrate real web search** (Bing API, Tavily, etc.)\n",
    "2. **Fine-tune the evaluator** on domain-specific examples\n",
    "3. **Add knowledge strip filtering** for sentence-level refinement\n",
    "4. **Implement caching** to reduce repeated evaluations\n",
    "5. **Monitor confidence calibration** and adjust thresholds\n",
    "6. **Add source citations** to generated answers\n",
    "\n",
    "---\n",
    "\n",
    "**Demo Complete! ✅**\n",
    "\n",
    "You've successfully implemented Corrective RAG with self-evaluation, dynamic routing, and web search fallback."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
