{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7bf3777",
   "metadata": {},
   "source": [
    "# Demo #5: Re-Ranking with Cross-Encoders - Post-Retrieval Refinement\n",
    "\n",
    "## Objective\n",
    "Demonstrate how a two-stage retrieval process (fast bi-encoder + accurate cross-encoder) improves precision by re-ordering initial results.\n",
    "\n",
    "## Core Concepts\n",
    "- **Two-stage retrieval architecture**: Fast bi-encoder for recall, accurate cross-encoder for precision\n",
    "- **Bi-encoder vs. Cross-encoder comparison**: Understanding the trade-offs\n",
    "- **Re-ranking for precision optimization**: Improving the quality of retrieved results\n",
    "\n",
    "## What is Re-Ranking?\n",
    "\n",
    "### The Problem\n",
    "Standard RAG systems use **bi-encoders** (like Azure OpenAI embeddings) that:\n",
    "- Encode queries and documents **independently**\n",
    "- Compare via simple cosine similarity\n",
    "- Are **fast** but may miss nuanced semantic relationships\n",
    "- Can rank less-relevant documents higher due to keyword overlap\n",
    "\n",
    "### The Solution: Two-Stage Retrieval\n",
    "1. **Stage 1 (Recall)**: Bi-encoder retrieves a larger set (e.g., top-20) quickly\n",
    "2. **Stage 2 (Precision)**: Cross-encoder re-ranks this smaller set more accurately\n",
    "\n",
    "### Bi-Encoder vs. Cross-Encoder\n",
    "\n",
    "**Bi-Encoder:**\n",
    "```\n",
    "Query ‚Üí Encoder A ‚Üí Vector Q\n",
    "Document ‚Üí Encoder B ‚Üí Vector D\n",
    "Similarity = cosine(Q, D)\n",
    "```\n",
    "- ‚úÖ Fast: Pre-compute document embeddings\n",
    "- ‚úÖ Scalable: Works with millions of documents\n",
    "- ‚ùå Limited: No direct query-document interaction\n",
    "\n",
    "**Cross-Encoder:**\n",
    "```\n",
    "[Query + Document] ‚Üí Single Encoder ‚Üí Relevance Score\n",
    "```\n",
    "- ‚úÖ Accurate: Deep attention between query and document\n",
    "- ‚úÖ Contextual: Captures subtle semantic relationships\n",
    "- ‚ùå Slow: Must process each query-document pair\n",
    "- ‚ùå Not scalable: Cannot pre-compute\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Query\n",
    "  ‚Üì\n",
    "Bi-encoder retrieval (top-20, fast)\n",
    "  ‚Üì\n",
    "Cross-encoder re-ranking (top-5, accurate)\n",
    "  ‚Üì\n",
    "LLM generation with highest-quality context\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfa8eb",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies and Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell if packages are not already installed\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai\n",
    "# !pip install sentence-transformers torch  # For cross-encoder model\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9915a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify Azure OpenAI credentials\n",
    "required_vars = [\n",
    "    'AZURE_OPENAI_API_KEY',\n",
    "    'AZURE_OPENAI_ENDPOINT',\n",
    "    'AZURE_OPENAI_API_VERSION',\n",
    "    'AZURE_OPENAI_DEPLOYMENT_NAME',\n",
    "    'AZURE_OPENAI_EMBEDDING_DEPLOYMENT'\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "if missing_vars:\n",
    "    print(f\"‚ùå Missing environment variables: {', '.join(missing_vars)}\")\n",
    "    print(\"Please ensure your .env file contains all required Azure OpenAI credentials.\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables are set.\")\n",
    "    print(f\"   Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "    print(f\"   LLM Deployment: {os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')}\")\n",
    "    print(f\"   Embedding Deployment: {os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687c325",
   "metadata": {},
   "source": [
    "## Step 1: Data and Baseline Setup\n",
    "\n",
    "We'll use a moderately sized knowledge base (11 documents total) combining:\n",
    "- ML concepts (5 docs): gradient boosting, neural networks, random forests, support vector machines, k-means clustering\n",
    "- Tech docs (6 docs): BERT, GPT-4, REST API, Transformer Architecture, Docker, Embeddings\n",
    "\n",
    "This mix includes topically similar but semantically distinct content, which is ideal for demonstrating re-ranking effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80045a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Initialize Azure OpenAI LLM\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "    temperature=0.1  # Low temperature for consistent results\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI Embedding Model\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "\n",
    "print(\"‚úÖ Azure OpenAI LLM and Embedding models initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c47d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from both directories\n",
    "ml_docs = SimpleDirectoryReader('data/ml_concepts').load_data()\n",
    "tech_docs = SimpleDirectoryReader('data/tech_docs').load_data()\n",
    "documents = ml_docs + tech_docs\n",
    "\n",
    "print(f\"üìö Loaded {len(documents)} documents:\")\n",
    "print(f\"   - ML Concepts: {len(ml_docs)} documents\")\n",
    "print(f\"   - Tech Docs: {len(tech_docs)} documents\")\n",
    "print(f\"\\nDocument names:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    filename = os.path.basename(doc.metadata.get('file_name', 'Unknown'))\n",
    "    print(f\"   {i}. {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c966c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk documents\n",
    "splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Created {len(nodes)} chunks from {len(documents)} documents\")\n",
    "print(f\"   Average chunks per document: {len(nodes) / len(documents):.1f}\")\n",
    "\n",
    "# Show sample chunk\n",
    "print(f\"\\nüìÑ Sample chunk (first 300 chars):\")\n",
    "print(f\"   {nodes[0].text[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ad75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "index = VectorStoreIndex(nodes, embed_model=azure_embed)\n",
    "print(\"‚úÖ Vector index created successfully with Azure OpenAI embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fbeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure baseline retriever with high top_k\n",
    "# We retrieve 20 chunks to simulate a realistic scenario where initial retrieval includes noise\n",
    "baseline_retriever = index.as_retriever(similarity_top_k=20)\n",
    "\n",
    "print(\"‚úÖ Baseline retriever configured (top_k=20)\")\n",
    "print(\"   This will retrieve a larger set that likely includes some irrelevant chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5bad6",
   "metadata": {},
   "source": [
    "## Step 2: Analyze Baseline Retrieval Quality\n",
    "\n",
    "Let's test the baseline retriever with a query that might retrieve some irrelevant content in the initial top-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd620f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query that spans multiple topics\n",
    "test_query = \"How do neural networks use backpropagation to learn features from data?\"\n",
    "\n",
    "print(f\"üîç Test Query: {test_query}\")\n",
    "print(\"\\nThis query is specifically about neural networks and backpropagation.\")\n",
    "print(\"We expect some retrieved chunks to be about other ML algorithms,\")\n",
    "print(\"which may have high cosine similarity but lower actual relevance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb68bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve with baseline\n",
    "baseline_results = baseline_retriever.retrieve(test_query)\n",
    "\n",
    "print(f\"\\nüìä Baseline Bi-Encoder Retrieval Results (Top 20):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, node in enumerate(baseline_results, 1):\n",
    "    score = node.score\n",
    "    filename = os.path.basename(node.node.metadata.get('file_name', 'Unknown'))\n",
    "    text_preview = node.node.text[:150].replace('\\n', ' ')\n",
    "    \n",
    "    print(f\"\\nRank {i:2d} | Score: {score:.4f} | Source: {filename}\")\n",
    "    print(f\"         Text: {text_preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079baaa",
   "metadata": {},
   "source": [
    "### Observations on Baseline Results\n",
    "\n",
    "Let's analyze the quality of these results:\n",
    "- Are all top-20 results relevant to neural networks and backpropagation?\n",
    "- Do we see chunks from other ML algorithms (SVM, Random Forests, etc.) in the results?\n",
    "- Are the most relevant chunks always at the top?\n",
    "\n",
    "**The Issue**: Bi-encoders compute similarity based on cosine distance between independently-encoded vectors. This can lead to:\n",
    "- False positives from keyword overlap (e.g., \"learning\", \"data\", \"training\")\n",
    "- Missing nuanced semantic relationships\n",
    "- Sub-optimal ranking where less relevant docs appear higher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac43007",
   "metadata": {},
   "source": [
    "## Step 3: Implement Cross-Encoder Re-Ranking\n",
    "\n",
    "We'll use a Sentence-Transformers cross-encoder model that processes [query, document] pairs jointly.\n",
    "\n",
    "**Model**: `cross-encoder/ms-marco-MiniLM-L-6-v2`\n",
    "- Trained on MS MARCO dataset for passage ranking\n",
    "- Lightweight (6 layers)\n",
    "- High accuracy for re-ranking tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e5947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import List, Optional\n",
    "\n",
    "# Load cross-encoder model\n",
    "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"‚úÖ Cross-encoder model loaded: cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8029076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom re-ranker post-processor\n",
    "from llama_index.core.postprocessor import BaseNodePostprocessor\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "class CrossEncoderReranker(BaseNodePostprocessor):\n",
    "    \"\"\"\n",
    "    Custom post-processor that uses a cross-encoder model to re-rank retrieved nodes.\n",
    "    \n",
    "    The cross-encoder processes [query, document] pairs together, allowing for\n",
    "    deep attention between query and document tokens, resulting in more accurate\n",
    "    relevance scores compared to bi-encoder cosine similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, top_n: int = 5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Sentence-Transformers CrossEncoder model\n",
    "            top_n: Number of top results to return after re-ranking\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.top_n = top_n\n",
    "        self._metadata = {}\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_bundle: Optional[QueryBundle] = None,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        Re-rank nodes using cross-encoder model.\n",
    "        \"\"\"\n",
    "        if query_bundle is None:\n",
    "            return nodes\n",
    "        \n",
    "        query_str = query_bundle.query_str\n",
    "        \n",
    "        # Prepare [query, document] pairs for cross-encoder\n",
    "        pairs = [[query_str, node.node.text] for node in nodes]\n",
    "        \n",
    "        # Get cross-encoder scores\n",
    "        scores = self.model.predict(pairs)\n",
    "        \n",
    "        # Update node scores\n",
    "        for node, score in zip(nodes, scores):\n",
    "            node.score = float(score)\n",
    "        \n",
    "        # Sort by new scores (descending)\n",
    "        nodes = sorted(nodes, key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        # Return top_n\n",
    "        return nodes[:self.top_n]\n",
    "\n",
    "# Create re-ranker instance\n",
    "reranker = CrossEncoderReranker(model=cross_encoder_model, top_n=5)\n",
    "print(\"‚úÖ Custom CrossEncoderReranker created (will return top-5 after re-ranking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04375fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine with re-ranking\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Standard query engine (no re-ranking)\n",
    "baseline_query_engine = RetrieverQueryEngine(\n",
    "    retriever=baseline_retriever,\n",
    "    node_postprocessors=[],  # No post-processing\n",
    ")\n",
    "\n",
    "# Query engine with re-ranking\n",
    "rerank_query_engine = RetrieverQueryEngine(\n",
    "    retriever=baseline_retriever,\n",
    "    node_postprocessors=[reranker],  # Apply cross-encoder re-ranking\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Query engines created:\")\n",
    "print(\"   1. Baseline (bi-encoder only, top-20)\")\n",
    "print(\"   2. Re-ranked (bi-encoder ‚Üí cross-encoder, top-5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da2470",
   "metadata": {},
   "source": [
    "## Step 4: Execute Re-Ranking and Compare Results\n",
    "\n",
    "Now let's retrieve and re-rank with the same query to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53011116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve with baseline (top-20)\n",
    "print(f\"üîç Query: {test_query}\\n\")\n",
    "baseline_results = baseline_retriever.retrieve(test_query)\n",
    "\n",
    "print(f\"üìä STAGE 1: Bi-Encoder Retrieval (Top 20)\")\n",
    "print(\"=\" * 80)\n",
    "for i, node in enumerate(baseline_results[:10], 1):  # Show first 10 for brevity\n",
    "    score = node.score\n",
    "    filename = os.path.basename(node.node.metadata.get('file_name', 'Unknown'))\n",
    "    text_preview = node.node.text[:100].replace('\\n', ' ')\n",
    "    print(f\"Rank {i:2d} | Bi-Score: {score:.4f} | {filename}\")\n",
    "    print(f\"         {text_preview}...\\n\")\n",
    "\n",
    "print(\"... (10 more results with scores ranging lower) ...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rank with cross-encoder\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "query_bundle = QueryBundle(query_str=test_query)\n",
    "reranked_results = reranker._postprocess_nodes(baseline_results, query_bundle=query_bundle)\n",
    "\n",
    "print(f\"üìä STAGE 2: Cross-Encoder Re-Ranking (Top 5)\")\n",
    "print(\"=\" * 80)\n",
    "for i, node in enumerate(reranked_results, 1):\n",
    "    cross_score = node.score\n",
    "    filename = os.path.basename(node.node.metadata.get('file_name', 'Unknown'))\n",
    "    text_preview = node.node.text[:100].replace('\\n', ' ')\n",
    "    \n",
    "    # Find original rank in baseline results\n",
    "    original_rank = None\n",
    "    for orig_idx, orig_node in enumerate(baseline_results, 1):\n",
    "        if orig_node.node.node_id == node.node.node_id:\n",
    "            original_rank = orig_idx\n",
    "            break\n",
    "    \n",
    "    rank_change = f\" (‚Üë moved from rank {original_rank})\" if original_rank and original_rank > i else \"\"\n",
    "    \n",
    "    print(f\"Rank {i} | Cross-Score: {cross_score:.4f} | {filename}{rank_change}\")\n",
    "    print(f\"        {text_preview}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466a466",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Rank Changes\n",
    "\n",
    "Let's create a visualization showing how documents moved in ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95525726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Track rank changes\n",
    "rank_changes = []\n",
    "labels = []\n",
    "\n",
    "for new_rank, node in enumerate(reranked_results, 1):\n",
    "    # Find original rank\n",
    "    for orig_rank, orig_node in enumerate(baseline_results, 1):\n",
    "        if orig_node.node.node_id == node.node.node_id:\n",
    "            filename = os.path.basename(node.node.metadata.get('file_name', 'Unknown'))\n",
    "            rank_changes.append((orig_rank, new_rank))\n",
    "            labels.append(f\"{filename[:20]}...\")\n",
    "            break\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, (orig, new) in enumerate(rank_changes):\n",
    "    color = 'green' if orig > 5 else 'blue'\n",
    "    ax.plot([0, 1], [orig, new], 'o-', color=color, linewidth=2, markersize=8)\n",
    "    ax.text(-0.05, orig, f\"{orig}\", ha='right', va='center', fontsize=10)\n",
    "    ax.text(1.05, new, f\"{new}\", ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-0.2, 1.2)\n",
    "ax.set_ylim(0, 21)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Bi-Encoder\\n(Top 20)', 'Cross-Encoder\\n(Top 5)'], fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Rank Position', fontsize=12)\n",
    "ax.set_title('Re-Ranking Impact: Document Rank Changes', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', linewidth=2, label='Already in top-5'),\n",
    "    Line2D([0], [0], color='green', linewidth=2, label='Promoted to top-5')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Rank Changes Summary:\")\n",
    "for i, (orig, new) in enumerate(rank_changes):\n",
    "    change = orig - new\n",
    "    arrow = \"‚Üë\" if change > 0 else \"‚Üí\"\n",
    "    print(f\"   {labels[i]}: Rank {orig} ‚Üí {new} {arrow} (moved {abs(change)} positions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a03d726",
   "metadata": {},
   "source": [
    "## Step 6: Compare Generated Answers\n",
    "\n",
    "Let's see how the quality of retrieved context affects the final generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer with baseline (top-5 from bi-encoder)\n",
    "baseline_top5_retriever = index.as_retriever(similarity_top_k=5)\n",
    "baseline_top5_engine = RetrieverQueryEngine(retriever=baseline_top5_retriever)\n",
    "\n",
    "print(\"ü§ñ Generating answer with BASELINE approach (Bi-encoder top-5 only)...\\n\")\n",
    "baseline_response = baseline_top5_engine.query(test_query)\n",
    "\n",
    "print(\"üìù BASELINE Answer (Bi-Encoder Top-5):\")\n",
    "print(\"=\" * 80)\n",
    "print(baseline_response.response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7aeff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer with re-ranking (top-5 from cross-encoder)\n",
    "print(\"\\nü§ñ Generating answer with RE-RANKING approach (Cross-encoder top-5)...\\n\")\n",
    "reranked_response = rerank_query_engine.query(test_query)\n",
    "\n",
    "print(\"üìù RE-RANKED Answer (Cross-Encoder Top-5):\")\n",
    "print(\"=\" * 80)\n",
    "print(reranked_response.response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd896f",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Answer Quality\n",
    "\n",
    "Let's use the LLM as a judge to compare the quality of both answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-judge evaluation\n",
    "judge_prompt = f\"\"\"\n",
    "You are an expert evaluator. Compare two answers to the following question and determine which is better.\n",
    "\n",
    "Question: {test_query}\n",
    "\n",
    "Answer A (Baseline - Bi-Encoder):\n",
    "{baseline_response.response}\n",
    "\n",
    "Answer B (Re-Ranked - Cross-Encoder):\n",
    "{reranked_response.response}\n",
    "\n",
    "Evaluate based on:\n",
    "1. Accuracy: Does it correctly answer the question?\n",
    "2. Completeness: Does it cover all aspects of the question?\n",
    "3. Relevance: Does it stay focused on the question?\n",
    "4. Clarity: Is it well-structured and easy to understand?\n",
    "\n",
    "Provide:\n",
    "- Brief analysis of each answer's strengths/weaknesses\n",
    "- Overall verdict: Which is better and why?\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚öñÔ∏è Using LLM as Judge to evaluate answer quality...\\n\")\n",
    "judgment = azure_llm.complete(judge_prompt)\n",
    "\n",
    "print(\"üéØ Quality Evaluation:\")\n",
    "print(\"=\" * 80)\n",
    "print(judgment.text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ee205",
   "metadata": {},
   "source": [
    "## Step 8: Test with Multiple Queries\n",
    "\n",
    "Let's test several different query types to demonstrate the robustness of re-ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd39ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define diverse test queries\n",
    "test_queries = [\n",
    "    \"What are the main differences between BERT and GPT-4 architectures?\",\n",
    "    \"How does k-means clustering determine the optimal number of clusters?\",\n",
    "    \"Explain the concept of containerization in Docker and its benefits.\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing re-ranking across diverse queries...\\n\")\n",
    "\n",
    "for idx, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Query {idx}: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Bi-encoder results\n",
    "    bi_results = baseline_top5_retriever.retrieve(query)\n",
    "    \n",
    "    # Cross-encoder re-ranking\n",
    "    qb = QueryBundle(query_str=query)\n",
    "    bi_results_20 = baseline_retriever.retrieve(query)\n",
    "    cross_results = reranker._postprocess_nodes(bi_results_20, query_bundle=qb)\n",
    "    \n",
    "    print(f\"\\nüìä Top-3 Sources Comparison:\")\n",
    "    print(f\"\\nBi-Encoder Top-3:\")\n",
    "    for i, node in enumerate(bi_results[:3], 1):\n",
    "        filename = os.path.basename(node.node.metadata.get('file_name', 'Unknown'))\n",
    "        print(f\"  {i}. {filename} (score: {node.score:.4f})\")\n",
    "    \n",
    "    print(f\"\\nCross-Encoder Top-3:\")\n",
    "    for i, node in enumerate(cross_results[:3], 1):\n",
    "        filename = os.path.basename(node.node.metadata.get('file_name', 'Unknown'))\n",
    "        print(f\"  {i}. {filename} (score: {node.score:.4f})\")\n",
    "    \n",
    "    # Check if re-ranking changed the top-3\n",
    "    bi_top3_ids = {n.node.node_id for n in bi_results[:3]}\n",
    "    cross_top3_ids = {n.node.node_id for n in cross_results[:3]}\n",
    "    \n",
    "    if bi_top3_ids != cross_top3_ids:\n",
    "        print(f\"\\n‚úÖ Re-ranking changed the top-3 results (improved precision)\")\n",
    "    else:\n",
    "        print(f\"\\n‚Üí Re-ranking confirmed the top-3 results (validation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165c132",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. **Two-Stage Architecture is Optimal**\n",
    "- **Stage 1 (Bi-Encoder)**: Fast retrieval over millions of documents\n",
    "- **Stage 2 (Cross-Encoder)**: Accurate re-ranking of a smaller candidate set\n",
    "- This combines the best of both worlds: speed + accuracy\n",
    "\n",
    "### 2. **Cross-Encoders Provide Superior Precision**\n",
    "- Deep attention between query and document captures nuanced semantics\n",
    "- More accurate than simple cosine similarity\n",
    "- Can identify subtle relevance differences that bi-encoders miss\n",
    "\n",
    "### 3. **When to Use Re-Ranking**\n",
    "‚úÖ **Use when:**\n",
    "- You need high precision (e.g., question answering, search)\n",
    "- Initial retrieval includes noise or false positives\n",
    "- Query complexity requires deep semantic understanding\n",
    "- You have a two-stage budget: cheap first-pass, expensive second-pass\n",
    "\n",
    "‚ùå **Skip when:**\n",
    "- Latency is critical and bi-encoder quality is sufficient\n",
    "- Dataset is very small (< 100 documents)\n",
    "- Simple keyword matching suffices\n",
    "\n",
    "### 4. **Performance Considerations**\n",
    "- **Bi-Encoder**: ~1ms per query (pre-computed embeddings)\n",
    "- **Cross-Encoder**: ~50-100ms for 20 pairs (must compute on-the-fly)\n",
    "- Total: ~100ms for full pipeline (very acceptable for most applications)\n",
    "\n",
    "### 5. **Implementation Tips**\n",
    "- Start with top-20 to top-50 from bi-encoder\n",
    "- Re-rank to top-5 to top-10 for generation\n",
    "- Choose cross-encoder model based on domain:\n",
    "  - `ms-marco-MiniLM`: General web/passage ranking\n",
    "  - `ms-marco-TinyBERT`: Faster, slightly lower quality\n",
    "  - Domain-specific models: If available for your use case\n",
    "\n",
    "## Architecture Diagram\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Stage 1: Bi-Encoder Retrieval    ‚îÇ\n",
    "‚îÇ   (Fast, High Recall)               ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Retrieve top-20                 ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Cosine similarity               ‚îÇ\n",
    "‚îÇ   ‚Ä¢ ~1ms latency                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ 20 candidates\n",
    "              ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Stage 2: Cross-Encoder Re-Ranking  ‚îÇ\n",
    "‚îÇ  (Accurate, High Precision)         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Re-score all 20 pairs            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Deep attention                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Return top-5                     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ ~50-100ms latency                ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ 5 best chunks\n",
    "              ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ      LLM Generation                 ‚îÇ\n",
    "‚îÇ      (High-Quality Context)         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further improve your RAG system:\n",
    "1. **Experiment with different cross-encoder models** for your domain\n",
    "2. **Tune the top-K parameters** (Stage 1: K=20-50, Stage 2: K=5-10)\n",
    "3. **Combine with other techniques**: Hybrid search ‚Üí Re-ranking ‚Üí Context compression\n",
    "4. **Monitor performance**: Track retrieval precision and generation quality\n",
    "5. **Consider fine-tuning** cross-encoders on domain-specific data\n",
    "\n",
    "---\n",
    "\n",
    "**Demo Complete! ‚úÖ**\n",
    "\n",
    "You've successfully implemented re-ranking with cross-encoders and seen how two-stage retrieval dramatically improves precision in RAG systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
