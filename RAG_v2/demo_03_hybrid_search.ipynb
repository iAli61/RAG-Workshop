{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78e071a",
   "metadata": {},
   "source": [
    "# Demo #3: Hybrid Search - Combining Semantic and Keyword Retrieval\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo demonstrates how **Hybrid Search** combines the strengths of semantic (dense vector) search and keyword (sparse/BM25) search to achieve superior retrieval performance across diverse query types.\n",
    "\n",
    "### Core Concepts:\n",
    "- **Semantic Search (Dense Vectors)**: Understanding meaning and context\n",
    "- **Keyword Search (BM25)**: Exact term matching and statistical relevance\n",
    "- **Reciprocal Rank Fusion (RRF)**: Merging results from multiple retrievers\n",
    "- **Hybrid Retrieval**: Combining complementary search paradigms\n",
    "\n",
    "### Why Hybrid Search Works:\n",
    "\n",
    "**Semantic search alone** struggles with:\n",
    "- Exact technical terms, acronyms, product names\n",
    "- Rare or domain-specific terminology\n",
    "- Queries requiring precise matches\n",
    "\n",
    "**Keyword search alone** struggles with:\n",
    "- Semantic similarity (synonyms, paraphrases)\n",
    "- Conceptual queries without exact terms\n",
    "- Understanding context and meaning\n",
    "\n",
    "**Hybrid search** combines both:\n",
    "- Semantic search finds conceptually relevant documents\n",
    "- Keyword search ensures exact matches aren't missed\n",
    "- RRF intelligently merges both result sets\n",
    "\n",
    "### Demo Structure:\n",
    "1. Setup with technical documents\n",
    "2. Test pure semantic search\n",
    "3. Test pure keyword (BM25) search\n",
    "4. Implement hybrid search with RRF\n",
    "5. Comparative evaluation across query types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e7156",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell only once\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai llama-index-retrievers-bm25 python-dotenv rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# LlamaIndex core components\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever, QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# BM25 retriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# Azure OpenAI components\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f897e8d",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "llm_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "\n",
    "# Validate configuration\n",
    "if not all([api_key, azure_endpoint, llm_deployment, embedding_deployment]):\n",
    "    raise ValueError(\"Missing required Azure OpenAI configuration. Check your .env file.\")\n",
    "\n",
    "print(\"âœ“ Azure OpenAI configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI models\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=llm_deployment,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=embedding_deployment,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"âœ“ Azure OpenAI models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaadf352",
   "metadata": {},
   "source": [
    "## 3. Load Technical Documents\n",
    "\n",
    "We'll use documents with specific technical terms and acronyms (BERT, GPT-4, API, Docker) to demonstrate the difference between semantic and keyword search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = Path(\"./data/tech_docs\")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=str(data_dir),\n",
    "    required_exts=['.md']\n",
    ").load_data()\n",
    "\n",
    "print(f\"âœ“ Loaded {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    filename = Path(doc.metadata.get('file_name', 'unknown')).stem\n",
    "    print(f\"  - {filename} ({len(doc.text)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter and parse documents\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"âœ“ Created {len(nodes)} text chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"Text: {nodes[0].text[:200]}...\")\n",
    "print(f\"Source: {Path(nodes[0].metadata.get('file_name', 'unknown')).stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd163b",
   "metadata": {},
   "source": [
    "## 4. Pure Semantic Search (Dense Vectors)\n",
    "\n",
    "First, let's build a standard vector search baseline using Azure OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "vector_index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    embed_model=azure_embed\n",
    ")\n",
    "\n",
    "print(\"âœ“ Vector index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector retriever\n",
    "vector_retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "# Create query engine from vector retriever\n",
    "vector_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    llm=azure_llm\n",
    ")\n",
    "\n",
    "print(\"âœ“ Semantic search engine created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600719e4",
   "metadata": {},
   "source": [
    "## 5. Test Semantic Search with Different Query Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries of different types\n",
    "test_queries = [\n",
    "    # Exact term queries (should favor keyword search)\n",
    "    \"What is BERT?\",\n",
    "    \"Tell me about GPT-4\",\n",
    "    \"How do I use REST API?\",\n",
    "    \n",
    "    # Conceptual queries (should favor semantic search)\n",
    "    \"How do attention mechanisms work in deep learning?\",\n",
    "    \"What is a lightweight way to package applications?\",\n",
    "    \"How are words represented as vectors?\"\n",
    "]\n",
    "\n",
    "# Test with an exact term query\n",
    "exact_query = test_queries[0]\n",
    "print(f\"Test Query (Exact Term): {exact_query}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve using semantic search\n",
    "retrieved_nodes = vector_retriever.retrieve(exact_query)\n",
    "\n",
    "print(\"\\nğŸ“Š SEMANTIC SEARCH RESULTS (Dense Vectors)\")\n",
    "print(\"=\" * 100)\n",
    "for i, node in enumerate(retrieved_nodes, 1):\n",
    "    print(f\"\\n[Rank {i}] Score: {node.score:.4f} | Source: {Path(node.metadata.get('file_name', 'unknown')).stem}\")\n",
    "    print(f\"{node.text[:250]}...\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff29d7",
   "metadata": {},
   "source": [
    "## 6. Implement BM25 Keyword Search\n",
    "\n",
    "Now let's create a BM25 retriever for keyword-based search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b41f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "print(\"âœ“ BM25 retriever created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BM25 with the same query\n",
    "bm25_nodes = bm25_retriever.retrieve(exact_query)\n",
    "\n",
    "print(f\"\\nQuery: {exact_query}\")\n",
    "print(\"\\nğŸ” BM25 KEYWORD SEARCH RESULTS (Sparse)\")\n",
    "print(\"=\" * 100)\n",
    "for i, node in enumerate(bm25_nodes, 1):\n",
    "    print(f\"\\n[Rank {i}] Score: {node.score:.4f} | Source: {Path(node.metadata.get('file_name', 'unknown')).stem}\")\n",
    "    print(f\"{node.text[:250]}...\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8932bd5",
   "metadata": {},
   "source": [
    "## 7. Create Hybrid Retriever with Reciprocal Rank Fusion\n",
    "\n",
    "Now let's combine both retrievers using QueryFusionRetriever with RRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937eb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hybrid retriever using Query Fusion\n",
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=5,\n",
    "    num_queries=1,  # Use original query only (no query generation)\n",
    "    mode=\"reciprocal_rerank\",  # Use Reciprocal Rank Fusion\n",
    "    use_async=False\n",
    ")\n",
    "\n",
    "print(\"âœ“ Hybrid retriever created with Reciprocal Rank Fusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine from hybrid retriever\n",
    "hybrid_engine = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever,\n",
    "    llm=azure_llm\n",
    ")\n",
    "\n",
    "print(\"âœ“ Hybrid search query engine created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056be35f",
   "metadata": {},
   "source": [
    "## 8. Test Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve using hybrid search\n",
    "hybrid_nodes = hybrid_retriever.retrieve(exact_query)\n",
    "\n",
    "print(f\"\\nQuery: {exact_query}\")\n",
    "print(\"\\nğŸš€ HYBRID SEARCH RESULTS (Dense + Sparse + RRF)\")\n",
    "print(\"=\" * 100)\n",
    "for i, node in enumerate(hybrid_nodes, 1):\n",
    "    print(f\"\\n[Rank {i}] Score: {node.score:.4f} | Source: {Path(node.metadata.get('file_name', 'unknown')).stem}\")\n",
    "    print(f\"{node.text[:250]}...\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a734f",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Comparison Across Query Types\n",
    "\n",
    "Let's systematically compare all three approaches across different query types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_approaches(query: str, vector_ret, bm25_ret, hybrid_ret):\n",
    "    \"\"\"Compare three retrieval approaches for a given query.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Retrieve from all three\n",
    "    vector_nodes = vector_ret.retrieve(query)\n",
    "    bm25_nodes = bm25_ret.retrieve(query)\n",
    "    hybrid_nodes = hybrid_ret.retrieve(query)\n",
    "    \n",
    "    # Extract sources\n",
    "    vector_sources = [Path(n.metadata.get('file_name', 'unknown')).stem for n in vector_nodes]\n",
    "    bm25_sources = [Path(n.metadata.get('file_name', 'unknown')).stem for n in bm25_nodes]\n",
    "    hybrid_sources = [Path(n.metadata.get('file_name', 'unknown')).stem for n in hybrid_nodes]\n",
    "    \n",
    "    # Display top result from each\n",
    "    print(\"\\nğŸ“Š Semantic (Vector) - Top Result:\")\n",
    "    print(f\"   Source: {vector_sources[0]}\")\n",
    "    print(f\"   Score: {vector_nodes[0].score:.4f}\")\n",
    "    print(f\"   Preview: {vector_nodes[0].text[:150]}...\")\n",
    "    \n",
    "    print(\"\\nğŸ” Keyword (BM25) - Top Result:\")\n",
    "    print(f\"   Source: {bm25_sources[0]}\")\n",
    "    print(f\"   Score: {bm25_nodes[0].score:.4f}\")\n",
    "    print(f\"   Preview: {bm25_nodes[0].text[:150]}...\")\n",
    "    \n",
    "    print(\"\\nğŸš€ Hybrid (RRF) - Top Result:\")\n",
    "    print(f\"   Source: {hybrid_sources[0]}\")\n",
    "    print(f\"   Score: {hybrid_nodes[0].score:.4f}\")\n",
    "    print(f\"   Preview: {hybrid_nodes[0].text[:150]}...\")\n",
    "    \n",
    "    # Show ranking comparison\n",
    "    print(\"\\nğŸ“‹ Top-5 Source Rankings:\")\n",
    "    print(f\"   Semantic: {vector_sources}\")\n",
    "    print(f\"   Keyword:  {bm25_sources}\")\n",
    "    print(f\"   Hybrid:   {hybrid_sources}\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(\"âœ“ Comparison function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364eecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with exact term queries (favor keyword search)\n",
    "print(\"\\n\" + \"#\" * 100)\n",
    "print(\"# EXACT TERM QUERIES (Should favor keyword/BM25 search)\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "exact_queries = [\n",
    "    \"What is BERT?\",\n",
    "    \"Tell me about Docker containers\",\n",
    "    \"How do REST APIs work?\"\n",
    "]\n",
    "\n",
    "for query in exact_queries:\n",
    "    compare_retrieval_approaches(query, vector_retriever, bm25_retriever, hybrid_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c8e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with conceptual queries (favor semantic search)\n",
    "print(\"\\n\" + \"#\" * 100)\n",
    "print(\"# CONCEPTUAL QUERIES (Should favor semantic/vector search)\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "conceptual_queries = [\n",
    "    \"How do attention mechanisms work in deep learning models?\",\n",
    "    \"What's a lightweight way to package and deploy applications?\",\n",
    "    \"How are words represented as numerical vectors?\"\n",
    "]\n",
    "\n",
    "for query in conceptual_queries:\n",
    "    compare_retrieval_approaches(query, vector_retriever, bm25_retriever, hybrid_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5904882",
   "metadata": {},
   "source": [
    "## 10. Understanding Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "Let's understand how RRF combines rankings from multiple retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd091ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    RECIPROCAL RANK FUSION (RRF) EXPLAINED                      â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "RRF is a simple yet effective method to merge rankings from multiple retrievers.\n",
    "\n",
    "FORMULA:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "RRF_score(doc) = Î£(1 / (k + rank_i(doc)))\n",
    "\n",
    "Where:\n",
    "- k = constant (typically 60) to prevent division by zero\n",
    "- rank_i(doc) = rank of document in retriever i\n",
    "- Î£ = sum across all retrievers\n",
    "\n",
    "EXAMPLE:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Document A appears at:\n",
    "  - Rank 1 in Semantic Search\n",
    "  - Rank 5 in BM25 Search\n",
    "\n",
    "RRF_score(A) = 1/(60+1) + 1/(60+5)\n",
    "             = 1/61 + 1/65\n",
    "             = 0.0164 + 0.0154\n",
    "             = 0.0318\n",
    "\n",
    "Document B appears at:\n",
    "  - Rank 3 in Semantic Search\n",
    "  - Rank 2 in BM25 Search\n",
    "\n",
    "RRF_score(B) = 1/(60+3) + 1/(60+2)\n",
    "             = 1/63 + 1/62\n",
    "             = 0.0159 + 0.0161\n",
    "             = 0.0320\n",
    "\n",
    "Result: Document B ranks higher (0.0320 > 0.0318)\n",
    "\n",
    "KEY ADVANTAGES:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ No score normalization needed (different retrievers have different score ranges)\n",
    "âœ“ Simple and interpretable\n",
    "âœ“ Gives higher weight to top-ranked documents\n",
    "âœ“ Balances multiple retrieval signals\n",
    "âœ“ Robust to outliers\n",
    "\n",
    "WHY IT WORKS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Documents appearing in top ranks of MULTIPLE retrievers get boosted\n",
    "- Single retriever mistakes are mitigated\n",
    "- Leverages strengths of different retrieval paradigms\n",
    "- Documents must be relevant by MULTIPLE criteria\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb199da",
   "metadata": {},
   "source": [
    "## 11. Generate Answers and Compare Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test answer generation with all three approaches\n",
    "test_query = \"What is BERT and how does it differ from previous language models?\"\n",
    "\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Semantic search answer\n",
    "vector_response = vector_engine.query(test_query)\n",
    "print(\"\\nğŸ“Š SEMANTIC SEARCH ANSWER:\")\n",
    "print(vector_response.response)\n",
    "print(f\"\\nSources: {[Path(n.metadata.get('file_name', 'unknown')).stem for n in vector_response.source_nodes]}\")\n",
    "\n",
    "# Hybrid search answer\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "hybrid_response = hybrid_engine.query(test_query)\n",
    "print(\"\\nğŸš€ HYBRID SEARCH ANSWER:\")\n",
    "print(hybrid_response.response)\n",
    "print(f\"\\nSources: {[Path(n.metadata.get('file_name', 'unknown')).stem for n in hybrid_response.source_nodes]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61a1c9",
   "metadata": {},
   "source": [
    "## 12. Visualize Data Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         HYBRID SEARCH DATA FLOW                                â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SEMANTIC SEARCH (Dense Vectors):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Query: \"What is BERT?\"\n",
    "   â”‚\n",
    "   â–¼\n",
    "Embed Query with Azure OpenAI\n",
    "   â”‚\n",
    "   â–¼\n",
    "Vector: [0.023, -0.145, 0.891, ...] (1536 dimensions)\n",
    "   â”‚\n",
    "   â–¼\n",
    "Cosine Similarity with Document Embeddings\n",
    "   â”‚\n",
    "   â–¼\n",
    "Ranked Results (based on semantic similarity)\n",
    "\n",
    "\n",
    "KEYWORD SEARCH (BM25):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Query: \"What is BERT?\"\n",
    "   â”‚\n",
    "   â–¼\n",
    "Tokenize: [\"what\", \"is\", \"BERT\"]\n",
    "   â”‚\n",
    "   â–¼\n",
    "BM25 Scoring:\n",
    "  - Term frequency in document\n",
    "  - Inverse document frequency\n",
    "  - Document length normalization\n",
    "   â”‚\n",
    "   â–¼\n",
    "Ranked Results (based on term statistics)\n",
    "\n",
    "\n",
    "HYBRID SEARCH WITH RRF:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Query: \"What is BERT?\"\n",
    "         â”‚\n",
    "         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                      â”‚\n",
    "         â–¼                      â–¼\n",
    "  Semantic Search         BM25 Search\n",
    "         â”‚                      â”‚\n",
    "         â–¼                      â–¼\n",
    "  [Doc A, Doc C, Doc B]   [Doc A, Doc B, Doc D]\n",
    "  (Ranks: 1, 2, 3)        (Ranks: 1, 2, 3)\n",
    "         â”‚                      â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â–¼\n",
    "         Reciprocal Rank Fusion\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "         Doc A: 1/61 + 1/61 = 0.0328  â† Top (in both at rank 1)\n",
    "         Doc B: 1/63 + 1/62 = 0.0320\n",
    "         Doc C: 1/62 + 0    = 0.0161\n",
    "         Doc D: 0    + 1/63 = 0.0159\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "         Final Ranking: [Doc A, Doc B, Doc C, Doc D]\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "              LLM Generation\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "              Final Answer\n",
    "\n",
    "\n",
    "KEY INSIGHT:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Documents appearing high in BOTH rankings get the highest combined scores.\n",
    "This ensures retrieved documents are relevant by MULTIPLE criteria!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92414f87",
   "metadata": {},
   "source": [
    "## 13. Key Takeaways and Best Practices\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Complementary strengths**: Semantic search excels at conceptual queries, while BM25 excels at exact matches.\n",
    "\n",
    "2. **RRF is simple and effective**: No score normalization needed, just rank-based fusion.\n",
    "\n",
    "3. **Hybrid search is robust**: Works well across diverse query types without query classification.\n",
    "\n",
    "4. **Best of both worlds**: Hybrid search retrieves documents that are relevant by MULTIPLE criteria.\n",
    "\n",
    "### When to Use Hybrid Search:\n",
    "\n",
    "âœ… **Use when:**\n",
    "- Query types are diverse (mix of exact terms and conceptual)\n",
    "- Domain has important acronyms, product names, or technical terms\n",
    "- Can't predict query patterns in advance\n",
    "- Retrieval precision is critical\n",
    "- Want robust performance without query classification\n",
    "\n",
    "âŒ **May not need when:**\n",
    "- All queries are purely semantic/conceptual\n",
    "- Documents don't contain specific terms that require exact matching\n",
    "- Computational budget is extremely limited\n",
    "- Simple vector search is already performing well\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Test both retrievers separately first**: Understand their individual strengths and weaknesses.\n",
    "\n",
    "2. **Tune top-k appropriately**: Start with 5-10 results from each retriever.\n",
    "\n",
    "3. **Monitor retrieval quality**: Track which retriever contributes more to final rankings.\n",
    "\n",
    "4. **Consider weighted fusion**: Some implementations allow weighting retrievers differently.\n",
    "\n",
    "5. **Preprocessing matters**: Ensure consistent text normalization across both retrievers.\n",
    "\n",
    "6. **BM25 hyperparameters**: Tune k1 (term saturation) and b (length normalization) for your domain.\n",
    "\n",
    "### Trade-offs:\n",
    "\n",
    "**Advantages:**\n",
    "- Better precision across diverse queries\n",
    "- More robust to query variations\n",
    "- Leverages multiple relevance signals\n",
    "\n",
    "**Costs:**\n",
    "- Higher computational cost (two retrievers)\n",
    "- Slightly increased latency\n",
    "- More complex to debug and optimize\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In Demo #4, we'll explore **Hierarchical Retrieval** using the Parent Document Retriever pattern to solve the chunking trade-off problem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
