{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165e9b74",
   "metadata": {},
   "source": [
    "# Demo #6: Context Compression - Strategic Ordering and Pruning\n",
    "\n",
    "## Objective\n",
    "Show how intelligent context management‚Äîboth reordering to address \"lost in the middle\" and compression via extractive pruning‚Äîoptimizes LLM generation.\n",
    "\n",
    "## Core Concepts\n",
    "- **\"Lost in the Middle\" problem**: LLMs pay less attention to information in the middle of long contexts\n",
    "- **Strategic context reordering**: Placing most relevant information at the beginning and end\n",
    "- **Extractive context compression**: Sentence-level pruning to reduce noise and token count\n",
    "\n",
    "## The \"Lost in the Middle\" Problem\n",
    "\n",
    "Research has shown that LLMs exhibit a **U-shaped attention pattern**:\n",
    "- ‚úÖ **High attention** to information at the **beginning** of context\n",
    "- ‚úÖ **High attention** to information at the **end** of context  \n",
    "- ‚ùå **Low attention** to information in the **middle** of context\n",
    "\n",
    "This means that even with relevant information retrieved, the LLM might miss it if it's buried in the middle of a long context window.\n",
    "\n",
    "### Solution: Strategic Reordering\n",
    "```\n",
    "Standard Order:        Strategic Order:\n",
    "Rank 1 (best)         Rank 1 (best)      ‚Üê Beginning\n",
    "Rank 2                Rank 3\n",
    "Rank 3                Rank 4\n",
    "Rank 4                Rank 5\n",
    "...                   ...\n",
    "Rank N-1              Rank N-1\n",
    "Rank N (worst)        Rank 2 (2nd best)  ‚Üê End\n",
    "```\n",
    "\n",
    "## Context Compression\n",
    "\n",
    "Beyond reordering, we can also **compress** each chunk by:\n",
    "1. Splitting into sentences\n",
    "2. Scoring each sentence's relevance to the query\n",
    "3. Keeping only high-relevance sentences\n",
    "4. Reducing token count while maintaining information density\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Query\n",
    "  ‚Üì\n",
    "Retrieve top-15 chunks\n",
    "  ‚Üì\n",
    "For each chunk: Split into sentences ‚Üí Score each ‚Üí Prune low-relevance\n",
    "  ‚Üì\n",
    "Reorder: [Best, Mid3, Mid4, ..., MidN-1, 2nd-Best]\n",
    "  ‚Üì\n",
    "Compressed, strategically-ordered context ‚Üí LLM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34bb4d",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies and Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db58b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell if packages are not already installed\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai\n",
    "# !pip install python-dotenv nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import tiktoken\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify Azure OpenAI credentials\n",
    "required_vars = [\n",
    "    'AZURE_OPENAI_API_KEY',\n",
    "    'AZURE_OPENAI_ENDPOINT',\n",
    "    'AZURE_OPENAI_API_VERSION',\n",
    "    'AZURE_OPENAI_DEPLOYMENT_NAME',\n",
    "    'AZURE_OPENAI_EMBEDDING_DEPLOYMENT'\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "if missing_vars:\n",
    "    print(f\"‚ùå Missing environment variables: {', '.join(missing_vars)}\")\n",
    "    print(\"Please ensure your .env file contains all required Azure OpenAI credentials.\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables are set.\")\n",
    "    print(f\"   Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "    print(f\"   LLM Deployment: {os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')}\")\n",
    "    print(f\"   Embedding Deployment: {os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d67767",
   "metadata": {},
   "source": [
    "## Step 1: Setup with Long Context\n",
    "\n",
    "We'll create a scenario requiring retrieval of many chunks (15) to properly demonstrate the \"lost in the middle\" problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Initialize Azure OpenAI LLM\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI Embedding Model\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "\n",
    "print(\"‚úÖ Azure OpenAI LLM and Embedding models initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b951946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all documents (ML + Tech + Long-form)\n",
    "ml_docs = SimpleDirectoryReader('data/ml_concepts').load_data()\n",
    "tech_docs = SimpleDirectoryReader('data/tech_docs').load_data()\n",
    "long_docs = SimpleDirectoryReader('data/long_form_docs').load_data()\n",
    "documents = ml_docs + tech_docs + long_docs\n",
    "\n",
    "print(f\"üìö Loaded {len(documents)} documents:\")\n",
    "print(f\"   - ML Concepts: {len(ml_docs)} documents\")\n",
    "print(f\"   - Tech Docs: {len(tech_docs)} documents\")\n",
    "print(f\"   - Long-form: {len(long_docs)} documents\")\n",
    "print(f\"\\nThis diverse mix will allow us to retrieve 15 chunks with varying relevance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf17638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk documents with smaller chunk size to get more chunks\n",
    "splitter = SentenceSplitter(chunk_size=400, chunk_overlap=50)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Created {len(nodes)} chunks from {len(documents)} documents\")\n",
    "print(f\"   Average chunks per document: {len(nodes) / len(documents):.1f}\")\n",
    "print(f\"   Chunk size: 400 tokens (smaller for more granularity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cfd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "index = VectorStoreIndex(nodes, embed_model=azure_embed)\n",
    "\n",
    "# Create retriever with high top-k\n",
    "retriever = index.as_retriever(similarity_top_k=15)\n",
    "\n",
    "print(\"‚úÖ Vector index and retriever created (top_k=15)\")\n",
    "print(\"   This will retrieve enough chunks to demonstrate the 'lost in the middle' problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69558a8a",
   "metadata": {},
   "source": [
    "## Step 2: Demonstrate \"Lost in the Middle\" Problem\n",
    "\n",
    "Let's retrieve 15 chunks and see their distribution. We'll use a query that has relevant information spread across the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"Explain the advantages and limitations of different machine learning algorithms for classification tasks, including neural networks, random forests, and support vector machines.\"\n",
    "\n",
    "print(f\"üîç Test Query:\")\n",
    "print(f\"{test_query}\\n\")\n",
    "print(\"This query requires information from multiple sources.\")\n",
    "print(\"We expect relevant information to be distributed across all 15 retrieved chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b88e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve chunks\n",
    "retrieved_nodes = retriever.retrieve(test_query)\n",
    "\n",
    "print(f\"\\nüìä Retrieved {len(retrieved_nodes)} chunks:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, node in enumerate(retrieved_nodes, 1):\n",
    "    score = node.score\n",
    "    filename = os.path.basename(node.node.metadata.get('file_name', 'Unknown'))\n",
    "    text_preview = node.node.text[:120].replace('\\n', ' ')\n",
    "    \n",
    "    # Mark position in context\n",
    "    position = \"START\" if i <= 2 else (\"END\" if i >= 14 else \"MIDDLE\")\n",
    "    attention = \"HIGH\" if position in [\"START\", \"END\"] else \"LOW\"\n",
    "    \n",
    "    print(f\"Rank {i:2d} | Score: {score:.4f} | Position: {position:6s} | Attention: {attention}\")\n",
    "    print(f\"         Source: {filename}\")\n",
    "    print(f\"         Text: {text_preview}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa10905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline query engine (standard ordering)\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "baseline_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    llm=azure_llm\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Generating baseline answer (standard ordering)...\\n\")\n",
    "baseline_response = baseline_engine.query(test_query)\n",
    "\n",
    "print(\"üìù BASELINE Answer (Standard Ordering):\")\n",
    "print(\"=\" * 80)\n",
    "print(baseline_response.response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc16bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens in baseline context\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "baseline_context = \"\\n\\n\".join([node.node.text for node in retrieved_nodes])\n",
    "baseline_tokens = len(encoding.encode(baseline_context))\n",
    "\n",
    "print(f\"\\nüìä Baseline Context Statistics:\")\n",
    "print(f\"   Total chunks: {len(retrieved_nodes)}\")\n",
    "print(f\"   Total tokens: {baseline_tokens:,}\")\n",
    "print(f\"   Avg tokens per chunk: {baseline_tokens / len(retrieved_nodes):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15eb6b",
   "metadata": {},
   "source": [
    "## Step 3: Implement Strategic Reordering\n",
    "\n",
    "We'll create a custom post-processor that reorders chunks to place the most relevant information at the beginning and second-most relevant at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21159f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import BaseNodePostprocessor\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import List, Optional\n",
    "\n",
    "class StrategicReorderProcessor(BaseNodePostprocessor):\n",
    "    \"\"\"\n",
    "    Reorder nodes to address the 'lost in the middle' problem.\n",
    "    \n",
    "    Strategy:\n",
    "    - Most relevant (rank 1) ‚Üí Beginning\n",
    "    - Second most relevant (rank 2) ‚Üí End\n",
    "    - Remaining nodes (rank 3-N) ‚Üí Middle (in descending order of relevance)\n",
    "    \n",
    "    This ensures high-attention positions (start/end) contain the best information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_bundle: Optional[QueryBundle] = None,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        Reorder nodes strategically.\n",
    "        \"\"\"\n",
    "        if len(nodes) <= 2:\n",
    "            return nodes\n",
    "        \n",
    "        # Nodes are already sorted by relevance (highest first)\n",
    "        best = nodes[0]              # Most relevant ‚Üí Start\n",
    "        second_best = nodes[1]       # Second most relevant ‚Üí End\n",
    "        middle_nodes = nodes[2:]     # Rest ‚Üí Middle\n",
    "        \n",
    "        # Construct reordered list\n",
    "        reordered = [best] + middle_nodes + [second_best]\n",
    "        \n",
    "        return reordered\n",
    "\n",
    "# Create reorder processor\n",
    "reorder_processor = StrategicReorderProcessor()\n",
    "print(\"‚úÖ Strategic Reorder Processor created\")\n",
    "print(\"   Will place: Best ‚Üí Start, 2nd-Best ‚Üí End, Rest ‚Üí Middle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31787324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine with reordering\n",
    "reordered_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[reorder_processor],\n",
    "    llm=azure_llm\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Generating answer with strategic reordering...\\n\")\n",
    "reordered_response = reordered_engine.query(test_query)\n",
    "\n",
    "print(\"üìù REORDERED Answer (Strategic Ordering):\")\n",
    "print(\"=\" * 80)\n",
    "print(reordered_response.response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfa9f2",
   "metadata": {},
   "source": [
    "## Step 4: Implement Extractive Compression\n",
    "\n",
    "Now we'll add sentence-level compression to reduce tokens while maintaining information density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f571794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class SentenceCompressionProcessor(BaseNodePostprocessor):\n",
    "    \"\"\"\n",
    "    Compress each node by keeping only the most relevant sentences.\n",
    "    \n",
    "    Steps:\n",
    "    1. Split each chunk into sentences\n",
    "    2. Embed each sentence\n",
    "    3. Score each sentence's similarity to the query\n",
    "    4. Keep top 50% of sentences (or sentences above threshold)\n",
    "    5. Reconstruct compressed chunk\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_model, threshold: float = 0.3, keep_ratio: float = 0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_model: Embedding model for scoring sentences\n",
    "            threshold: Minimum similarity score to keep a sentence\n",
    "            keep_ratio: Fraction of sentences to keep (0.5 = 50%)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_model = embed_model\n",
    "        self.threshold = threshold\n",
    "        self.keep_ratio = keep_ratio\n",
    "    \n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences using simple regex.\"\"\"\n",
    "        # Split on period, exclamation, or question mark followed by space\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        # Filter out empty strings and very short sentences\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
    "        return sentences\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_bundle: Optional[QueryBundle] = None,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        Compress each node by keeping only relevant sentences.\n",
    "        \"\"\"\n",
    "        if query_bundle is None:\n",
    "            return nodes\n",
    "        \n",
    "        query_str = query_bundle.query_str\n",
    "        query_embedding = self.embed_model.get_text_embedding(query_str)\n",
    "        \n",
    "        compressed_nodes = []\n",
    "        \n",
    "        for node in nodes:\n",
    "            # Split into sentences\n",
    "            sentences = self._split_sentences(node.node.text)\n",
    "            \n",
    "            if len(sentences) <= 2:\n",
    "                # Too short to compress\n",
    "                compressed_nodes.append(node)\n",
    "                continue\n",
    "            \n",
    "            # Embed each sentence\n",
    "            sentence_embeddings = [\n",
    "                self.embed_model.get_text_embedding(sent) \n",
    "                for sent in sentences\n",
    "            ]\n",
    "            \n",
    "            # Score each sentence\n",
    "            scores = [\n",
    "                np.dot(query_embedding, sent_emb) / \n",
    "                (np.linalg.norm(query_embedding) * np.linalg.norm(sent_emb))\n",
    "                for sent_emb in sentence_embeddings\n",
    "            ]\n",
    "            \n",
    "            # Determine sentences to keep\n",
    "            num_keep = max(2, int(len(sentences) * self.keep_ratio))\n",
    "            \n",
    "            # Get top sentences by score\n",
    "            scored_sentences = list(zip(sentences, scores, range(len(sentences))))\n",
    "            scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Keep top sentences and sort by original order\n",
    "            kept_sentences = scored_sentences[:num_keep]\n",
    "            kept_sentences.sort(key=lambda x: x[2])  # Restore original order\n",
    "            \n",
    "            # Reconstruct compressed text\n",
    "            compressed_text = \" \".join([s[0] for s in kept_sentences])\n",
    "            \n",
    "            # Create new node with compressed text\n",
    "            compressed_node = NodeWithScore(\n",
    "                node=node.node.copy(),\n",
    "                score=node.score\n",
    "            )\n",
    "            compressed_node.node.text = compressed_text\n",
    "            \n",
    "            compressed_nodes.append(compressed_node)\n",
    "        \n",
    "        return compressed_nodes\n",
    "\n",
    "# Create compression processor\n",
    "compression_processor = SentenceCompressionProcessor(\n",
    "    embed_model=azure_embed,\n",
    "    keep_ratio=0.5  # Keep top 50% of sentences\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sentence Compression Processor created\")\n",
    "print(\"   Will keep top 50% of sentences based on query relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ab639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine with both compression and reordering\n",
    "compressed_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[\n",
    "        compression_processor,  # First: Compress\n",
    "        reorder_processor       # Then: Reorder\n",
    "    ],\n",
    "    llm=azure_llm\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Generating answer with compression + strategic reordering...\\n\")\n",
    "compressed_response = compressed_engine.query(test_query)\n",
    "\n",
    "print(\"üìù COMPRESSED + REORDERED Answer:\")\n",
    "print(\"=\" * 80)\n",
    "print(compressed_response.response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dabd4cc",
   "metadata": {},
   "source": [
    "## Step 5: Comparative Analysis\n",
    "\n",
    "Let's compare all three approaches across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79039105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate token counts for each approach\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# Baseline context\n",
    "baseline_nodes = retriever.retrieve(test_query)\n",
    "baseline_context = \"\\n\\n\".join([n.node.text for n in baseline_nodes])\n",
    "baseline_tokens = len(encoding.encode(baseline_context))\n",
    "\n",
    "# Reordered context (same tokens, different order)\n",
    "qb = QueryBundle(query_str=test_query)\n",
    "reordered_nodes = reorder_processor._postprocess_nodes(baseline_nodes, query_bundle=qb)\n",
    "reordered_context = \"\\n\\n\".join([n.node.text for n in reordered_nodes])\n",
    "reordered_tokens = len(encoding.encode(reordered_context))\n",
    "\n",
    "# Compressed + Reordered context\n",
    "compressed_nodes = compression_processor._postprocess_nodes(baseline_nodes, query_bundle=qb)\n",
    "compressed_reordered_nodes = reorder_processor._postprocess_nodes(compressed_nodes, query_bundle=qb)\n",
    "compressed_context = \"\\n\\n\".join([n.node.text for n in compressed_reordered_nodes])\n",
    "compressed_tokens = len(encoding.encode(compressed_context))\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\nüìä Token Count Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Approach':<30} {'Tokens':<12} {'Reduction':<15} {'Efficiency'}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'1. Baseline (Standard)':<30} {baseline_tokens:<12,} {'-':<15} {'Baseline'}\")\n",
    "print(f\"{'2. Reordered (Full Context)':<30} {reordered_tokens:<12,} {'-':<15} {'Same'}\")\n",
    "reduction = ((baseline_tokens - compressed_tokens) / baseline_tokens) * 100\n",
    "print(f\"{'3. Compressed + Reordered':<30} {compressed_tokens:<12,} {f'{reduction:.1f}%':<15} {'Optimized'}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüí° Token Savings: {baseline_tokens - compressed_tokens:,} tokens ({reduction:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize context ordering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Baseline ordering\n",
    "positions = list(range(1, len(baseline_nodes) + 1))\n",
    "scores = [node.score for node in baseline_nodes]\n",
    "colors_baseline = ['green' if i <= 2 or i >= 14 else 'red' for i in positions]\n",
    "\n",
    "ax1.barh(positions, scores, color=colors_baseline, alpha=0.7)\n",
    "ax1.set_xlabel('Relevance Score', fontsize=11)\n",
    "ax1.set_ylabel('Position in Context', fontsize=11)\n",
    "ax1.set_title('Baseline: Standard Ordering', fontsize=13, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.axhspan(0.5, 2.5, alpha=0.2, color='green', label='High Attention (Start)')\n",
    "ax1.axhspan(13.5, 15.5, alpha=0.2, color='green', label='High Attention (End)')\n",
    "ax1.axhspan(2.5, 13.5, alpha=0.2, color='red', label='Low Attention (Middle)')\n",
    "ax1.legend(loc='lower right', fontsize=9)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Strategic reordering\n",
    "reordered_positions = list(range(1, len(reordered_nodes) + 1))\n",
    "reordered_scores = [node.score for node in reordered_nodes]\n",
    "colors_reordered = ['green' if i <= 2 or i >= 14 else 'orange' for i in reordered_positions]\n",
    "\n",
    "ax2.barh(reordered_positions, reordered_scores, color=colors_reordered, alpha=0.7)\n",
    "ax2.set_xlabel('Relevance Score', fontsize=11)\n",
    "ax2.set_ylabel('Position in Context', fontsize=11)\n",
    "ax2.set_title('Strategic: Reordered (Best ‚Üí Start, 2nd ‚Üí End)', fontsize=13, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.axhspan(0.5, 2.5, alpha=0.2, color='green', label='High Attention (Start)')\n",
    "ax2.axhspan(13.5, 15.5, alpha=0.2, color='green', label='High Attention (End)')\n",
    "ax2.axhspan(2.5, 13.5, alpha=0.2, color='orange', label='Lower Attention (Middle)')\n",
    "ax2.legend(loc='lower right', fontsize=9)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Visualization Insights:\")\n",
    "print(\"   LEFT (Baseline): Best information scattered throughout context\")\n",
    "print(\"   RIGHT (Reordered): Best information concentrated at high-attention positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2359665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-judge comparison\n",
    "judge_prompt = f\"\"\"\n",
    "You are an expert evaluator. Compare three answers to the following question:\n",
    "\n",
    "Question: {test_query}\n",
    "\n",
    "Answer A (Baseline - Standard Ordering):\n",
    "{baseline_response.response}\n",
    "\n",
    "Answer B (Strategic Reordering):\n",
    "{reordered_response.response}\n",
    "\n",
    "Answer C (Compressed + Reordered):\n",
    "{compressed_response.response}\n",
    "\n",
    "Evaluate based on:\n",
    "1. Completeness: Does it cover all three algorithms mentioned?\n",
    "2. Accuracy: Is the information correct?\n",
    "3. Balance: Does it give appropriate attention to each algorithm?\n",
    "4. Clarity: Is it well-structured?\n",
    "\n",
    "Rank the three answers (1st, 2nd, 3rd) and explain your reasoning briefly.\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚öñÔ∏è Using LLM as Judge to evaluate answer quality...\\n\")\n",
    "judgment = azure_llm.complete(judge_prompt)\n",
    "\n",
    "print(\"üéØ Quality Evaluation:\")\n",
    "print(\"=\" * 80)\n",
    "print(judgment.text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db286308",
   "metadata": {},
   "source": [
    "## Step 6: Test with Edge Cases\n",
    "\n",
    "Let's test scenarios where compression and reordering have the most impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a very specific query (where compression helps most)\n",
    "specific_query = \"What is the vanishing gradient problem in deep neural networks and how does it affect backpropagation?\"\n",
    "\n",
    "print(f\"üîç Specific Query Test: {specific_query}\\n\")\n",
    "\n",
    "# Get responses\n",
    "baseline_specific = baseline_engine.query(specific_query)\n",
    "compressed_specific = compressed_engine.query(specific_query)\n",
    "\n",
    "# Calculate token savings\n",
    "baseline_nodes_specific = retriever.retrieve(specific_query)\n",
    "qb_specific = QueryBundle(query_str=specific_query)\n",
    "compressed_nodes_specific = compression_processor._postprocess_nodes(baseline_nodes_specific, query_bundle=qb_specific)\n",
    "\n",
    "baseline_ctx_specific = \"\\n\\n\".join([n.node.text for n in baseline_nodes_specific])\n",
    "compressed_ctx_specific = \"\\n\\n\".join([n.node.text for n in compressed_nodes_specific])\n",
    "\n",
    "baseline_tok_specific = len(encoding.encode(baseline_ctx_specific))\n",
    "compressed_tok_specific = len(encoding.encode(compressed_ctx_specific))\n",
    "savings = baseline_tok_specific - compressed_tok_specific\n",
    "savings_pct = (savings / baseline_tok_specific) * 100\n",
    "\n",
    "print(f\"\\nüìä Results for Specific Query:\")\n",
    "print(f\"   Baseline tokens: {baseline_tok_specific:,}\")\n",
    "print(f\"   Compressed tokens: {compressed_tok_specific:,}\")\n",
    "print(f\"   Savings: {savings:,} tokens ({savings_pct:.1f}% reduction)\")\n",
    "\n",
    "print(f\"\\nüìù Baseline Answer (Standard):\")\n",
    "print(baseline_specific.response[:500] + \"...\\n\")\n",
    "\n",
    "print(f\"üìù Compressed Answer (Optimized):\")\n",
    "print(compressed_specific.response[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19051af2",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. **The \"Lost in the Middle\" Problem is Real**\n",
    "- LLMs exhibit U-shaped attention: high at start/end, low in middle\n",
    "- Relevant information in the middle may be underutilized\n",
    "- Strategic reordering addresses this without adding complexity\n",
    "\n",
    "### 2. **Strategic Reordering Improves Answer Quality**\n",
    "‚úÖ **Benefits:**\n",
    "- Places best information where LLM pays most attention\n",
    "- No additional compute cost (just reordering)\n",
    "- Simple to implement\n",
    "- Works with any retrieval method\n",
    "\n",
    "**Strategy:**\n",
    "```\n",
    "Position 1: Most relevant (Rank 1)\n",
    "Position 2-N-1: Middle-ranked (Rank 3 onwards)\n",
    "Position N: Second most relevant (Rank 2)\n",
    "```\n",
    "\n",
    "### 3. **Context Compression Reduces Costs**\n",
    "‚úÖ **Benefits:**\n",
    "- 30-50% token reduction typical\n",
    "- Removes irrelevant sentences within chunks\n",
    "- Maintains information density\n",
    "- Reduces API costs and latency\n",
    "\n",
    "‚ö†Ô∏è **Trade-offs:**\n",
    "- Additional embedding calls for sentence scoring\n",
    "- Risk of removing relevant sentences (if threshold too aggressive)\n",
    "- Adds processing latency\n",
    "\n",
    "### 4. **Combined Approach is Optimal**\n",
    "**Best Practice: Compress THEN Reorder**\n",
    "1. Retrieve top-K chunks (e.g., K=15)\n",
    "2. Compress each chunk (sentence-level pruning)\n",
    "3. Reorder chunks strategically\n",
    "4. Send to LLM\n",
    "\n",
    "This combines:\n",
    "- **Efficiency** (fewer tokens)\n",
    "- **Quality** (better attention alignment)\n",
    "- **Cost savings** (reduced API costs)\n",
    "\n",
    "### 5. **When to Use These Techniques**\n",
    "\n",
    "‚úÖ **Use Strategic Reordering when:**\n",
    "- Retrieving 10+ chunks\n",
    "- Answers require information from multiple chunks\n",
    "- Zero-cost optimization (just reordering)\n",
    "\n",
    "‚úÖ **Use Compression when:**\n",
    "- Token limits are a constraint\n",
    "- Chunks contain verbose or repetitive content\n",
    "- API costs are a concern\n",
    "- Query is specific (easier to identify relevant sentences)\n",
    "\n",
    "‚ùå **Skip when:**\n",
    "- Retrieving <5 chunks (reordering less impactful)\n",
    "- Chunks are already concise (compression minimal gains)\n",
    "- Latency is critical (compression adds processing time)\n",
    "\n",
    "### 6. **Implementation Considerations**\n",
    "\n",
    "**Compression Parameters:**\n",
    "- `keep_ratio=0.5`: Good default (50% of sentences)\n",
    "- `keep_ratio=0.7`: Conservative (less aggressive)\n",
    "- `keep_ratio=0.3`: Aggressive (maximum compression)\n",
    "\n",
    "**Reordering Variants:**\n",
    "- Simple: Best ‚Üí Start, 2nd-Best ‚Üí End\n",
    "- Advanced: Alternate high-relevance items at both ends\n",
    "- Domain-specific: Customize based on query type\n",
    "\n",
    "### 7. **Performance Metrics**\n",
    "\n",
    "Typical improvements:\n",
    "- **Token reduction**: 30-50% with compression\n",
    "- **Answer quality**: 10-20% improvement with reordering (subjective)\n",
    "- **Cost savings**: Proportional to token reduction\n",
    "- **Latency**: +100-200ms for compression (sentence embedding)\n",
    "\n",
    "## Architecture Diagram\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "Retrieval (top-15 chunks)\n",
    "    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  For Each Chunk:                ‚îÇ\n",
    "‚îÇ  1. Split into sentences        ‚îÇ\n",
    "‚îÇ  2. Embed each sentence          ‚îÇ\n",
    "‚îÇ  3. Score vs. query              ‚îÇ\n",
    "‚îÇ  4. Keep top 50% sentences       ‚îÇ\n",
    "‚îÇ  5. Reconstruct chunk            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    Compressed Chunks\n",
    "           ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Strategic Reordering:          ‚îÇ\n",
    "‚îÇ  [Best, Mid3, Mid4, ...,        ‚îÇ\n",
    "‚îÇ   MidN-1, 2nd-Best]             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "   Optimized Context\n",
    "   (50% fewer tokens,\n",
    "    strategic ordering)\n",
    "           ‚Üì\n",
    "    LLM Generation\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further optimize:\n",
    "1. **Experiment with compression ratios** for your use case\n",
    "2. **Try different reordering strategies** (e.g., alternate best chunks at start/end)\n",
    "3. **Combine with re-ranking**: Re-rank ‚Üí Compress ‚Üí Reorder\n",
    "4. **Monitor metrics**: Track token usage, costs, and answer quality\n",
    "5. **A/B test**: Compare approaches with real users\n",
    "\n",
    "---\n",
    "\n",
    "**Demo Complete! ‚úÖ**\n",
    "\n",
    "You've successfully implemented context compression and strategic reordering to optimize RAG performance while reducing costs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
