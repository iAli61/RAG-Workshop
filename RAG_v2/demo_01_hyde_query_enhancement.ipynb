{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e97eefc",
   "metadata": {},
   "source": [
    "# Demo #1: HyDE (Hypothetical Document Embeddings) - Query Enhancement\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo demonstrates how **HyDE (Hypothetical Document Embeddings)** can dramatically improve semantic retrieval by generating a hypothetical answer document before performing the search.\n",
    "\n",
    "### Core Concepts:\n",
    "- **Query Enhancement**: Transforming user queries before retrieval\n",
    "- **HyDE Paradigm**: Answer-to-answer similarity search (instead of question-to-answer)\n",
    "- **Pre-retrieval Optimization**: Using LLM-generated context to improve semantic matching\n",
    "\n",
    "### Why HyDE Works:\n",
    "Traditional RAG embeds the user's question and searches for similar documents. However:\n",
    "- Questions and answers often use different vocabulary\n",
    "- Questions are typically short and lack context\n",
    "- Documents contain answers, not questions\n",
    "\n",
    "HyDE solves this by:\n",
    "1. Using an LLM to generate a hypothetical answer to the question\n",
    "2. Embedding this hypothetical answer (which resembles actual documents)\n",
    "3. Searching for similar documents using answer-to-answer similarity\n",
    "\n",
    "### Demo Structure:\n",
    "1. Setup and data ingestion\n",
    "2. Build baseline RAG pipeline\n",
    "3. Implement HyDE enhancement\n",
    "4. Comparative evaluation\n",
    "5. Results visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc205472",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ef0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell only once\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a21159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# LlamaIndex core components\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "\n",
    "# Azure OpenAI components\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127d6bf",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI Connection\n",
    "\n",
    "**Important**: Create a `.env` file in the project root with your Azure OpenAI credentials:\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_API_KEY=your_api_key\n",
    "AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\n",
    "AZURE_OPENAI_API_VERSION=2024-02-15-preview\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=your-gpt4-deployment\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT=your-embedding-deployment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "llm_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "\n",
    "# Validate configuration\n",
    "if not all([api_key, azure_endpoint, llm_deployment, embedding_deployment]):\n",
    "    raise ValueError(\"Missing required Azure OpenAI configuration. Check your .env file.\")\n",
    "\n",
    "print(\"âœ“ Azure OpenAI configuration loaded\")\n",
    "print(f\"  Endpoint: {azure_endpoint}\")\n",
    "print(f\"  LLM Deployment: {llm_deployment}\")\n",
    "print(f\"  Embedding Deployment: {embedding_deployment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065782a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI LLM\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=llm_deployment,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    temperature=0.1,  # Low temperature for consistent responses\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI Embedding Model\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=embedding_deployment,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# Set global defaults for LlamaIndex\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"âœ“ Azure OpenAI models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340587f",
   "metadata": {},
   "source": [
    "## 3. Load and Process Documents\n",
    "\n",
    "We'll use a small knowledge base of machine learning concepts to demonstrate HyDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = Path(\"./data/ml_concepts\")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=str(data_dir),\n",
    "    required_exts=['.md']\n",
    ").load_data()\n",
    "\n",
    "print(f\"âœ“ Loaded {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    filename = Path(doc.metadata.get('file_name', 'unknown')).stem\n",
    "    print(f\"  - {filename} ({len(doc.text)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter for chunking\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Parse documents into nodes (chunks)\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"âœ“ Created {len(nodes)} text chunks\")\n",
    "print(f\"\\nExample chunk:\")\n",
    "print(f\"Text: {nodes[0].text[:200]}...\")\n",
    "print(f\"Source: {nodes[0].metadata.get('file_name', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3387d1d6",
   "metadata": {},
   "source": [
    "## 4. Build Baseline RAG Pipeline\n",
    "\n",
    "First, we'll create a standard RAG pipeline without HyDE to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bbb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    embed_model=azure_embed\n",
    ")\n",
    "\n",
    "print(\"âœ“ Vector index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee011f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline query engine\n",
    "baseline_query_engine = index.as_query_engine(\n",
    "    llm=azure_llm,\n",
    "    similarity_top_k=3  # Retrieve top 3 most similar chunks\n",
    ")\n",
    "\n",
    "print(\"âœ“ Baseline query engine created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9cbcba",
   "metadata": {},
   "source": [
    "## 5. Test Baseline RAG\n",
    "\n",
    "Let's test the baseline with a query that requires understanding ML concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0cee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test query\n",
    "test_query = \"What are the main differences between ensemble methods that build trees sequentially versus in parallel?\"\n",
    "\n",
    "print(f\"Test Query: {test_query}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute baseline query\n",
    "baseline_response = baseline_query_engine.query(test_query)\n",
    "\n",
    "print(\"\\nðŸ“Š BASELINE APPROACH (Standard RAG)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAnswer:\\n{baseline_response.response}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Retrieved Chunks:\")\n",
    "for i, node in enumerate(baseline_response.source_nodes, 1):\n",
    "    print(f\"\\n[Chunk {i}] Score: {node.score:.4f} | Source: {node.metadata.get('file_name', 'unknown')}\")\n",
    "    print(f\"{node.text[:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11745701",
   "metadata": {},
   "source": [
    "## 6. Implement HyDE Query Enhancement\n",
    "\n",
    "Now we'll enhance the query engine with HyDE. The process:\n",
    "1. User query â†’ LLM generates hypothetical answer\n",
    "2. Hypothetical answer â†’ Embedded\n",
    "3. Search using answer embedding (not query embedding)\n",
    "4. Retrieved chunks â†’ Final generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HyDE query transformation\n",
    "hyde_transform = HyDEQueryTransform(\n",
    "    llm=azure_llm,\n",
    "    include_original=False  # Use only the hypothetical document, not the original query\n",
    ")\n",
    "\n",
    "print(\"âœ“ HyDE transformation created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e093772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the baseline query engine with HyDE transformation\n",
    "hyde_query_engine = TransformQueryEngine(\n",
    "    query_engine=baseline_query_engine,\n",
    "    query_transform=hyde_transform\n",
    ")\n",
    "\n",
    "print(\"âœ“ HyDE-enhanced query engine created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65384f8c",
   "metadata": {},
   "source": [
    "## 7. Visualize HyDE Transformation\n",
    "\n",
    "Let's see what hypothetical document HyDE generates for our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa347d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hypothetical document to see the transformation\n",
    "from llama_index.core.schema import QueryBundle\n",
    "\n",
    "query_bundle = QueryBundle(query_str=test_query)\n",
    "transformed_query = hyde_transform.run(query_bundle)\n",
    "\n",
    "print(\"\\nðŸ”„ HYDE TRANSFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOriginal Query:\\n{test_query}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nHypothetical Document Generated by LLM:\\n{transformed_query.query_str}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef80434",
   "metadata": {},
   "source": [
    "## 8. Execute HyDE-Enhanced Query\n",
    "\n",
    "Now let's run the same query through the HyDE-enhanced pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6cbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute HyDE query\n",
    "hyde_response = hyde_query_engine.query(test_query)\n",
    "\n",
    "print(\"\\nðŸš€ HYDE-ENHANCED APPROACH\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAnswer:\\n{hyde_response.response}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Retrieved Chunks:\")\n",
    "for i, node in enumerate(hyde_response.source_nodes, 1):\n",
    "    print(f\"\\n[Chunk {i}] Score: {node.score:.4f} | Source: {node.metadata.get('file_name', 'unknown')}\")\n",
    "    print(f\"{node.text[:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697d1ab",
   "metadata": {},
   "source": [
    "## 9. Side-by-Side Comparison\n",
    "\n",
    "Let's compare the retrieved chunks and answers from both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66570927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison function\n",
    "def compare_approaches(baseline_resp, hyde_resp):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"ðŸ“Š COMPARATIVE ANALYSIS: BASELINE vs. HYDE\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Compare retrieved sources\n",
    "    print(\"\\n1. RETRIEVED SOURCES COMPARISON\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    baseline_sources = [node.metadata.get('file_name', 'unknown') for node in baseline_resp.source_nodes]\n",
    "    hyde_sources = [node.metadata.get('file_name', 'unknown') for node in hyde_resp.source_nodes]\n",
    "    \n",
    "    print(f\"\\nBaseline sources: {baseline_sources}\")\n",
    "    print(f\"HyDE sources: {hyde_sources}\")\n",
    "    \n",
    "    # Compare relevance scores\n",
    "    print(\"\\n2. RELEVANCE SCORES COMPARISON\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    print(\"\\nBaseline Scores:\")\n",
    "    for i, node in enumerate(baseline_resp.source_nodes, 1):\n",
    "        print(f\"  Chunk {i}: {node.score:.4f}\")\n",
    "    \n",
    "    print(\"\\nHyDE Scores:\")\n",
    "    for i, node in enumerate(hyde_resp.source_nodes, 1):\n",
    "        print(f\"  Chunk {i}: {node.score:.4f}\")\n",
    "    \n",
    "    # Compare answers\n",
    "    print(\"\\n3. GENERATED ANSWERS COMPARISON\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    print(f\"\\nBaseline Answer ({len(baseline_resp.response)} characters):\\n{baseline_resp.response}\")\n",
    "    print(f\"\\n{'-' * 100}\")\n",
    "    print(f\"\\nHyDE Answer ({len(hyde_resp.response)} characters):\\n{hyde_resp.response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Run comparison\n",
    "compare_approaches(baseline_response, hyde_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a226f7ec",
   "metadata": {},
   "source": [
    "## 10. Test with Additional Queries\n",
    "\n",
    "Let's test both approaches with more queries to see consistent patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa456fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional test queries\n",
    "additional_queries = [\n",
    "    \"How do you prevent a model from memorizing training data?\",\n",
    "    \"What technique uses multiple random trees for prediction?\",\n",
    "    \"Which algorithm finds optimal separating boundaries in high-dimensional spaces?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸ”¬ ADDITIONAL QUERY TESTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, query in enumerate(additional_queries, 1):\n",
    "    print(f\"\\n\\n{'=' * 100}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_resp = baseline_query_engine.query(query)\n",
    "    print(f\"\\nðŸ“Š Baseline: {[n.metadata.get('file_name', 'unknown') for n in baseline_resp.source_nodes]}\")\n",
    "    print(f\"Scores: {[f'{n.score:.4f}' for n in baseline_resp.source_nodes]}\")\n",
    "    \n",
    "    # HyDE\n",
    "    hyde_resp = hyde_query_engine.query(query)\n",
    "    print(f\"\\nðŸš€ HyDE: {[n.metadata.get('file_name', 'unknown') for n in hyde_resp.source_nodes]}\")\n",
    "    print(f\"Scores: {[f'{n.score:.4f}' for n in hyde_resp.source_nodes]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b9687",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways and Analysis\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **HyDE improves semantic matching**: By generating a hypothetical answer that resembles actual documents, we bridge the vocabulary gap between questions and answers.\n",
    "\n",
    "2. **Answer-to-answer similarity**: Traditional RAG uses question-to-answer similarity, which can be suboptimal. HyDE uses answer-to-answer similarity, which is more effective.\n",
    "\n",
    "3. **Pre-retrieval optimization**: HyDE is a pre-retrieval technique - it transforms the query before searching, not after.\n",
    "\n",
    "4. **Trade-offs**: HyDE adds one extra LLM call to generate the hypothetical document, increasing latency and cost. The benefit is improved retrieval quality.\n",
    "\n",
    "### When to Use HyDE:\n",
    "- Queries where vocabulary mismatch is a problem\n",
    "- Technical domains with specialized terminology\n",
    "- When retrieval quality is more important than latency\n",
    "- Complex questions that require conceptual understanding\n",
    "\n",
    "### Data Flow Visualization:\n",
    "```\n",
    "Standard RAG:\n",
    "User Query â†’ Embed Query â†’ Vector Search â†’ Retrieved Chunks â†’ LLM Generation â†’ Answer\n",
    "\n",
    "HyDE RAG:\n",
    "User Query â†’ LLM Generate Hypothetical Answer â†’ Embed Hypothetical Answer â†’ \n",
    "Vector Search â†’ Retrieved Chunks â†’ LLM Generation â†’ Answer\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "In the next demo, we'll explore **Multi-Query Decomposition** for handling complex queries that require information from multiple sources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
