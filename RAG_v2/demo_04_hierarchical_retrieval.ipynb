{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82725cfd",
   "metadata": {},
   "source": [
    "# Demo #4: Hierarchical Retrieval - Parent Document Retriever\n",
    "\n",
    "## Objective\n",
    "Demonstrate how hierarchical chunking solves the chunking trade-off by retrieving with small, precise child chunks while generating with larger, context-rich parent chunks.\n",
    "\n",
    "## Core Concepts\n",
    "- **Hierarchical chunking strategy**: Multiple chunk sizes for different purposes\n",
    "- **Parent-Child relationship**: Small chunks for retrieval, large chunks for generation\n",
    "- **Precision vs Context trade-off**: Getting the best of both worlds\n",
    "\n",
    "## The Chunking Problem\n",
    "One of the fundamental challenges in RAG systems is choosing the right chunk size:\n",
    "- **Small chunks (128-256 tokens)**: Precise retrieval but insufficient context for LLM\n",
    "- **Large chunks (1024+ tokens)**: Rich context but imprecise retrieval (too much noise)\n",
    "- **Medium chunks (512 tokens)**: Compromise but doesn't excel at either\n",
    "\n",
    "Hierarchical retrieval solves this by using **small chunks for finding** and **large chunks for generating**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42275c",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LlamaIndex core components\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "# Azure OpenAI\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "# For visualization\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5528d1e",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Azure OpenAI LLM\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure Azure OpenAI Embeddings\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "print(\"‚úì Azure OpenAI configured successfully\")\n",
    "print(f\"  LLM: {azure_llm.model}\")\n",
    "print(f\"  Embedding: {azure_embed.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5f747",
   "metadata": {},
   "source": [
    "## 3. Load Long-Form Documents\n",
    "\n",
    "We'll use comprehensive technical documents that benefit from hierarchical chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202552d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load long-form documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"./data/long_form_docs\",\n",
    "    recursive=True\n",
    ").load_data()\n",
    "\n",
    "print(f\"‚úì Loaded {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"  {i}. {doc.metadata.get('file_name', 'Unknown')} ({len(doc.text)} characters)\")\n",
    "\n",
    "total_chars = sum(len(doc.text) for doc in documents)\n",
    "print(f\"\\nTotal content: {total_chars:,} characters (~{total_chars//4:,} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201d6c0",
   "metadata": {},
   "source": [
    "## 4. Baseline Approach #1: Medium-Sized Chunks (512 tokens)\n",
    "\n",
    "First, let's try the standard compromise - medium-sized chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create medium-sized chunks (standard approach)\n",
    "medium_splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "medium_nodes = medium_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"‚úì Created {len(medium_nodes)} medium-sized chunks (512 tokens)\")\n",
    "print(f\"  Average chunk size: {sum(len(node.text) for node in medium_nodes) // len(medium_nodes)} characters\")\n",
    "\n",
    "# Show example chunk\n",
    "print(\"\\nüìÑ Example medium chunk:\")\n",
    "print(\"=\" * 80)\n",
    "print(medium_nodes[5].text[:300] + \"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index with medium chunks\n",
    "medium_index = VectorStoreIndex(\n",
    "    medium_nodes,\n",
    "    embed_model=azure_embed,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "medium_query_engine = medium_index.as_query_engine(\n",
    "    llm=azure_llm,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "print(\"‚úì Medium-chunk query engine ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d086d",
   "metadata": {},
   "source": [
    "## 5. Baseline Approach #2: Small Chunks (128 tokens)\n",
    "\n",
    "Now let's try small chunks for precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c469d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small chunks for precision\n",
    "small_splitter = SentenceSplitter(\n",
    "    chunk_size=128,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "small_nodes = small_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"‚úì Created {len(small_nodes)} small chunks (128 tokens)\")\n",
    "print(f\"  Average chunk size: {sum(len(node.text) for node in small_nodes) // len(small_nodes)} characters\")\n",
    "\n",
    "# Show example chunk\n",
    "print(\"\\nüìÑ Example small chunk:\")\n",
    "print(\"=\" * 80)\n",
    "print(small_nodes[20].text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd63c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index with small chunks\n",
    "small_index = VectorStoreIndex(\n",
    "    small_nodes,\n",
    "    embed_model=azure_embed,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "small_query_engine = small_index.as_query_engine(\n",
    "    llm=azure_llm,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "print(\"‚úì Small-chunk query engine ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c71f6c",
   "metadata": {},
   "source": [
    "## 6. Advanced Approach: Hierarchical Chunking (Parent-Child)\n",
    "\n",
    "Now let's implement the hierarchical approach:\n",
    "- **Parent chunks**: 1024 tokens (rich context for generation)\n",
    "- **Child chunks**: 256 tokens (precise retrieval)\n",
    "- **Strategy**: Search children, return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a706b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parent chunks (large, context-rich)\n",
    "parent_splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "parent_nodes = parent_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"‚úì Created {len(parent_nodes)} parent chunks (1024 tokens)\")\n",
    "print(f\"  Average parent size: {sum(len(node.text) for node in parent_nodes) // len(parent_nodes)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create child chunks from each parent\n",
    "child_splitter = SentenceSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=25\n",
    ")\n",
    "\n",
    "# Store parent nodes in document store\n",
    "docstore = SimpleDocumentStore()\n",
    "\n",
    "# Create child nodes and link to parents\n",
    "all_child_nodes = []\n",
    "\n",
    "for parent_node in parent_nodes:\n",
    "    # Store parent in docstore\n",
    "    docstore.add_documents([parent_node])\n",
    "    \n",
    "    # Create children from parent text\n",
    "    child_nodes = child_splitter.get_nodes_from_documents(\n",
    "        [parent_node.to_document()]\n",
    "    )\n",
    "    \n",
    "    # Link each child to its parent\n",
    "    for child_node in child_nodes:\n",
    "        child_node.metadata[\"parent_id\"] = parent_node.node_id\n",
    "        child_node.metadata[\"file_name\"] = parent_node.metadata.get(\"file_name\", \"Unknown\")\n",
    "    \n",
    "    all_child_nodes.extend(child_nodes)\n",
    "\n",
    "print(f\"‚úì Created {len(all_child_nodes)} child chunks (256 tokens)\")\n",
    "print(f\"  Average child size: {sum(len(node.text) for node in all_child_nodes) // len(all_child_nodes)} characters\")\n",
    "print(f\"  Ratio: {len(all_child_nodes) / len(parent_nodes):.1f} children per parent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a799cce",
   "metadata": {},
   "source": [
    "### Create Custom Parent Document Retriever\n",
    "\n",
    "This retriever:\n",
    "1. Searches over child embeddings (precise)\n",
    "2. Maps children to parent IDs\n",
    "3. Fetches parent nodes from docstore\n",
    "4. Returns parent nodes to LLM (context-rich)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "\n",
    "class ParentDocumentRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever that searches child chunks but returns parent chunks.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        child_index: VectorStoreIndex,\n",
    "        docstore: SimpleDocumentStore,\n",
    "        similarity_top_k: int = 3,\n",
    "    ):\n",
    "        self._child_retriever = child_index.as_retriever(\n",
    "            similarity_top_k=similarity_top_k\n",
    "        )\n",
    "        self._docstore = docstore\n",
    "        super().__init__()\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve parent nodes based on child node matches.\"\"\"\n",
    "        # Step 1: Retrieve child nodes\n",
    "        child_nodes_with_scores = self._child_retriever.retrieve(query_bundle)\n",
    "        \n",
    "        # Step 2: Get unique parent IDs\n",
    "        parent_ids = set()\n",
    "        parent_scores = {}  # Track best score for each parent\n",
    "        \n",
    "        for node_with_score in child_nodes_with_scores:\n",
    "            parent_id = node_with_score.node.metadata.get(\"parent_id\")\n",
    "            if parent_id:\n",
    "                parent_ids.add(parent_id)\n",
    "                # Keep highest child score for each parent\n",
    "                if parent_id not in parent_scores or node_with_score.score > parent_scores[parent_id]:\n",
    "                    parent_scores[parent_id] = node_with_score.score\n",
    "        \n",
    "        # Step 3: Fetch parent nodes from docstore\n",
    "        parent_nodes_with_scores = []\n",
    "        for parent_id in parent_ids:\n",
    "            parent_node = self._docstore.get_document(parent_id)\n",
    "            if parent_node:\n",
    "                parent_nodes_with_scores.append(\n",
    "                    NodeWithScore(\n",
    "                        node=parent_node,\n",
    "                        score=parent_scores[parent_id]\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # Sort by score\n",
    "        parent_nodes_with_scores.sort(key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        return parent_nodes_with_scores\n",
    "\n",
    "print(\"‚úì Custom ParentDocumentRetriever defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40205b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build child index (only child nodes are embedded and indexed)\n",
    "child_index = VectorStoreIndex(\n",
    "    all_child_nodes,\n",
    "    embed_model=azure_embed,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Child index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b02c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parent document retriever\n",
    "parent_retriever = ParentDocumentRetriever(\n",
    "    child_index=child_index,\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# Build query engine with parent retriever\n",
    "hierarchical_query_engine = RetrieverQueryEngine(\n",
    "    retriever=parent_retriever,\n",
    "    llm=azure_llm\n",
    ")\n",
    "\n",
    "print(\"‚úì Hierarchical query engine ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d59689",
   "metadata": {},
   "source": [
    "## 7. Comparative Evaluation\n",
    "\n",
    "Let's test all three approaches with queries that require both precision and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82350824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries that benefit from hierarchical retrieval\n",
    "test_queries = [\n",
    "    \"What are the advantages and disadvantages of semantic chunking compared to fixed-size chunking?\",\n",
    "    \"Explain the differences between bi-encoders and cross-encoders in retrieval systems.\",\n",
    "    \"How does hierarchical chunking solve the precision-context trade-off?\"\n",
    "]\n",
    "\n",
    "print(\"Test Queries:\")\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e313f4",
   "metadata": {},
   "source": [
    "### Test Query 1: Semantic vs Fixed-Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da787cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = test_queries[0]\n",
    "print(f\"\\nüîç Query: {query}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56974456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medium chunks (512 tokens)\n",
    "print(\"\\nüìä APPROACH 1: Medium Chunks (512 tokens)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "medium_response = medium_query_engine.query(query)\n",
    "\n",
    "print(\"\\nüìÑ Retrieved Chunks:\")\n",
    "for i, node in enumerate(medium_response.source_nodes, 1):\n",
    "    print(f\"\\nChunk {i} (score: {node.score:.4f}):\")\n",
    "    print(f\"Source: {node.node.metadata.get('file_name', 'Unknown')}\")\n",
    "    print(f\"Length: {len(node.node.text)} characters\")\n",
    "    print(f\"Preview: {node.node.text[:200]}...\")\n",
    "\n",
    "print(\"\\nüí° Generated Answer:\")\n",
    "print(\"-\" * 100)\n",
    "print(medium_response.response)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faacb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small chunks (128 tokens)\n",
    "print(\"\\nüìä APPROACH 2: Small Chunks (128 tokens)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "small_response = small_query_engine.query(query)\n",
    "\n",
    "print(\"\\nüìÑ Retrieved Chunks:\")\n",
    "for i, node in enumerate(small_response.source_nodes, 1):\n",
    "    print(f\"\\nChunk {i} (score: {node.score:.4f}):\")\n",
    "    print(f\"Source: {node.node.metadata.get('file_name', 'Unknown')}\")\n",
    "    print(f\"Length: {len(node.node.text)} characters\")\n",
    "    print(f\"Content: {node.node.text}\")\n",
    "\n",
    "print(\"\\nüí° Generated Answer:\")\n",
    "print(\"-\" * 100)\n",
    "print(small_response.response)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical (256 token children -> 1024 token parents)\n",
    "print(\"\\nüìä APPROACH 3: Hierarchical Chunking (Child: 256 ‚Üí Parent: 1024)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "hierarchical_response = hierarchical_query_engine.query(query)\n",
    "\n",
    "print(\"\\nüìÑ Retrieved Parent Chunks:\")\n",
    "for i, node in enumerate(hierarchical_response.source_nodes, 1):\n",
    "    print(f\"\\nParent Chunk {i} (score: {node.score:.4f}):\")\n",
    "    print(f\"Source: {node.node.metadata.get('file_name', 'Unknown')}\")\n",
    "    print(f\"Length: {len(node.node.text)} characters\")\n",
    "    print(f\"Preview: {node.node.text[:300]}...\")\n",
    "\n",
    "print(\"\\nüí° Generated Answer:\")\n",
    "print(\"-\" * 100)\n",
    "print(hierarchical_response.response)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458aec8d",
   "metadata": {},
   "source": [
    "### Analysis: Query 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the approaches\n",
    "comparison_data = [\n",
    "    {\n",
    "        \"Approach\": \"Medium Chunks (512)\",\n",
    "        \"Avg Chunk Size\": f\"{sum(len(n.node.text) for n in medium_response.source_nodes) // len(medium_response.source_nodes)} chars\",\n",
    "        \"Total Context\": f\"{sum(len(n.node.text) for n in medium_response.source_nodes)} chars\",\n",
    "        \"Answer Length\": f\"{len(medium_response.response)} chars\"\n",
    "    },\n",
    "    {\n",
    "        \"Approach\": \"Small Chunks (128)\",\n",
    "        \"Avg Chunk Size\": f\"{sum(len(n.node.text) for n in small_response.source_nodes) // len(small_response.source_nodes)} chars\",\n",
    "        \"Total Context\": f\"{sum(len(n.node.text) for n in small_response.source_nodes)} chars\",\n",
    "        \"Answer Length\": f\"{len(small_response.response)} chars\"\n",
    "    },\n",
    "    {\n",
    "        \"Approach\": \"Hierarchical (256‚Üí1024)\",\n",
    "        \"Avg Chunk Size\": f\"{sum(len(n.node.text) for n in hierarchical_response.source_nodes) // len(hierarchical_response.source_nodes)} chars\",\n",
    "        \"Total Context\": f\"{sum(len(n.node.text) for n in hierarchical_response.source_nodes)} chars\",\n",
    "        \"Answer Length\": f\"{len(hierarchical_response.response)} chars\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìä Comparison Table:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b06872",
   "metadata": {},
   "source": [
    "### Test Query 2: Bi-encoders vs Cross-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = test_queries[1]\n",
    "print(f\"\\nüîç Query: {query}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Test all three approaches\n",
    "medium_response_2 = medium_query_engine.query(query)\n",
    "small_response_2 = small_query_engine.query(query)\n",
    "hierarchical_response_2 = hierarchical_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí° MEDIUM CHUNKS Answer:\")\n",
    "print(\"-\" * 100)\n",
    "print(medium_response_2.response)\n",
    "print(f\"\\nContext provided: {sum(len(n.node.text) for n in medium_response_2.source_nodes)} characters\")\n",
    "\n",
    "print(\"\\n\\nüí° SMALL CHUNKS Answer:\")\n",
    "print(\"-\" * 100)\n",
    "print(small_response_2.response)\n",
    "print(f\"\\nContext provided: {sum(len(n.node.text) for n in small_response_2.source_nodes)} characters\")\n",
    "\n",
    "print(\"\\n\\nüí° HIERARCHICAL Answer:\")\n",
    "print(\"-\" * 100)\n",
    "print(hierarchical_response_2.response)\n",
    "print(f\"\\nContext provided: {sum(len(n.node.text) for n in hierarchical_response_2.source_nodes)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67f2b5",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Hierarchical Retrieval Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efa793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä DATA FLOW: Hierarchical Retrieval Process\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\"\"\\n\n",
    "1. USER QUERY\n",
    "   ‚Üì\n",
    "   'What are the advantages of semantic chunking?'\n",
    "   ‚Üì\n",
    "\n",
    "2. EMBED QUERY (Azure OpenAI)\n",
    "   ‚Üì\n",
    "   Query Embedding: [0.123, -0.456, 0.789, ...] (1536 dimensions)\n",
    "   ‚Üì\n",
    "\n",
    "3. SEARCH CHILD EMBEDDINGS (Precise)\n",
    "   ‚Üì\n",
    "   Vector similarity search over small chunks (256 tokens)\n",
    "   ‚Üì\n",
    "   Top-K Child Chunks Retrieved:\n",
    "   - Child 1 (score: 0.89) ‚Üí Parent ID: abc123\n",
    "   - Child 2 (score: 0.85) ‚Üí Parent ID: abc123\n",
    "   - Child 3 (score: 0.82) ‚Üí Parent ID: def456\n",
    "   ‚Üì\n",
    "\n",
    "4. MAP CHILDREN ‚Üí PARENTS\n",
    "   ‚Üì\n",
    "   Unique Parent IDs: {abc123, def456}\n",
    "   ‚Üì\n",
    "\n",
    "5. FETCH PARENT CHUNKS (Rich Context)\n",
    "   ‚Üì\n",
    "   Document Store Lookup:\n",
    "   - Parent abc123: 1024 tokens (comprehensive explanation)\n",
    "   - Parent def456: 1024 tokens (related concepts)\n",
    "   ‚Üì\n",
    "\n",
    "6. PASS TO LLM (Context-Rich Generation)\n",
    "   ‚Üì\n",
    "   LLM receives large parent chunks (not small children)\n",
    "   ‚Üì\n",
    "\n",
    "7. GENERATE ANSWER\n",
    "   ‚Üì\n",
    "   Comprehensive, well-contextualized response\n",
    "\n",
    "\"\"\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n‚ú® KEY INSIGHT:\")\n",
    "print(\"  - RETRIEVAL uses SMALL chunks (precise targeting)\")\n",
    "print(\"  - GENERATION uses LARGE chunks (rich context)\")\n",
    "print(\"  - Best of both worlds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad445f3f",
   "metadata": {},
   "source": [
    "## 9. Key Findings and Best Practices\n",
    "\n",
    "### The Chunking Trade-off\n",
    "\n",
    "| Approach | Retrieval Precision | Context for LLM | Best Use Case |\n",
    "|----------|-------------------|-----------------|---------------|\n",
    "| **Small Chunks** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê Limited | Factual lookups, specific data extraction |\n",
    "| **Medium Chunks** | ‚≠ê‚≠ê‚≠ê Moderate | ‚≠ê‚≠ê‚≠ê Moderate | General-purpose RAG (compromise) |\n",
    "| **Large Chunks** | ‚≠ê‚≠ê Low | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Rich | When you know what section to retrieve |\n",
    "| **Hierarchical** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Rich | Complex queries requiring comprehension |\n",
    "\n",
    "### When to Use Hierarchical Chunking\n",
    "\n",
    "‚úÖ **Ideal Scenarios:**\n",
    "- Long-form documents (whitepapers, articles, documentation)\n",
    "- Queries requiring deep understanding and context\n",
    "- When answer quality is more important than latency\n",
    "- Complex analytical questions\n",
    "\n",
    "‚ùå **When to Avoid:**\n",
    "- Simple factual queries (\"What is X?\")\n",
    "- Very short documents\n",
    "- When minimizing token usage is critical\n",
    "- Real-time, high-throughput applications (extra complexity)\n",
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "1. **Storage**: Need to store both child embeddings and parent documents\n",
    "2. **Complexity**: More moving parts than simple chunking\n",
    "3. **Cost**: Larger chunks ‚Üí more tokens ‚Üí higher LLM costs\n",
    "4. **Latency**: Fetching parent chunks adds minimal overhead\n",
    "5. **Maintenance**: Need to keep parent-child mappings consistent\n",
    "\n",
    "### Optimization Tips\n",
    "\n",
    "- **Parent size**: 1024-2048 tokens (balance context and cost)\n",
    "- **Child size**: 128-256 tokens (precise but not too fragmented)\n",
    "- **Overlap**: Use overlap in parent chunks to avoid boundary issues\n",
    "- **De-duplication**: If multiple children from same parent retrieved, return parent once\n",
    "- **Metadata**: Store useful metadata in child chunks for filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b32f0e",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "Hierarchical chunking elegantly solves the fundamental chunking dilemma in RAG systems:\n",
    "\n",
    "üéØ **The Problem**: Small chunks provide precise retrieval but lack context for generation. Large chunks provide context but dilute retrieval precision.\n",
    "\n",
    "‚ú® **The Solution**: Use small child chunks for retrieval (precision) and large parent chunks for generation (context).\n",
    "\n",
    "üìà **The Result**: Improved answer quality, especially for complex queries requiring deep understanding.\n",
    "\n",
    "This technique is particularly powerful for:\n",
    "- Technical documentation\n",
    "- Research papers and whitepapers  \n",
    "- Legal and regulatory documents\n",
    "- Educational content\n",
    "- Any long-form content where context matters\n",
    "\n",
    "**Next Steps**: Experiment with different parent/child size ratios for your specific use case!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
