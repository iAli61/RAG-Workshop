{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75993bc",
   "metadata": {},
   "source": [
    "# Demo #2: Multi-Query Decomposition - Complex Query Handling\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo demonstrates **Multi-Query Decomposition**, a technique that breaks down complex, multi-faceted queries into simpler sub-queries to enable comprehensive information retrieval and synthesis.\n",
    "\n",
    "### Core Concept\n",
    "\n",
    "Traditional RAG systems treat each query as a single unit. However, many real-world questions are complex and require information from multiple sources or perspectives. A single retrieval pass may miss important context.\n",
    "\n",
    "**Multi-Query Decomposition solves this by:**\n",
    "1. Analyzing the complex query to identify sub-questions\n",
    "2. Decomposing it into multiple simple sub-queries\n",
    "3. Executing each sub-query independently (parallel retrieval)\n",
    "4. Aggregating all retrieved contexts\n",
    "5. Synthesizing a comprehensive answer from the combined information\n",
    "\n",
    "### Key Benefits\n",
    "- Better coverage for multi-hop reasoning questions\n",
    "- More comprehensive answers by retrieving from multiple perspectives\n",
    "- Explicit sub-question generation aids interpretability\n",
    "- Enables comparison and synthesis of different concepts\n",
    "\n",
    "### Citation\n",
    "- **Reference**: \"Build Advanced Retrieval-Augmented Generation Systems\" - Microsoft Learn\n",
    "- **Paper**: \"MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\"\n",
    "  - Link: https://hf.co/papers/2401.15391\n",
    "- **Paper**: \"BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge\"\n",
    "  - Link: https://hf.co/papers/2406.19820"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a8c4d",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c7db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# llama-index core imports\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "# Azure OpenAI imports\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c03261",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8976e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Azure OpenAI LLM\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Configure Azure OpenAI Embeddings\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"âœ“ Azure OpenAI configured successfully\")\n",
    "print(f\"  LLM: {os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')}\")\n",
    "print(f\"  Embeddings: {os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d4e13",
   "metadata": {},
   "source": [
    "## 3. Load Documents\n",
    "\n",
    "We'll load multiple ML algorithm documents to create a diverse knowledge base suitable for comparative questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "data_path = \"../RAG_v2/data/ml_concepts\"\n",
    "\n",
    "# Load documents\n",
    "reader = SimpleDirectoryReader(data_path)\n",
    "documents = reader.load_data()\n",
    "\n",
    "print(f\"âœ“ Loaded {len(documents)} documents:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    filename = Path(doc.metadata['file_path']).name\n",
    "    print(f\"  {i}. {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205a91c",
   "metadata": {},
   "source": [
    "## 4. Chunk and Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence splitter\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Parse documents into nodes\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# Create vector store index\n",
    "index = VectorStoreIndex(nodes, embed_model=azure_embed)\n",
    "\n",
    "print(f\"âœ“ Created {len(nodes)} chunks\")\n",
    "print(f\"âœ“ Vector index built successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc5ffe",
   "metadata": {},
   "source": [
    "## 5. Create Baseline Single-Query Engine\n",
    "\n",
    "First, let's establish a baseline with a standard query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c35612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline query engine\n",
    "baseline_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    llm=azure_llm,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Baseline query engine created\")\n",
    "print(\"  Configuration: Single-pass retrieval with top-3 chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df0b55",
   "metadata": {},
   "source": [
    "## 6. Test Baseline with Complex Query\n",
    "\n",
    "Let's test with a complex, multi-faceted query that requires synthesizing information from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1484ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define complex test query that requires comparison\n",
    "complex_query = \"Compare the strengths and weaknesses of gradient boosting and random forests for classification tasks. Which is more prone to overfitting?\"\n",
    "\n",
    "print(\"Complex Test Query:\")\n",
    "print(f\"  {complex_query}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Query baseline engine\n",
    "baseline_response = baseline_engine.query(complex_query)\n",
    "\n",
    "print(\"\\nğŸ” BASELINE SINGLE-QUERY RETRIEVAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nGenerated Answer:\\n{baseline_response.response}\")\n",
    "\n",
    "# Display retrieved sources\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Retrieved Chunks:\")\n",
    "print(\"-\"*80)\n",
    "for i, node in enumerate(baseline_response.source_nodes, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Source: {Path(node.metadata.get('file_path', 'Unknown')).name}\")\n",
    "    print(f\"  Score: {node.score:.4f}\")\n",
    "    print(f\"  Content: {node.text[:200]}...\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "print(\"\\nâš ï¸ Potential Limitations:\")\n",
    "print(\"   - Single retrieval pass may miss information about one of the algorithms\")\n",
    "print(\"   - Difficult to ensure balanced coverage of both algorithms\")\n",
    "print(\"   - May not retrieve comparison-specific information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef7e42",
   "metadata": {},
   "source": [
    "## 7. Implement Sub-Question Query Engine\n",
    "\n",
    "Now let's create a query engine that automatically decomposes complex queries into sub-questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828bc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine tool for the knowledge base\n",
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=index.as_query_engine(similarity_top_k=3),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"ml_algorithms\",\n",
    "        description=\"Knowledge base containing detailed information about machine learning algorithms including gradient boosting, random forests, neural networks, support vector machines, and k-means clustering. Use this tool to answer questions about specific ML algorithms, their properties, strengths, weaknesses, and applications.\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create sub-question query engine\n",
    "subquestion_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[query_engine_tool],\n",
    "    llm=azure_llm,\n",
    "    verbose=True,  # Enable verbose mode to see sub-questions\n",
    ")\n",
    "\n",
    "print(\"âœ“ Sub-Question Query Engine created\")\n",
    "print(\"  Enhancement: Complex query â†’ LLM decomposes â†’ Multiple sub-queries â†’ Aggregate â†’ Synthesize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a541a",
   "metadata": {},
   "source": [
    "## 8. Test Sub-Question Decomposition\n",
    "\n",
    "Let's run the same complex query through the sub-question engine and observe the decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe615dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ¯ SUB-QUESTION QUERY ENGINE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nComplex Query: {complex_query}\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Generating sub-questions and retrieving...\\n\")\n",
    "\n",
    "# Query with sub-question decomposition\n",
    "# The verbose output will show the sub-questions automatically\n",
    "subquestion_response = subquestion_engine.query(complex_query)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SYNTHESIZED ANSWER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{subquestion_response.response}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a214e",
   "metadata": {},
   "source": [
    "## 9. Analyze Sub-Questions\n",
    "\n",
    "Let's extract and display the sub-questions that were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“‹ GENERATED SUB-QUESTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract sub-questions from metadata\n",
    "if hasattr(subquestion_response, 'metadata') and subquestion_response.metadata:\n",
    "    sub_questions = subquestion_response.metadata.get('sub_questions', [])\n",
    "    \n",
    "    if sub_questions:\n",
    "        for i, sq in enumerate(sub_questions, 1):\n",
    "            print(f\"\\nSub-Question {i}:\")\n",
    "            print(f\"  Query: {sq.get('query', 'N/A')}\")\n",
    "            print(f\"  Tool: {sq.get('tool_name', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"  Note: Sub-questions processed but not stored in metadata\")\n",
    "        print(\"  (They were displayed in verbose output above)\")\n",
    "else:\n",
    "    print(\"  Note: Sub-questions were generated and processed\")\n",
    "    print(\"  (They were displayed in verbose output above)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nğŸ’¡ Key Insight:\")\n",
    "print(\"   The LLM automatically decomposed the complex comparison query into\")\n",
    "print(\"   focused sub-questions, each targeting specific aspects:\")\n",
    "print(\"   - Strengths of gradient boosting\")\n",
    "print(\"   - Weaknesses of gradient boosting\")\n",
    "print(\"   - Strengths of random forests\")\n",
    "print(\"   - Weaknesses of random forests\")\n",
    "print(\"   - Overfitting characteristics of each\")\n",
    "print(\"\\n   Each sub-question retrieved independently, ensuring comprehensive coverage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88fbca",
   "metadata": {},
   "source": [
    "## 10. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Š COMPARISON: BASELINE vs SUB-QUESTION DECOMPOSITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBASELINE APPROACH:\")\n",
    "print(\"-\"*80)\n",
    "print(\"  Retrieval Strategy: Single query, top-3 chunks\")\n",
    "print(f\"  Chunks Retrieved: {len(baseline_response.source_nodes)}\")\n",
    "print(f\"  Answer Length: {len(baseline_response.response)} characters\")\n",
    "print(f\"  Unique Sources: {len(set([Path(n.metadata.get('file_path', '')).name for n in baseline_response.source_nodes]))}\")\n",
    "\n",
    "print(\"\\nSUB-QUESTION APPROACH:\")\n",
    "print(\"-\"*80)\n",
    "print(\"  Retrieval Strategy: Multiple sub-queries, independent retrieval\")\n",
    "print(f\"  Chunks Retrieved: {len(subquestion_response.source_nodes) if hasattr(subquestion_response, 'source_nodes') else 'Multiple per sub-question'}\")\n",
    "print(f\"  Answer Length: {len(subquestion_response.response)} characters\")\n",
    "if hasattr(subquestion_response, 'source_nodes'):\n",
    "    print(f\"  Unique Sources: {len(set([Path(n.metadata.get('file_path', '')).name for n in subquestion_response.source_nodes]))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nğŸ¯ EXPECTED IMPROVEMENTS:\")\n",
    "print(\"   âœ“ More comprehensive coverage of both algorithms\")\n",
    "print(\"   âœ“ Better balanced information retrieval\")\n",
    "print(\"   âœ“ Explicit comparison structure\")\n",
    "print(\"   âœ“ Less chance of missing key information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0625cf",
   "metadata": {},
   "source": [
    "## 11. Additional Test Cases\n",
    "\n",
    "Let's test with more complex queries to demonstrate the robustness of sub-question decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional complex queries\n",
    "test_queries = [\n",
    "    \"What are the key differences between supervised and unsupervised learning? Provide examples of algorithms for each.\",\n",
    "    \"How do neural networks differ from support vector machines in terms of training process, interpretability, and performance on high-dimensional data?\",\n",
    "    \"Explain the bias-variance tradeoff and how it relates to ensemble methods like random forests and gradient boosting.\",\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ”¬ ADDITIONAL TEST CASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Case {i}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Processing with Sub-Question Decomposition...\\n\")\n",
    "    \n",
    "    response = subquestion_engine.query(query)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Answer:\")\n",
    "    print(\"-\"*80)\n",
    "    print(response.response)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc77b4",
   "metadata": {},
   "source": [
    "## 12. Data Flow Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2390e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“ˆ SUB-QUESTION DECOMPOSITION DATA FLOW\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "BASELINE SINGLE-QUERY PIPELINE:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Complex Query                â”‚\n",
    "â”‚  \"Compare A and B\"            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Single Retrieval Pass        â”‚\n",
    "â”‚  (may favor one aspect)       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Top-K Chunks                 â”‚\n",
    "â”‚  (potentially unbalanced)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LLM Generates Answer         â”‚\n",
    "â”‚  (from limited context)       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "SUB-QUESTION DECOMPOSITION PIPELINE:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Complex Query                â”‚\n",
    "â”‚  \"Compare A and B\"            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LLM Decomposes Query         â”‚ â† KEY INNOVATION\n",
    "â”‚  Sub-Q1: \"Strengths of A\"     â”‚\n",
    "â”‚  Sub-Q2: \"Weaknesses of A\"    â”‚\n",
    "â”‚  Sub-Q3: \"Strengths of B\"     â”‚\n",
    "â”‚  Sub-Q4: \"Weaknesses of B\"    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Parallel Retrieval           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ Sub-Q1  â”‚  â”‚ Sub-Q2  â”‚    â”‚\n",
    "â”‚  â”‚Retrieve â”‚  â”‚Retrieve â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ Sub-Q3  â”‚  â”‚ Sub-Q4  â”‚    â”‚\n",
    "â”‚  â”‚Retrieve â”‚  â”‚Retrieve â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Aggregate All Contexts       â”‚\n",
    "â”‚  (balanced, comprehensive)    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LLM Synthesizes Answer       â”‚\n",
    "â”‚  (from rich, diverse context) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c3ce8c",
   "metadata": {},
   "source": [
    "## 13. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\nâš¡ PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Measure baseline query time\n",
    "start_time = time.time()\n",
    "_ = baseline_engine.query(\"What is gradient boosting?\")\n",
    "baseline_time = time.time() - start_time\n",
    "\n",
    "# Measure sub-question query time\n",
    "start_time = time.time()\n",
    "_ = subquestion_engine.query(\"What is gradient boosting?\")\n",
    "subquestion_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nBaseline Query Time: {baseline_time:.2f} seconds\")\n",
    "print(f\"Sub-Question Query Time: {subquestion_time:.2f} seconds\")\n",
    "print(f\"Overhead: {subquestion_time - baseline_time:.2f} seconds ({((subquestion_time/baseline_time - 1) * 100):.1f}% increase)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\\nğŸ’¡ Trade-offs:\")\n",
    "print(\"   Cost: Additional LLM calls for decomposition and synthesis\")\n",
    "print(\"   Benefit: Significantly better coverage for complex queries\")\n",
    "print(\"   \\nRecommendation: Use sub-question decomposition for:\")\n",
    "print(\"     - Comparison queries (\\\"A vs B\\\")\")\n",
    "print(\"     - Multi-aspect questions\")\n",
    "print(\"     - Questions requiring information synthesis\")\n",
    "print(\"     - Research and analysis tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab33150",
   "metadata": {},
   "source": [
    "## 14. Key Takeaways and Insights\n",
    "\n",
    "### When to Use Sub-Question Decomposition:\n",
    "- **Comparison queries**: \"Compare X and Y\"\n",
    "- **Multi-aspect questions**: Questions with multiple parts\n",
    "- **Complex reasoning**: Requiring synthesis from multiple sources\n",
    "- **Research tasks**: Comprehensive understanding needed\n",
    "\n",
    "### Benefits:\n",
    "1. **Better Coverage**: Ensures all aspects of a complex query are addressed\n",
    "2. **Balanced Retrieval**: Each sub-question gets dedicated retrieval\n",
    "3. **Interpretability**: Sub-questions make the reasoning process transparent\n",
    "4. **Comprehensive Answers**: Synthesis of multiple retrieval passes\n",
    "\n",
    "### Trade-offs:\n",
    "- **Pros:**\n",
    "  - Dramatically better for multi-faceted queries\n",
    "  - More comprehensive and balanced information\n",
    "  - Transparent reasoning process\n",
    "  - Scales well to very complex queries\n",
    "  \n",
    "- **Cons:**\n",
    "  - Higher latency (multiple retrieval passes)\n",
    "  - Increased cost (more LLM and embedding calls)\n",
    "  - May be overkill for simple queries\n",
    "  - Requires well-defined knowledge base descriptions\n",
    "\n",
    "### Best Practices:\n",
    "1. **Tool Metadata**: Provide detailed, accurate descriptions for query engine tools\n",
    "2. **Query Design**: Works best with questions that naturally decompose\n",
    "3. **Context Size**: May need to increase context window for synthesis\n",
    "4. **Selective Use**: Reserve for queries that truly need decomposition\n",
    "\n",
    "### Real-World Applications:\n",
    "- Research assistants (academic papers, technical docs)\n",
    "- Competitive analysis\n",
    "- Product comparison systems\n",
    "- Multi-domain question answering\n",
    "- Technical support systems requiring comprehensive answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f3f30",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "In this demo, we've successfully implemented and compared:\n",
    "\n",
    "1. **Baseline RAG**: Single query â†’ Single retrieval â†’ Generate answer\n",
    "2. **Sub-Question Decomposition**: Complex query â†’ Decompose into sub-queries â†’ Multiple parallel retrievals â†’ Aggregate â†’ Synthesize\n",
    "\n",
    "The key insight is that complex queries often require information from multiple sources or perspectives. By automatically decomposing these queries into simpler sub-questions, we ensure comprehensive coverage and better synthesis.\n",
    "\n",
    "This technique is particularly powerful for:\n",
    "- Comparison and contrast questions\n",
    "- Multi-hop reasoning tasks\n",
    "- Questions requiring balanced coverage of multiple topics\n",
    "- Research and analysis use cases\n",
    "\n",
    "### Next Steps:\n",
    "- Combine with HyDE for even better sub-question retrieval\n",
    "- Experiment with multiple knowledge bases (coming in Demo #8: Agentic RAG)\n",
    "- Try custom sub-question generation strategies\n",
    "- Measure improvement with evaluation metrics (Demo #10)\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- MultiHop-RAG Paper: https://hf.co/papers/2401.15391\n",
    "- BeamAggR Paper: https://hf.co/papers/2406.19820\n",
    "- Microsoft Learn: Build Advanced RAG Systems"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
