{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a0cb34",
   "metadata": {},
   "source": [
    "# Demo #10: RAG Evaluation - Systematic Metrics and Frameworks\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to **systematically evaluate and improve RAG systems** using quantitative metrics. We'll use the RAGAS (RAG Assessment) framework to measure performance across multiple dimensions.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **RAG-Specific Metrics**: Beyond traditional NLP metrics\n",
    "2. **LLM-as-Judge Pattern**: Using LLMs to evaluate RAG quality\n",
    "3. **Multi-Dimensional Evaluation**: Assess retrieval AND generation\n",
    "4. **Iterative Improvement**: Data-driven optimization workflow\n",
    "\n",
    "### The Four Core RAGAS Metrics\n",
    "\n",
    "1. **Context Precision**: Are retrieved documents relevant? (measures retrieval noise)\n",
    "2. **Context Recall**: Are all necessary documents retrieved? (measures retrieval completeness)\n",
    "3. **Faithfulness**: Is the answer grounded in the context? (measures hallucination)\n",
    "4. **Answer Relevance**: Does the answer address the question? (measures response quality)\n",
    "\n",
    "### Why RAG Evaluation Matters\n",
    "\n",
    "```\n",
    "You can't improve what you don't measure.\n",
    "```\n",
    "\n",
    "- **Identify Bottlenecks**: Is retrieval or generation failing?\n",
    "- **Compare Approaches**: Which advanced technique actually helps?\n",
    "- **Monitor Production**: Detect degradation over time\n",
    "- **Justify Costs**: Prove that improvements are worth the investment\n",
    "\n",
    "### Citations\n",
    "\n",
    "- **RAG Evaluation Metrics Guide** - Reference #84\n",
    "- **RAGAS Framework** - Reference #89\n",
    "- **A complete guide to RAG evaluation** - Evidently AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdccf71f",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce12753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# LlamaIndex for RAG\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "# RAGAS for evaluation\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbb751",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Configuration\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Initialize Embedding Model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"‚úì Azure OpenAI configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4e6dc",
   "metadata": {},
   "source": [
    "## 3. Build RAG System\n",
    "\n",
    "Create a standard RAG system that we'll evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ML concepts documents\n",
    "data_path = Path(\"../RAG_v2/data/ml_concepts\")\n",
    "documents = SimpleDirectoryReader(str(data_path)).load_data()\n",
    "\n",
    "print(f\"‚úì Loaded {len(documents)} documents\")\n",
    "\n",
    "# Parse into chunks\n",
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"‚úì Created {len(nodes)} chunks\")\n",
    "\n",
    "# Build vector index\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "# Create query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG system ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219fe66",
   "metadata": {},
   "source": [
    "## 4. Create Evaluation Test Set\n",
    "\n",
    "Define questions with ground truth answers and expected context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions about ML concepts\n",
    "test_questions = [\n",
    "    {\n",
    "        \"question\": \"What is gradient boosting and how does it work?\",\n",
    "        \"ground_truth\": \"Gradient boosting is an ensemble learning technique that builds models sequentially, where each new model corrects the errors of the previous ones. It combines weak learners (typically decision trees) to create a strong predictive model by optimizing a loss function through gradient descent.\",\n",
    "        \"ground_truth_context\": \"gradient_boosting.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain how K-means clustering algorithm works.\",\n",
    "        \"ground_truth\": \"K-means clustering is an unsupervised learning algorithm that partitions data into K clusters by iteratively assigning points to the nearest centroid and updating centroids based on the mean of assigned points. It minimizes within-cluster variance.\",\n",
    "        \"ground_truth_context\": \"kmeans_clustering.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are neural networks and how do they learn?\",\n",
    "        \"ground_truth\": \"Neural networks are computational models inspired by biological neural networks, consisting of layers of interconnected nodes (neurons). They learn through backpropagation, adjusting weights based on the error between predicted and actual outputs to minimize a loss function.\",\n",
    "        \"ground_truth_context\": \"neural_networks.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do random forests improve upon single decision trees?\",\n",
    "        \"ground_truth\": \"Random forests improve upon single decision trees by creating an ensemble of multiple decision trees trained on random subsets of data and features. This reduces overfitting and variance through bootstrap aggregating (bagging) and averaging predictions.\",\n",
    "        \"ground_truth_context\": \"random_forests.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the kernel trick in Support Vector Machines?\",\n",
    "        \"ground_truth\": \"The kernel trick in SVMs allows the algorithm to operate in a high-dimensional feature space without explicitly computing coordinates in that space. It uses kernel functions to implicitly map data to higher dimensions where it becomes linearly separable.\",\n",
    "        \"ground_truth_context\": \"support_vector_machines.md\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Compare gradient boosting with random forests in terms of bias and variance.\",\n",
    "        \"ground_truth\": \"Gradient boosting reduces bias by sequentially correcting errors, making it powerful but prone to overfitting (high variance). Random forests reduce variance through ensemble averaging but may have higher bias. Gradient boosting typically achieves better accuracy but requires more careful tuning.\",\n",
    "        \"ground_truth_context\": \"gradient_boosting.md, random_forests.md\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"‚úì Created test set with {len(test_questions)} questions\")\n",
    "print(\"\\nSample question:\")\n",
    "print(f\"Q: {test_questions[0]['question']}\")\n",
    "print(f\"Ground Truth: {test_questions[0]['ground_truth'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d08b4",
   "metadata": {},
   "source": [
    "## 5. Run RAG System and Collect Data\n",
    "\n",
    "Execute each test question and collect: question, answer, contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3af0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect RAG outputs\n",
    "evaluation_data = []\n",
    "\n",
    "print(\"Running RAG system on test questions...\\n\")\n",
    "\n",
    "for i, test in enumerate(test_questions, 1):\n",
    "    question = test[\"question\"]\n",
    "    print(f\"[{i}/{len(test_questions)}] Processing: {question[:60]}...\")\n",
    "    \n",
    "    # Query RAG system\n",
    "    response = query_engine.query(question)\n",
    "    \n",
    "    # Extract retrieved contexts\n",
    "    contexts = [node.node.text for node in response.source_nodes]\n",
    "    \n",
    "    # Store data\n",
    "    evaluation_data.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": response.response,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": test[\"ground_truth\"],\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úì Collected {len(evaluation_data)} RAG outputs\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nSample RAG Output:\")\n",
    "print(f\"Question: {evaluation_data[0]['question']}\")\n",
    "print(f\"Answer: {evaluation_data[0]['answer'][:200]}...\")\n",
    "print(f\"Retrieved {len(evaluation_data[0]['contexts'])} context chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad30c5",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for RAGAS\n",
    "\n",
    "Convert to Hugging Face Dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"question\": [d[\"question\"] for d in evaluation_data],\n",
    "    \"answer\": [d[\"answer\"] for d in evaluation_data],\n",
    "    \"contexts\": [d[\"contexts\"] for d in evaluation_data],\n",
    "    \"ground_truth\": [d[\"ground_truth\"] for d in evaluation_data],\n",
    "})\n",
    "\n",
    "print(\"‚úì Dataset prepared for RAGAS evaluation\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86fb1f",
   "metadata": {},
   "source": [
    "## 7. Configure RAGAS with Azure OpenAI\n",
    "\n",
    "RAGAS uses LLM-as-judge, so we need to configure it with our Azure OpenAI LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain wrappers for RAGAS\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "# Create LangChain-compatible LLM for RAGAS\n",
    "ragas_llm = AzureChatOpenAI(\n",
    "    openai_api_version=api_version,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_key=api_key,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Create LangChain-compatible embeddings for RAGAS\n",
    "ragas_embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=api_version,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "print(\"‚úì RAGAS configured with Azure OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4918c40",
   "metadata": {},
   "source": [
    "## 8. Run RAGAS Evaluation\n",
    "\n",
    "Calculate all four core metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING RAGAS EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis may take a few minutes as each metric requires LLM evaluation...\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "result = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02bb90",
   "metadata": {},
   "source": [
    "## 9. Display Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = result.to_pandas()\n",
    "\n",
    "# Display aggregate metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAGAS EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAggregate Scores (0.0 - 1.0):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "metrics_summary = {\n",
    "    \"Context Precision\": results_df[\"context_precision\"].mean(),\n",
    "    \"Context Recall\": results_df[\"context_recall\"].mean(),\n",
    "    \"Faithfulness\": results_df[\"faithfulness\"].mean(),\n",
    "    \"Answer Relevancy\": results_df[\"answer_relevancy\"].mean(),\n",
    "}\n",
    "\n",
    "for metric, score in metrics_summary.items():\n",
    "    print(f\"{metric:.<40} {score:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Overall RAG Score (average of all metrics)\n",
    "overall_score = sum(metrics_summary.values()) / len(metrics_summary)\n",
    "print(f\"\\nOverall RAG Score: {overall_score:.3f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05d386",
   "metadata": {},
   "source": [
    "## 10. Analyze Per-Question Results\n",
    "\n",
    "Identify which questions performed poorly and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbb546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per-question scores\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-QUESTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add question summary\n",
    "results_df[\"question_short\"] = results_df[\"question\"].str[:50] + \"...\"\n",
    "\n",
    "# Select relevant columns\n",
    "analysis_df = results_df[[\n",
    "    \"question_short\",\n",
    "    \"context_precision\",\n",
    "    \"context_recall\",\n",
    "    \"faithfulness\",\n",
    "    \"answer_relevancy\",\n",
    "]].copy()\n",
    "\n",
    "# Format scores\n",
    "for col in [\"context_precision\", \"context_recall\", \"faithfulness\", \"answer_relevancy\"]:\n",
    "    analysis_df[col] = analysis_df[col].apply(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "print(\"\\n\" + analysis_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e9a15",
   "metadata": {},
   "source": [
    "## 11. Identify Failure Modes\n",
    "\n",
    "Diagnose bottlenecks based on metric patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c8a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FAILURE MODE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define thresholds\n",
    "threshold = 0.7\n",
    "\n",
    "# Analyze each metric\n",
    "print(\"\\nüìä Diagnostic Insights:\\n\")\n",
    "\n",
    "# Context Precision issues\n",
    "low_precision = results_df[results_df[\"context_precision\"] < threshold]\n",
    "if len(low_precision) > 0:\n",
    "    print(f\"‚ö†Ô∏è  Low Context Precision ({len(low_precision)} questions):\")\n",
    "    print(\"   ‚Üí Too much irrelevant information in retrieval\")\n",
    "    print(\"   ‚Üí Solution: Add re-ranker, improve chunking, or use hybrid search\")\n",
    "    print()\n",
    "\n",
    "# Context Recall issues\n",
    "low_recall = results_df[results_df[\"context_recall\"] < threshold]\n",
    "if len(low_recall) > 0:\n",
    "    print(f\"‚ö†Ô∏è  Low Context Recall ({len(low_recall)} questions):\")\n",
    "    print(\"   ‚Üí Missing relevant documents in retrieval\")\n",
    "    print(\"   ‚Üí Solution: Increase top-k, use query expansion, or improve embeddings\")\n",
    "    print()\n",
    "\n",
    "# Faithfulness issues\n",
    "low_faithfulness = results_df[results_df[\"faithfulness\"] < threshold]\n",
    "if len(low_faithfulness) > 0:\n",
    "    print(f\"‚ö†Ô∏è  Low Faithfulness ({len(low_faithfulness)} questions):\")\n",
    "    print(\"   ‚Üí LLM hallucinating or adding unsupported facts\")\n",
    "    print(\"   ‚Üí Solution: Improve prompting, fine-tune LLM, or use stricter grounding\")\n",
    "    print()\n",
    "\n",
    "# Answer Relevancy issues\n",
    "low_relevancy = results_df[results_df[\"answer_relevancy\"] < threshold]\n",
    "if len(low_relevancy) > 0:\n",
    "    print(f\"‚ö†Ô∏è  Low Answer Relevancy ({len(low_relevancy)} questions):\")\n",
    "    print(\"   ‚Üí Answers not addressing the question properly\")\n",
    "    print(\"   ‚Üí Solution: Improve prompt engineering or query understanding\")\n",
    "    print()\n",
    "\n",
    "if (len(low_precision) + len(low_recall) + len(low_faithfulness) + len(low_relevancy)) == 0:\n",
    "    print(\"‚úÖ All metrics above threshold! RAG system performing well.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e25374",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c32d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create radar chart for aggregate metrics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Radar chart\n",
    "categories = list(metrics_summary.keys())\n",
    "values = list(metrics_summary.values())\n",
    "values += values[:1]  # Close the circle\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "ax1 = plt.subplot(121, projection='polar')\n",
    "ax1.plot(angles, values, 'o-', linewidth=2, color='blue')\n",
    "ax1.fill(angles, values, alpha=0.25, color='blue')\n",
    "ax1.set_xticks(angles[:-1])\n",
    "ax1.set_xticklabels(categories, size=10)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax1.set_title(\"RAG System Performance\\n(Aggregate Metrics)\", size=12, pad=20)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Bar chart for per-question scores\n",
    "ax2 = plt.subplot(122)\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.2\n",
    "\n",
    "ax2.bar(x - 1.5*width, results_df[\"context_precision\"], width, label='Context Precision', alpha=0.8)\n",
    "ax2.bar(x - 0.5*width, results_df[\"context_recall\"], width, label='Context Recall', alpha=0.8)\n",
    "ax2.bar(x + 0.5*width, results_df[\"faithfulness\"], width, label='Faithfulness', alpha=0.8)\n",
    "ax2.bar(x + 1.5*width, results_df[\"answer_relevancy\"], width, label='Answer Relevancy', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Question Number')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Per-Question Metric Breakdown')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f\"Q{i+1}\" for i in range(len(results_df))])\n",
    "ax2.legend(loc='lower left', fontsize=8)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.axhline(y=0.7, color='r', linestyle='--', alpha=0.5, label='Threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rag_evaluation_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved as: rag_evaluation_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2cd29",
   "metadata": {},
   "source": [
    "## 13. Simulate Iterative Improvement\n",
    "\n",
    "Show how to apply improvements based on evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa401cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ITERATIVE IMPROVEMENT WORKFLOW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìã Recommended Improvements Based on Evaluation:\\n\")\n",
    "\n",
    "# Determine primary bottleneck\n",
    "avg_precision = results_df[\"context_precision\"].mean()\n",
    "avg_recall = results_df[\"context_recall\"].mean()\n",
    "avg_faithfulness = results_df[\"faithfulness\"].mean()\n",
    "avg_relevancy = results_df[\"answer_relevancy\"].mean()\n",
    "\n",
    "bottlenecks = [\n",
    "    (\"Context Precision\", avg_precision),\n",
    "    (\"Context Recall\", avg_recall),\n",
    "    (\"Faithfulness\", avg_faithfulness),\n",
    "    (\"Answer Relevancy\", avg_relevancy),\n",
    "]\n",
    "\n",
    "# Sort by score (lowest first)\n",
    "bottlenecks.sort(key=lambda x: x[1])\n",
    "\n",
    "print(f\"1Ô∏è‚É£  Primary Bottleneck: {bottlenecks[0][0]} (Score: {bottlenecks[0][1]:.3f})\")\n",
    "\n",
    "# Provide specific recommendations\n",
    "if bottlenecks[0][0] == \"Context Precision\":\n",
    "    print(\"\\n   üìå Recommended Actions:\")\n",
    "    print(\"      ‚Ä¢ Implement re-ranking (Demo #5) to filter irrelevant results\")\n",
    "    print(\"      ‚Ä¢ Use context compression (Demo #6) to remove noise\")\n",
    "    print(\"      ‚Ä¢ Improve chunking strategy (Demo #4) for better granularity\")\n",
    "    print(\"      ‚Ä¢ Add hybrid search (Demo #3) for better exact matching\")\n",
    "\n",
    "elif bottlenecks[0][0] == \"Context Recall\":\n",
    "    print(\"\\n   üìå Recommended Actions:\")\n",
    "    print(\"      ‚Ä¢ Increase similarity_top_k from 3 to 5 or higher\")\n",
    "    print(\"      ‚Ä¢ Use query expansion (Demo #2) to find more relevant docs\")\n",
    "    print(\"      ‚Ä¢ Apply HyDE (Demo #1) for better semantic matching\")\n",
    "    print(\"      ‚Ä¢ Fine-tune embeddings (Demo #9) for domain adaptation\")\n",
    "\n",
    "elif bottlenecks[0][0] == \"Faithfulness\":\n",
    "    print(\"\\n   üìå Recommended Actions:\")\n",
    "    print(\"      ‚Ä¢ Improve system prompt to emphasize grounding\")\n",
    "    print(\"      ‚Ä¢ Use lower temperature (more deterministic)\")\n",
    "    print(\"      ‚Ä¢ Implement CRAG (Demo #7) for self-correction\")\n",
    "    print(\"      ‚Ä¢ Consider fine-tuning the generator LLM\")\n",
    "\n",
    "elif bottlenecks[0][0] == \"Answer Relevancy\":\n",
    "    print(\"\\n   üìå Recommended Actions:\")\n",
    "    print(\"      ‚Ä¢ Refine prompt engineering for better instruction following\")\n",
    "    print(\"      ‚Ä¢ Use multi-query decomposition (Demo #2) for complex questions\")\n",
    "    print(\"      ‚Ä¢ Implement agentic RAG (Demo #8) for adaptive reasoning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   1. Apply recommended improvement\")\n",
    "print(\"   2. Re-run evaluation on same test set\")\n",
    "print(\"   3. Compare before/after metrics\")\n",
    "print(\"   4. Iterate until target performance achieved\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239eb6f",
   "metadata": {},
   "source": [
    "## 14. Export Results for Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to CSV\n",
    "results_df.to_csv(\"rag_evaluation_detailed.csv\", index=False)\n",
    "print(\"‚úì Detailed results saved to: rag_evaluation_detailed.csv\")\n",
    "\n",
    "# Save summary report\n",
    "summary_report = f\"\"\"\n",
    "RAG SYSTEM EVALUATION REPORT\n",
    "============================\n",
    "\n",
    "Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Test Questions: {len(test_questions)}\n",
    "\n",
    "AGGREGATE METRICS\n",
    "-----------------\n",
    "Context Precision:  {avg_precision:.3f}\n",
    "Context Recall:     {avg_recall:.3f}\n",
    "Faithfulness:       {avg_faithfulness:.3f}\n",
    "Answer Relevancy:   {avg_relevancy:.3f}\n",
    "\n",
    "Overall Score:      {overall_score:.3f}\n",
    "\n",
    "PRIMARY BOTTLENECK\n",
    "------------------\n",
    "{bottlenecks[0][0]}: {bottlenecks[0][1]:.3f}\n",
    "\n",
    "QUESTIONS BELOW THRESHOLD (0.7)\n",
    "--------------------------------\n",
    "Low Context Precision:  {len(low_precision)}\n",
    "Low Context Recall:     {len(low_recall)}\n",
    "Low Faithfulness:       {len(low_faithfulness)}\n",
    "Low Answer Relevancy:   {len(low_relevancy)}\n",
    "\n",
    "See rag_evaluation_detailed.csv for per-question analysis.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"rag_evaluation_summary.txt\", \"w\") as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"‚úì Summary report saved to: rag_evaluation_summary.txt\")\n",
    "print(\"\\n\" + summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbdb740",
   "metadata": {},
   "source": [
    "## 15. Key Takeaways\n",
    "\n",
    "### The Four RAGAS Metrics Explained\n",
    "\n",
    "1. **Context Precision** (0-1): \n",
    "   - Measures: What % of retrieved documents are actually relevant?\n",
    "   - Low score means: Too much noise in retrieval\n",
    "   - Fix with: Re-ranking, better chunking, hybrid search\n",
    "\n",
    "2. **Context Recall** (0-1):\n",
    "   - Measures: What % of relevant documents were retrieved?\n",
    "   - Low score means: Missing critical information\n",
    "   - Fix with: Higher top-k, query expansion, better embeddings\n",
    "\n",
    "3. **Faithfulness** (0-1):\n",
    "   - Measures: Is the answer supported by retrieved context?\n",
    "   - Low score means: LLM is hallucinating\n",
    "   - Fix with: Better prompting, stricter grounding, fine-tuning\n",
    "\n",
    "4. **Answer Relevancy** (0-1):\n",
    "   - Measures: Does the answer actually address the question?\n",
    "   - Low score means: Answers are off-topic\n",
    "   - Fix with: Prompt engineering, query understanding\n",
    "\n",
    "### LLM-as-Judge Pattern\n",
    "\n",
    "RAGAS uses LLMs to evaluate RAG outputs because:\n",
    "- Traditional metrics (BLEU, ROUGE) don't capture semantic quality\n",
    "- Human evaluation is expensive and slow\n",
    "- LLMs can assess relevance, faithfulness, and coherence\n",
    "\n",
    "**Trade-off**: LLM evaluation adds cost and latency but provides nuanced insights.\n",
    "\n",
    "### Evaluation-Driven Development Workflow\n",
    "\n",
    "```\n",
    "1. Build baseline RAG system\n",
    "2. Create test set with ground truth\n",
    "3. Run evaluation (RAGAS)\n",
    "4. Identify bottleneck (lowest metric)\n",
    "5. Apply targeted improvement\n",
    "6. Re-evaluate and compare\n",
    "7. Repeat until target performance\n",
    "```\n",
    "\n",
    "### When to Evaluate\n",
    "\n",
    "‚úÖ **Critical Moments**:\n",
    "- Before deploying to production\n",
    "- After major architecture changes\n",
    "- When adding new data sources\n",
    "- Regular monitoring (weekly/monthly)\n",
    "- After user complaints spike\n",
    "\n",
    "### Production Evaluation Strategy\n",
    "\n",
    "1. **Offline Evaluation** (what we did here):\n",
    "   - Fixed test set with ground truth\n",
    "   - Comprehensive metrics\n",
    "   - Compare different approaches\n",
    "\n",
    "2. **Online Evaluation** (production):\n",
    "   - User feedback (thumbs up/down)\n",
    "   - Click-through rates\n",
    "   - Session abandonment\n",
    "   - Response time monitoring\n",
    "\n",
    "3. **Hybrid Approach**:\n",
    "   - Automated offline evaluation weekly\n",
    "   - Sample online queries for manual review\n",
    "   - A/B testing for major changes\n",
    "\n",
    "### Alternative Evaluation Frameworks\n",
    "\n",
    "- **TruLens**: Real-time evaluation with feedback functions\n",
    "- **DeepEval**: Unit testing for LLM applications\n",
    "- **LangSmith**: LangChain's evaluation and observability\n",
    "- **Phoenix**: Arize's LLM observability platform\n",
    "- **Manual Evaluation**: Gold standard but expensive\n",
    "\n",
    "### Cost Considerations\n",
    "\n",
    "RAGAS evaluation costs:\n",
    "- **Per Question**: ~3-5 LLM calls (one per metric)\n",
    "- **For 100 questions**: ~400-500 API calls\n",
    "- **Estimated Cost**: $1-5 for GPT-4 (varies by model)\n",
    "\n",
    "**Optimization**:\n",
    "- Use smaller test set for rapid iteration (10-20 questions)\n",
    "- Full evaluation before major releases\n",
    "- Cache evaluation results\n",
    "- Use cheaper models for context metrics (GPT-3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca9279",
   "metadata": {},
   "source": [
    "## 16. Metric Formulas and Calculation Details\n",
    "\n",
    "### Context Precision\n",
    "\n",
    "$$\n",
    "\\text{Context Precision} = \\frac{\\sum_{k=1}^{K} (\\text{Precision@k} \\times v_k)}{\\text{Total Relevant in Top-K}}\n",
    "$$\n",
    "\n",
    "Where $v_k$ is 1 if the chunk at rank $k$ is relevant, 0 otherwise.\n",
    "\n",
    "### Context Recall\n",
    "\n",
    "$$\n",
    "\\text{Context Recall} = \\frac{|\\text{GT sentences in retrieved context}|}{|\\text{Total GT sentences}|}\n",
    "$$\n",
    "\n",
    "Measures how many ground truth sentences appear in retrieved chunks.\n",
    "\n",
    "### Faithfulness\n",
    "\n",
    "$$\n",
    "\\text{Faithfulness} = \\frac{|\\text{Supported claims}|}{|\\text{Total claims}|}\n",
    "$$\n",
    "\n",
    "LLM extracts claims from answer and verifies each against context.\n",
    "\n",
    "### Answer Relevancy\n",
    "\n",
    "$$\n",
    "\\text{Answer Relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{sim}(q, q'_i)\n",
    "$$\n",
    "\n",
    "Where $q'_i$ are generated questions from the answer, and $q$ is original question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141a49f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **RAG Evaluation Metrics Guide** - Future AGI (Reference #84)\n",
    "   - Link: https://futureagi.com/blogs/rag-evaluation-metrics-2025\n",
    "\n",
    "2. **A complete guide to RAG evaluation** - Evidently AI (Reference #85)\n",
    "   - Link: https://www.evidentlyai.com/llm-guide/rag-evaluation\n",
    "\n",
    "3. **RAG Evaluation Metrics: Answer Relevancy, Faithfulness** - Confident AI (Reference #86)\n",
    "   - Link: https://www.confident-ai.com/blog/rag-evaluation-metrics\n",
    "\n",
    "4. **Best 9 RAG Evaluation Tools of 2025** - Deepchecks (Reference #89)\n",
    "   - Link: https://www.deepchecks.com/best-rag-evaluation-tools/\n",
    "\n",
    "5. **RAGAS Framework**:\n",
    "   - GitHub: https://github.com/explodinggradients/ragas\n",
    "   - Documentation: https://docs.ragas.io/\n",
    "\n",
    "6. **LangChain Evaluation**:\n",
    "   - Link: https://python.langchain.com/docs/guides/evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
