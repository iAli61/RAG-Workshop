{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ce539b",
   "metadata": {},
   "source": [
    "# Demo #4: Hierarchical Retrieval - Parent Document Retriever\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **Hierarchical Retrieval** using the Parent-Child chunking pattern to solve the fundamental chunking trade-off:\n",
    "- **Small chunks**: Precise retrieval but insufficient context for generation\n",
    "- **Large chunks**: Rich context but imprecise retrieval\n",
    "\n",
    "### The Solution: Parent-Child Architecture\n",
    "\n",
    "1. **Child Chunks (Small)**: Used for precise embedding-based retrieval\n",
    "2. **Parent Chunks (Large)**: Returned to LLM for context-rich generation\n",
    "\n",
    "### Key Concepts Demonstrated\n",
    "- Hierarchical chunking strategy\n",
    "- Parent-Child relationship in document storage\n",
    "- Precision in retrieval, richness in generation\n",
    "- Trade-off optimization\n",
    "\n",
    "### Data Flow\n",
    "```\n",
    "Query ‚Üí Embed ‚Üí Search child embeddings (precise) ‚Üí Identify top-K children ‚Üí \n",
    "Lookup parent IDs ‚Üí Fetch parent chunks (rich context) ‚Üí LLM generation\n",
    "```\n",
    "\n",
    "### References\n",
    "- **Parent Document Retriever Pattern**: LangChain Documentation\n",
    "- **Research**: \"Rethinking Chunk Size For Long-Document Retrieval\" (arXiv:2505.21700)\n",
    "- **Research**: \"Late Chunking: Contextual Chunk Embeddings\" (arXiv:2409.04701)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ee0f2",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode, NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úì Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0cfa2d",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98838f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI LLM\n",
    "azure_llm = AzureOpenAI(\n",
    "    engine=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    model=\"gpt-4\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI Embedding\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"‚úì Azure OpenAI configured\")\n",
    "print(f\"  LLM: {os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')}\")\n",
    "print(f\"  Embeddings: {os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f606289",
   "metadata": {},
   "source": [
    "## 3. Load Long-Form Documents\n",
    "\n",
    "We'll use 3-4 long-form technical documents to demonstrate the effectiveness of hierarchical retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load long-form documents\n",
    "data_path = \"../RAG_v2/data/long_form_docs/\"\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=data_path)\n",
    "documents = reader.load_data()\n",
    "\n",
    "print(f\"‚úì Loaded {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"  {i+1}. {doc.metadata.get('file_name', 'Unknown')} ({len(doc.text)} characters)\")\n",
    "\n",
    "total_chars = sum(len(doc.text) for doc in documents)\n",
    "print(f\"\\nTotal content: {total_chars:,} characters (~{total_chars//4:,} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed397413",
   "metadata": {},
   "source": [
    "## 4. Baseline Approach #1: Medium-Sized Chunks\n",
    "\n",
    "First, let's establish a baseline with standard medium-sized chunks (512 tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f332125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create medium-sized chunks\n",
    "medium_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "medium_nodes = medium_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"‚úì Created {len(medium_nodes)} medium-sized chunks (512 tokens)\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"Length: {len(medium_nodes[0].text)} characters\")\n",
    "print(f\"Text preview: {medium_nodes[0].text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bafbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index with medium chunks\n",
    "medium_index = VectorStoreIndex(medium_nodes, embed_model=azure_embed)\n",
    "medium_query_engine = medium_index.as_query_engine(\n",
    "    llm=azure_llm,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "print(\"‚úì Medium-chunk baseline query engine ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52635be2",
   "metadata": {},
   "source": [
    "## 5. Baseline Approach #2: Small Chunks\n",
    "\n",
    "Next, let's try small chunks (128 tokens) for precise retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65674d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small chunks\n",
    "small_splitter = SentenceSplitter(chunk_size=128, chunk_overlap=10)\n",
    "small_nodes = small_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"‚úì Created {len(small_nodes)} small chunks (128 tokens)\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"Length: {len(small_nodes[0].text)} characters\")\n",
    "print(f\"Text preview: {small_nodes[0].text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ff5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index with small chunks\n",
    "small_index = VectorStoreIndex(small_nodes, embed_model=azure_embed)\n",
    "small_query_engine = small_index.as_query_engine(\n",
    "    llm=azure_llm,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "print(\"‚úì Small-chunk baseline query engine ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63f534",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Approach: Parent-Child Chunking\n",
    "\n",
    "Now implement the parent-child strategy:\n",
    "- **Parent chunks**: 1024 tokens (rich context)\n",
    "- **Child chunks**: 256 tokens (precise retrieval)\n",
    "\n",
    "### Architecture\n",
    "1. Split documents into parent chunks\n",
    "2. For each parent, create multiple child chunks\n",
    "3. Link each child to its parent via `parent_id`\n",
    "4. Index only children for embedding search\n",
    "5. Store parents in a document store\n",
    "6. Custom retriever: search children, return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad620f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parent and child splitters\n",
    "parent_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "child_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=25)\n",
    "\n",
    "print(\"‚úì Splitters configured:\")\n",
    "print(\"  Parent chunks: 1024 tokens, overlap 100\")\n",
    "print(\"  Child chunks: 256 tokens, overlap 25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parent nodes\n",
    "parent_nodes = parent_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"‚úì Created {len(parent_nodes)} parent nodes\")\n",
    "print(f\"\\nSample parent node:\")\n",
    "print(f\"Length: {len(parent_nodes[0].text)} characters\")\n",
    "print(f\"Text preview: {parent_nodes[0].text[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b239bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create child nodes linked to parents\n",
    "child_nodes = []\n",
    "\n",
    "for parent_node in parent_nodes:\n",
    "    # Get child chunks from parent text\n",
    "    parent_doc = parent_node.as_related_node_info()\n",
    "    \n",
    "    # Create temporary document for splitting\n",
    "    from llama_index.core.schema import Document\n",
    "    temp_doc = Document(text=parent_node.text, metadata=parent_node.metadata)\n",
    "    \n",
    "    # Split parent into children\n",
    "    children = child_splitter.get_nodes_from_documents([temp_doc])\n",
    "    \n",
    "    # Link each child to parent\n",
    "    for child in children:\n",
    "        child.relationships[\"parent\"] = parent_doc\n",
    "        child_nodes.append(child)\n",
    "\n",
    "print(f\"‚úì Created {len(child_nodes)} child nodes linked to parents\")\n",
    "print(f\"  Average: {len(child_nodes)/len(parent_nodes):.1f} children per parent\")\n",
    "print(f\"\\nSample child node:\")\n",
    "print(f\"Length: {len(child_nodes[0].text)} characters\")\n",
    "print(f\"Has parent link: {'parent' in child_nodes[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc08106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document store for parents\n",
    "docstore = SimpleDocumentStore()\n",
    "\n",
    "# Add parent nodes to document store\n",
    "for parent in parent_nodes:\n",
    "    docstore.add_documents([parent])\n",
    "\n",
    "print(f\"‚úì Parent document store created with {len(parent_nodes)} parents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e520967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index only child nodes for retrieval\n",
    "child_index = VectorStoreIndex(child_nodes, embed_model=azure_embed)\n",
    "\n",
    "print(\"‚úì Vector index created with child nodes for precise retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b00073",
   "metadata": {},
   "source": [
    "### 6.1 Implement Parent Document Retriever\n",
    "\n",
    "Custom retriever that:\n",
    "1. Retrieves top-k child nodes based on similarity\n",
    "2. Extracts parent IDs from children\n",
    "3. Fetches parent nodes from document store\n",
    "4. Returns parent nodes (with rich context) to query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aedec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParentDocumentRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that searches children but returns parents.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        child_index: VectorStoreIndex,\n",
    "        docstore: SimpleDocumentStore,\n",
    "        similarity_top_k: int = 3,\n",
    "    ):\n",
    "        self._child_retriever = child_index.as_retriever(\n",
    "            similarity_top_k=similarity_top_k\n",
    "        )\n",
    "        self._docstore = docstore\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "    \n",
    "    def _retrieve(self, query_bundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve parent nodes based on child node matches.\"\"\"\n",
    "        \n",
    "        # Step 1: Retrieve child nodes\n",
    "        child_nodes = self._child_retriever.retrieve(query_bundle)\n",
    "        \n",
    "        # Step 2: Extract parent IDs (use set to avoid duplicates)\n",
    "        parent_ids = set()\n",
    "        parent_scores = {}  # Track best score for each parent\n",
    "        \n",
    "        for child_node in child_nodes:\n",
    "            if \"parent\" in child_node.node.relationships:\n",
    "                parent_id = child_node.node.relationships[\"parent\"].node_id\n",
    "                parent_ids.add(parent_id)\n",
    "                \n",
    "                # Keep highest score for each parent\n",
    "                if parent_id not in parent_scores or child_node.score > parent_scores[parent_id]:\n",
    "                    parent_scores[parent_id] = child_node.score\n",
    "        \n",
    "        # Step 3: Fetch parent nodes from document store\n",
    "        parent_nodes = []\n",
    "        for parent_id in parent_ids:\n",
    "            parent_node = self._docstore.get_document(parent_id)\n",
    "            if parent_node:\n",
    "                # Create NodeWithScore using the best child score\n",
    "                parent_nodes.append(\n",
    "                    NodeWithScore(node=parent_node, score=parent_scores[parent_id])\n",
    "                )\n",
    "        \n",
    "        # Step 4: Sort by score and return top-k\n",
    "        parent_nodes.sort(key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        return parent_nodes[:self._similarity_top_k]\n",
    "\n",
    "# Create parent document retriever\n",
    "parent_retriever = ParentDocumentRetriever(\n",
    "    child_index=child_index,\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "print(\"‚úì Parent Document Retriever created\")\n",
    "print(\"  Retrieval: Uses child node embeddings (256 tokens, precise)\")\n",
    "print(\"  Context: Returns parent nodes (1024 tokens, rich)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f3004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine with parent retriever\n",
    "parent_query_engine = RetrieverQueryEngine(\n",
    "    retriever=parent_retriever,\n",
    "    llm=azure_llm\n",
    ")\n",
    "\n",
    "print(\"‚úì Hierarchical query engine ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4545f",
   "metadata": {},
   "source": [
    "## 7. Comparative Evaluation\n",
    "\n",
    "Test all three approaches with queries requiring both precision and context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fb417",
   "metadata": {},
   "source": [
    "### Test Query 1: Requires Precision + Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_1 = \"What are the key considerations when choosing chunk size for RAG systems?\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TEST QUERY: {test_query_1}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef34189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Medium chunks (512 tokens)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPROACH 1: MEDIUM CHUNKS (512 tokens)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response_medium = medium_query_engine.query(test_query_1)\n",
    "\n",
    "print(\"\\nüìÑ Retrieved Chunks:\")\n",
    "for i, node in enumerate(response_medium.source_nodes):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Score: {node.score:.4f}\")\n",
    "    print(f\"  Length: {len(node.text)} characters\")\n",
    "    print(f\"  Preview: {node.text[:200]}...\")\n",
    "\n",
    "print(\"\\nüí° Generated Answer:\")\n",
    "print(response_medium.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da18ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Small chunks (128 tokens)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPROACH 2: SMALL CHUNKS (128 tokens)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response_small = small_query_engine.query(test_query_1)\n",
    "\n",
    "print(\"\\nüìÑ Retrieved Chunks:\")\n",
    "for i, node in enumerate(response_small.source_nodes):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Score: {node.score:.4f}\")\n",
    "    print(f\"  Length: {len(node.text)} characters\")\n",
    "    print(f\"  Preview: {node.text}\")\n",
    "\n",
    "print(\"\\nüí° Generated Answer:\")\n",
    "print(response_small.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c738928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3: Parent-Child Hierarchical\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPROACH 3: HIERARCHICAL (Child retrieval: 256, Parent context: 1024)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response_parent = parent_query_engine.query(test_query_1)\n",
    "\n",
    "print(\"\\nüìÑ Retrieved Parent Chunks (via child matching):\")\n",
    "for i, node in enumerate(response_parent.source_nodes):\n",
    "    print(f\"\\nParent Chunk {i+1}:\")\n",
    "    print(f\"  Score: {node.score:.4f} (from best matching child)\")\n",
    "    print(f\"  Length: {len(node.text)} characters\")\n",
    "    print(f\"  Preview: {node.text[:300]}...\")\n",
    "\n",
    "print(\"\\nüí° Generated Answer:\")\n",
    "print(response_parent.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed4afa",
   "metadata": {},
   "source": [
    "### Test Query 2: Technical Detail Requiring Broad Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01034c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_2 = \"Explain the relationship between embedding quality and retrieval performance in RAG systems.\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TEST QUERY: {test_query_2}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b94735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three approaches\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Medium vs Small vs Hierarchical\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Medium chunks\n",
    "print(\"\\nüîµ Medium Chunks (512 tokens):\")\n",
    "response_m2 = medium_query_engine.query(test_query_2)\n",
    "print(f\"Context length: {sum(len(n.text) for n in response_m2.source_nodes)} chars\")\n",
    "print(f\"Answer preview: {response_m2.response[:250]}...\\n\")\n",
    "\n",
    "# Small chunks\n",
    "print(\"\\nüü° Small Chunks (128 tokens):\")\n",
    "response_s2 = small_query_engine.query(test_query_2)\n",
    "print(f\"Context length: {sum(len(n.text) for n in response_s2.source_nodes)} chars\")\n",
    "print(f\"Answer preview: {response_s2.response[:250]}...\\n\")\n",
    "\n",
    "# Hierarchical\n",
    "print(\"\\nüü¢ Hierarchical (Child: 256, Parent: 1024):\")\n",
    "response_p2 = parent_query_engine.query(test_query_2)\n",
    "print(f\"Context length: {sum(len(n.text) for n in response_p2.source_nodes)} chars\")\n",
    "print(f\"Answer preview: {response_p2.response[:250]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1341b",
   "metadata": {},
   "source": [
    "## 8. Analysis: The Chunking Trade-off\n",
    "\n",
    "Let's analyze the results across different chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Collect metrics\n",
    "comparison_data = {\n",
    "    'Approach': ['Small Chunks', 'Medium Chunks', 'Hierarchical (Parent-Child)'],\n",
    "    'Chunk Size': ['128 tokens', '512 tokens', 'Child: 256, Parent: 1024'],\n",
    "    'Retrieval Precision': ['‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent', '‚≠ê‚≠ê‚≠ê Good', '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent (via children)'],\n",
    "    'Context Richness': ['‚≠ê‚≠ê Limited', '‚≠ê‚≠ê‚≠ê‚≠ê Good', '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent (via parents)'],\n",
    "    'Answer Quality': ['‚≠ê‚≠ê‚≠ê Fair', '‚≠ê‚≠ê‚≠ê‚≠ê Good', '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent'],\n",
    "    'Trade-off': ['Precise but lacks context', 'Balanced but suboptimal', 'Best of both worlds'],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHUNKING STRATEGY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a719da",
   "metadata": {},
   "source": [
    "## 9. Visualization: How Hierarchical Retrieval Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40101474",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HIERARCHICAL RETRIEVAL: ARCHITECTURE & DATA FLOW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         DOCUMENT CORPUS                              ‚îÇ\n",
    "‚îÇ                  (Long-form technical documents)                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚îÇ\n",
    "                           ‚ñº\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ   PARENT SPLITTER      ‚îÇ\n",
    "              ‚îÇ   (1024 tokens,        ‚îÇ\n",
    "              ‚îÇ    overlap 100)        ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                       ‚îÇ\n",
    "       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "       ‚ñº                               ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ PARENT NODE  ‚îÇ              ‚îÇ PARENT NODE  ‚îÇ\n",
    "‚îÇ  (Rich       ‚îÇ              ‚îÇ  (Rich       ‚îÇ\n",
    "‚îÇ   Context)   ‚îÇ              ‚îÇ   Context)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ                             ‚îÇ\n",
    "       ‚îÇ Child Splitter              ‚îÇ Child Splitter\n",
    "       ‚îÇ (256 tokens)                ‚îÇ (256 tokens)\n",
    "       ‚ñº                             ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Child 1    ‚îÇ              ‚îÇ  Child 3    ‚îÇ\n",
    "‚îÇ  (Precise)  ‚îÇ              ‚îÇ  (Precise)  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Child 2    ‚îÇ              ‚îÇ  Child 4    ‚îÇ\n",
    "‚îÇ  (Precise)  ‚îÇ              ‚îÇ  (Precise)  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ                             ‚îÇ\n",
    "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  VECTOR INDEX    ‚îÇ\n",
    "        ‚îÇ  (Child Nodes    ‚îÇ\n",
    "        ‚îÇ   Only)          ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ   USER QUERY            ‚îÇ\n",
    "    ‚îÇ   \"Chunk size in RAG?\"  ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚îÇ\n",
    "                 ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  SIMILARITY SEARCH     ‚îÇ\n",
    "    ‚îÇ  (on child embeddings) ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚îÇ\n",
    "                 ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  TOP-K CHILDREN        ‚îÇ\n",
    "    ‚îÇ  (Precise matches)     ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚îÇ\n",
    "                 ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  EXTRACT PARENT IDs    ‚îÇ\n",
    "    ‚îÇ  (from relationships)  ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚îÇ\n",
    "                 ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  FETCH PARENTS         ‚îÇ\n",
    "    ‚îÇ  (from docstore)       ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚îÇ\n",
    "                 ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  RETURN TO LLM         ‚îÇ\n",
    "    ‚îÇ  (Rich parent context) ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚îÇ\n",
    "                 ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  GENERATED ANSWER      ‚îÇ\n",
    "    ‚îÇ  (Accurate + Complete) ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "KEY INSIGHT:\n",
    "  ‚Ä¢ Child nodes provide PRECISION in retrieval (small, focused embeddings)\n",
    "  ‚Ä¢ Parent nodes provide CONTEXT for generation (large, comprehensive text)\n",
    "  ‚Ä¢ This solves the \"chunking trade-off\" problem\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d8860",
   "metadata": {},
   "source": [
    "## 10. Key Findings & Best Practices\n",
    "\n",
    "### The Chunking Trade-off Problem\n",
    "\n",
    "**Small Chunks (128-256 tokens):**\n",
    "- ‚úÖ Precise semantic matching\n",
    "- ‚úÖ Lower noise in retrieval\n",
    "- ‚ùå Insufficient context for generation\n",
    "- ‚ùå May miss surrounding information\n",
    "\n",
    "**Large Chunks (512-1024 tokens):**\n",
    "- ‚úÖ Rich context for generation\n",
    "- ‚úÖ Complete information\n",
    "- ‚ùå Less precise retrieval\n",
    "- ‚ùå More noise in embeddings\n",
    "\n",
    "**Hierarchical (Parent-Child):**\n",
    "- ‚úÖ Precise retrieval (via small children)\n",
    "- ‚úÖ Rich context (via large parents)\n",
    "- ‚úÖ Best of both worlds\n",
    "- ‚ö†Ô∏è Slightly more complex implementation\n",
    "\n",
    "### Research-Backed Insights\n",
    "\n",
    "1. **Optimal Child Size**: 64-128 tokens for precision (per arXiv:2505.21700)\n",
    "2. **Optimal Parent Size**: 512-1024 tokens for context\n",
    "3. **Child-Parent Ratio**: Typically 3-5 children per parent\n",
    "4. **Use Cases**: Essential for long documents, technical content, legal/medical texts\n",
    "\n",
    "### Implementation Guidelines\n",
    "\n",
    "1. **Document Length Matters**: Use hierarchical retrieval for documents >2000 tokens\n",
    "2. **Domain Considerations**: More critical for specialized domains requiring precision\n",
    "3. **Storage Trade-off**: Requires storing both child and parent nodes\n",
    "4. **Alternative Approach**: Consider \"Late Chunking\" for token-level embeddings\n",
    "\n",
    "### When to Use Hierarchical Retrieval\n",
    "\n",
    "‚úÖ **Use when:**\n",
    "- Working with long-form documents\n",
    "- Need both precision and context\n",
    "- Complex technical content\n",
    "- Multi-paragraph reasoning required\n",
    "\n",
    "‚ùå **Skip when:**\n",
    "- Short documents (< 1000 tokens)\n",
    "- Simple Q&A over structured data\n",
    "- Memory/storage constraints\n",
    "- Single-sentence retrieval sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130ad7c",
   "metadata": {},
   "source": [
    "## 11. Further Reading & Resources\n",
    "\n",
    "### Research Papers\n",
    "1. **Rethinking Chunk Size** (arXiv:2505.21700)\n",
    "   - Multi-dataset analysis of optimal chunk sizes\n",
    "   - Recommends 64-1024 token range\n",
    "\n",
    "2. **Late Chunking** (arXiv:2409.04701)\n",
    "   - Alternative: Contextual chunk embeddings using long-context models\n",
    "\n",
    "3. **Is Semantic Chunking Worth It?** (arXiv:2410.13070)\n",
    "   - Critical analysis of chunking strategies\n",
    "\n",
    "### Implementation Resources\n",
    "- **LangChain**: Parent Document Retriever\n",
    "- **LlamaIndex**: Hierarchical Node Parser\n",
    "- **Hugging Face**: Alibaba-NLP/gte-large-en-v1.5 (8192 token context)\n",
    "\n",
    "### Related Techniques\n",
    "- **Sentence Window Retrieval**: Retrieve sentence, return surrounding window\n",
    "- **Recursive Retrieval**: Multi-level hierarchies\n",
    "- **Auto-merging Retriever**: Dynamically merge adjacent chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c061e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this demo, we successfully implemented **Hierarchical Retrieval using Parent-Child chunking**:\n",
    "\n",
    "1. ‚úÖ Demonstrated the fundamental chunking trade-off\n",
    "2. ‚úÖ Implemented parent-child document structure\n",
    "3. ‚úÖ Created custom ParentDocumentRetriever\n",
    "4. ‚úÖ Compared three approaches (small, medium, hierarchical)\n",
    "5. ‚úÖ Showed hierarchical approach achieves best results\n",
    "\n",
    "**Key Takeaway**: Hierarchical retrieval solves the precision vs. context trade-off by using small chunks for accurate retrieval and large chunks for comprehensive generation context."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
