{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3a1fb7",
   "metadata": {},
   "source": [
    "# Demo #7: Corrective RAG (CRAG) - Self-Correcting Retrieval\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **Corrective RAG (CRAG)**, a self-reflective system that evaluates retrieval quality and triggers corrective actions when internal knowledge is insufficient.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Self-Correction**: Automatically evaluate whether retrieved documents are relevant\n",
    "2. **Dynamic Routing**: Route queries based on confidence scores\n",
    "3. **Fallback Mechanisms**: Use external knowledge sources (web search) when internal knowledge fails\n",
    "4. **Knowledge Refinement**: Filter and grade documents at the sentence level\n",
    "\n",
    "### The CRAG Architecture\n",
    "\n",
    "```\n",
    "Query â†’ Retrieve from Internal KB â†’ Evaluate Relevance â†’ Route:\n",
    "â”œâ”€ High Confidence (>0.7): Use internal documents directly\n",
    "â”œâ”€ Low Confidence (<0.4): Discard internal results, use web search\n",
    "â””â”€ Ambiguous (0.4-0.7): Merge internal + web search results\n",
    "```\n",
    "\n",
    "### Citation\n",
    "\n",
    "This implementation is based on:\n",
    "- **Corrective Retrieval Augmented Generation** (arXiv:2401.15884)\n",
    "- Reference #67 in the workshop curriculum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c2360",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13103a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "# LlamaIndex core imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Azure OpenAI imports\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "# Web search (DuckDuckGo)\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0b364",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Configuration\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Initialize Embedding Model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"âœ“ Azure OpenAI configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e1aa7",
   "metadata": {},
   "source": [
    "## 3. Create Limited Internal Knowledge Base\n",
    "\n",
    "For CRAG demonstration, we'll create a **small, focused knowledge base** about specific technical topics. This will allow us to test scenarios where queries fall both within and outside the knowledge base scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7385e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from a subset of technical docs\n",
    "# Using tech_docs directory which contains: BERT, GPT-4, Docker, REST API, etc.\n",
    "data_path = Path(\"../RAG_v2/data/tech_docs\")\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(f\"Warning: Data path {data_path} not found. Using sample documents.\")\n",
    "    # Create sample documents if path doesn't exist\n",
    "    data_path = Path(\"./sample_data\")\n",
    "    data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(str(data_path)).load_data()\n",
    "\n",
    "print(f\"âœ“ Loaded {len(documents)} documents from internal knowledge base\")\n",
    "print(f\"\\nDocument sources:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    filename = Path(doc.metadata.get('file_name', 'unknown')).name\n",
    "    preview = doc.text[:100].replace('\\n', ' ')\n",
    "    print(f\"{i}. {filename}: {preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9f158",
   "metadata": {},
   "source": [
    "## 4. Build Internal Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ccef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse documents into chunks\n",
    "node_parser = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "print(f\"âœ“ Created {len(nodes)} chunks from documents\")\n",
    "\n",
    "# Build vector index\n",
    "internal_index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Vector index created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400faf8",
   "metadata": {},
   "source": [
    "## 5. Implement Retrieval Evaluator\n",
    "\n",
    "The evaluator uses the LLM to assess how relevant the retrieved documents are to the query. It returns a confidence score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_relevance(\n",
    "    query: str,\n",
    "    retrieved_nodes: List[NodeWithScore],\n",
    "    llm: AzureOpenAI\n",
    ") -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of retrieved documents to the query.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        retrieved_nodes: List of retrieved document nodes\n",
    "        llm: The language model for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (confidence_score, explanation)\n",
    "        - confidence_score: 0.0-1.0 indicating relevance\n",
    "        - explanation: Text explanation of the score\n",
    "    \"\"\"\n",
    "    # Combine retrieved documents\n",
    "    retrieved_text = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}:\\n{node.node.text[:500]}...\"\n",
    "        for i, node in enumerate(retrieved_nodes[:3])  # Evaluate top 3\n",
    "    ])\n",
    "    \n",
    "    # Create evaluation prompt\n",
    "    eval_prompt = f\"\"\"You are a relevance evaluator for a retrieval system.\n",
    "\n",
    "Given a user query and retrieved documents, assess how well the documents can answer the query.\n",
    "\n",
    "USER QUERY:\n",
    "{query}\n",
    "\n",
    "RETRIEVED DOCUMENTS:\n",
    "{retrieved_text}\n",
    "\n",
    "Evaluate the relevance and provide:\n",
    "1. A confidence score between 0.0 and 1.0:\n",
    "   - 0.0-0.3: Documents are irrelevant or missing critical information\n",
    "   - 0.4-0.6: Documents have partial information but gaps exist\n",
    "   - 0.7-1.0: Documents contain sufficient relevant information\n",
    "\n",
    "2. A brief explanation (2-3 sentences)\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "    \"confidence_score\": <float 0.0-1.0>,\n",
    "    \"explanation\": \"<your explanation>\"\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get LLM evaluation\n",
    "        response = llm.complete(eval_prompt)\n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        # Handle markdown code blocks if present\n",
    "        if \"```json\" in response_text:\n",
    "            response_text = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            response_text = response_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        result = json.loads(response_text)\n",
    "        confidence = float(result.get(\"confidence_score\", 0.5))\n",
    "        explanation = result.get(\"explanation\", \"No explanation provided\")\n",
    "        \n",
    "        # Clamp confidence to valid range\n",
    "        confidence = max(0.0, min(1.0, confidence))\n",
    "        \n",
    "        return confidence, explanation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluation: {e}\")\n",
    "        # Default to medium confidence on error\n",
    "        return 0.5, f\"Evaluation error: {str(e)}\"\n",
    "\n",
    "print(\"âœ“ Retrieval evaluator function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a389cf",
   "metadata": {},
   "source": [
    "## 6. Implement Web Search Fallback\n",
    "\n",
    "When internal knowledge is insufficient, we use DuckDuckGo to search the web for current information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ff0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search_fallback(query: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform web search using DuckDuckGo as fallback for missing knowledge.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        max_results: Maximum number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of formatted search result texts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ddgs = DDGS()\n",
    "        results = ddgs.text(query, max_results=max_results)\n",
    "        \n",
    "        formatted_results = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            formatted = f\"\"\"Source {i}: {result['title']}\n",
    "URL: {result['href']}\n",
    "Content: {result['body']}\n",
    "\"\"\"\n",
    "            formatted_results.append(formatted)\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Web search error: {e}\")\n",
    "        return [f\"Web search unavailable: {str(e)}\"]\n",
    "\n",
    "# Test web search\n",
    "test_results = web_search_fallback(\"machine learning latest trends\", max_results=2)\n",
    "print(\"âœ“ Web search function tested successfully\")\n",
    "print(f\"\\nSample web search result:\")\n",
    "print(test_results[0][:200] + \"...\" if test_results else \"No results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7beb7",
   "metadata": {},
   "source": [
    "## 7. Implement Sentence-Level Knowledge Filtering\n",
    "\n",
    "This is an optional enhancement that filters retrieved documents at the sentence level to remove irrelevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences_by_relevance(\n",
    "    query: str,\n",
    "    text: str,\n",
    "    embed_model: AzureOpenAIEmbedding,\n",
    "    threshold: float = 0.5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Filter sentences in text by relevance to query.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        text: The document text to filter\n",
    "        embed_model: Embedding model for similarity\n",
    "        threshold: Minimum similarity score to keep sentence\n",
    "    \n",
    "    Returns:\n",
    "        Filtered text with only relevant sentences\n",
    "    \"\"\"\n",
    "    # Split into sentences (simple approach)\n",
    "    sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 20]\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Get embeddings\n",
    "        query_embedding = embed_model.get_query_embedding(query)\n",
    "        sentence_embeddings = [embed_model.get_text_embedding(s) for s in sentences]\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        import numpy as np\n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        \n",
    "        similarities = [\n",
    "            cosine_similarity(query_embedding, sent_emb)\n",
    "            for sent_emb in sentence_embeddings\n",
    "        ]\n",
    "        \n",
    "        # Filter sentences above threshold\n",
    "        relevant_sentences = [\n",
    "            sentences[i] for i, sim in enumerate(similarities)\n",
    "            if sim >= threshold\n",
    "        ]\n",
    "        \n",
    "        if len(relevant_sentences) == 0:\n",
    "            # If nothing passes, keep top 3 sentences\n",
    "            top_indices = np.argsort(similarities)[-3:]\n",
    "            relevant_sentences = [sentences[i] for i in top_indices]\n",
    "        \n",
    "        return '. '.join(relevant_sentences) + '.'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in sentence filtering: {e}\")\n",
    "        return text  # Return original on error\n",
    "\n",
    "print(\"âœ“ Sentence filtering function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4b68d",
   "metadata": {},
   "source": [
    "## 8. Build CRAG Query Engine\n",
    "\n",
    "This is the core of CRAG: a custom query engine that evaluates retrieval quality and routes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff98df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRAGQueryEngine:\n",
    "    \"\"\"\n",
    "    Corrective RAG Query Engine with self-evaluation and dynamic routing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        internal_index: VectorStoreIndex,\n",
    "        llm: AzureOpenAI,\n",
    "        embed_model: AzureOpenAIEmbedding,\n",
    "        high_threshold: float = 0.7,\n",
    "        low_threshold: float = 0.4,\n",
    "        top_k: int = 5,\n",
    "        use_sentence_filtering: bool = True,\n",
    "    ):\n",
    "        self.internal_index = internal_index\n",
    "        self.llm = llm\n",
    "        self.embed_model = embed_model\n",
    "        self.high_threshold = high_threshold\n",
    "        self.low_threshold = low_threshold\n",
    "        self.top_k = top_k\n",
    "        self.use_sentence_filtering = use_sentence_filtering\n",
    "        \n",
    "        # Create internal retriever\n",
    "        self.internal_retriever = internal_index.as_retriever(\n",
    "            similarity_top_k=top_k\n",
    "        )\n",
    "    \n",
    "    def query(self, query_str: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Execute CRAG query with evaluation and routing.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "            - response: Final answer\n",
    "            - confidence: Confidence score\n",
    "            - route: Which route was taken\n",
    "            - source: Where information came from\n",
    "            - explanation: Evaluation explanation\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CRAG Query: {query_str}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Step 1: Retrieve from internal knowledge base\n",
    "        print(\"\\n[Step 1] Retrieving from internal knowledge base...\")\n",
    "        retrieved_nodes = self.internal_retriever.retrieve(query_str)\n",
    "        print(f\"Retrieved {len(retrieved_nodes)} documents\")\n",
    "        \n",
    "        # Step 2: Evaluate relevance\n",
    "        print(\"\\n[Step 2] Evaluating retrieval relevance...\")\n",
    "        confidence, explanation = evaluate_retrieval_relevance(\n",
    "            query_str, retrieved_nodes, self.llm\n",
    "        )\n",
    "        print(f\"Confidence Score: {confidence:.2f}\")\n",
    "        print(f\"Explanation: {explanation}\")\n",
    "        \n",
    "        # Step 3: Route based on confidence\n",
    "        context_chunks = []\n",
    "        source = \"\"\n",
    "        route = \"\"\n",
    "        \n",
    "        if confidence >= self.high_threshold:\n",
    "            # HIGH CONFIDENCE: Use internal documents only\n",
    "            route = \"HIGH_CONFIDENCE\"\n",
    "            source = \"Internal Knowledge Base\"\n",
    "            print(f\"\\n[Step 3] Route: {route} â†’ Using internal documents\")\n",
    "            \n",
    "            for node in retrieved_nodes:\n",
    "                text = node.node.text\n",
    "                if self.use_sentence_filtering:\n",
    "                    text = filter_sentences_by_relevance(\n",
    "                        query_str, text, self.embed_model\n",
    "                    )\n",
    "                context_chunks.append(text)\n",
    "        \n",
    "        elif confidence < self.low_threshold:\n",
    "            # LOW CONFIDENCE: Use web search only\n",
    "            route = \"LOW_CONFIDENCE\"\n",
    "            source = \"Web Search\"\n",
    "            print(f\"\\n[Step 3] Route: {route} â†’ Triggering web search fallback\")\n",
    "            \n",
    "            web_results = web_search_fallback(query_str, max_results=3)\n",
    "            context_chunks = web_results\n",
    "        \n",
    "        else:\n",
    "            # AMBIGUOUS: Merge internal + web search\n",
    "            route = \"AMBIGUOUS\"\n",
    "            source = \"Internal KB + Web Search\"\n",
    "            print(f\"\\n[Step 3] Route: {route} â†’ Merging internal and web sources\")\n",
    "            \n",
    "            # Add filtered internal documents\n",
    "            for node in retrieved_nodes[:2]:  # Top 2 internal\n",
    "                text = node.node.text\n",
    "                if self.use_sentence_filtering:\n",
    "                    text = filter_sentences_by_relevance(\n",
    "                        query_str, text, self.embed_model\n",
    "                    )\n",
    "                context_chunks.append(text)\n",
    "            \n",
    "            # Add web search results\n",
    "            web_results = web_search_fallback(query_str, max_results=2)\n",
    "            context_chunks.extend(web_results)\n",
    "        \n",
    "        # Step 4: Generate final answer\n",
    "        print(f\"\\n[Step 4] Generating final answer with {len(context_chunks)} context chunks...\")\n",
    "        \n",
    "        combined_context = \"\\n\\n\".join(context_chunks)\n",
    "        \n",
    "        generation_prompt = f\"\"\"You are a helpful assistant. Answer the question based on the provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{combined_context}\n",
    "\n",
    "QUESTION:\n",
    "{query_str}\n",
    "\n",
    "Provide a comprehensive answer based on the context. If the context is insufficient, acknowledge this.\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(generation_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"response\": response.text,\n",
    "            \"confidence\": confidence,\n",
    "            \"route\": route,\n",
    "            \"source\": source,\n",
    "            \"explanation\": explanation,\n",
    "            \"context_chunks\": context_chunks,\n",
    "        }\n",
    "\n",
    "# Create CRAG query engine\n",
    "crag_engine = CRAGQueryEngine(\n",
    "    internal_index=internal_index,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    high_threshold=0.7,\n",
    "    low_threshold=0.4,\n",
    "    top_k=5,\n",
    "    use_sentence_filtering=True,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ CRAG Query Engine initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201f6f6",
   "metadata": {},
   "source": [
    "## 9. Test Scenario 1: In-Domain Query (High Confidence)\n",
    "\n",
    "Query that should be well-covered by internal knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query that should be in our tech_docs knowledge base\n",
    "query_1 = \"What is BERT and how does it differ from GPT models?\"\n",
    "\n",
    "result_1 = crag_engine.query(query_1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Route: {result_1['route']}\")\n",
    "print(f\"Confidence: {result_1['confidence']:.2f}\")\n",
    "print(f\"Source: {result_1['source']}\")\n",
    "print(f\"\\nAnswer:\\n{result_1['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961afb15",
   "metadata": {},
   "source": [
    "## 10. Test Scenario 2: Out-of-Domain Query (Low Confidence)\n",
    "\n",
    "Query that is outside our internal knowledge base scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query about current events (not in our static knowledge base)\n",
    "query_2 = \"What are the latest developments in quantum computing as of 2025?\"\n",
    "\n",
    "result_2 = crag_engine.query(query_2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Route: {result_2['route']}\")\n",
    "print(f\"Confidence: {result_2['confidence']:.2f}\")\n",
    "print(f\"Source: {result_2['source']}\")\n",
    "print(f\"\\nAnswer:\\n{result_2['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbdeae7",
   "metadata": {},
   "source": [
    "## 11. Test Scenario 3: Ambiguous Query (Medium Confidence)\n",
    "\n",
    "Query that has partial information in internal KB but may benefit from external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bef755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query that might have partial coverage\n",
    "query_3 = \"How are transformer models being used in production applications today?\"\n",
    "\n",
    "result_3 = crag_engine.query(query_3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Route: {result_3['route']}\")\n",
    "print(f\"Confidence: {result_3['confidence']:.2f}\")\n",
    "print(f\"Source: {result_3['source']}\")\n",
    "print(f\"\\nAnswer:\\n{result_3['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08936017",
   "metadata": {},
   "source": [
    "## 12. Comparative Analysis\n",
    "\n",
    "Compare CRAG with a standard (non-corrective) RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standard RAG baseline for comparison\n",
    "baseline_engine = internal_index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "print(\"Standard RAG Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test on the out-of-domain query\n",
    "baseline_response = baseline_engine.query(query_2)\n",
    "\n",
    "print(f\"\\nQuery: {query_2}\")\n",
    "print(\"\\n--- Standard RAG (No Self-Correction) ---\")\n",
    "print(baseline_response.response)\n",
    "print(\"\\n--- CRAG (With Self-Correction) ---\")\n",
    "print(result_2['response'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸ” Key Difference:\")\n",
    "print(\"Standard RAG blindly uses internal documents even when irrelevant.\")\n",
    "print(f\"CRAG detected low confidence ({result_2['confidence']:.2f}) and switched to web search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd81138",
   "metadata": {},
   "source": [
    "## 13. Visualize CRAG Decision Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d83938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "results_summary = pd.DataFrame([\n",
    "    {\n",
    "        \"Query\": query_1[:50] + \"...\",\n",
    "        \"Confidence\": f\"{result_1['confidence']:.2f}\",\n",
    "        \"Route\": result_1['route'],\n",
    "        \"Source\": result_1['source'],\n",
    "    },\n",
    "    {\n",
    "        \"Query\": query_2[:50] + \"...\",\n",
    "        \"Confidence\": f\"{result_2['confidence']:.2f}\",\n",
    "        \"Route\": result_2['route'],\n",
    "        \"Source\": result_2['source'],\n",
    "    },\n",
    "    {\n",
    "        \"Query\": query_3[:50] + \"...\",\n",
    "        \"Confidence\": f\"{result_3['confidence']:.2f}\",\n",
    "        \"Route\": result_3['route'],\n",
    "        \"Source\": result_3['source'],\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\nCRAG Routing Summary\")\n",
    "print(\"=\"*80)\n",
    "print(results_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3fb78",
   "metadata": {},
   "source": [
    "## 14. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Self-Evaluation is Critical**: Blindly using retrieved documents can lead to poor answers when internal knowledge is insufficient.\n",
    "\n",
    "2. **Dynamic Routing Improves Robustness**: By evaluating confidence and routing accordingly, CRAG adapts to different query types:\n",
    "   - High confidence â†’ Use internal KB (fast, accurate for in-domain)\n",
    "   - Low confidence â†’ Use web search (access to current information)\n",
    "   - Ambiguous â†’ Merge both (comprehensive coverage)\n",
    "\n",
    "3. **Knowledge Refinement Helps**: Sentence-level filtering removes noise and focuses on relevant information.\n",
    "\n",
    "4. **Fallback Mechanisms Essential**: External knowledge sources (web search) handle queries outside the knowledge base scope.\n",
    "\n",
    "### When to Use CRAG\n",
    "\n",
    "- **Limited/Specialized Knowledge Bases**: When you know your KB won't cover all queries\n",
    "- **Current Information Needs**: When users ask about recent events\n",
    "- **High-Stakes Applications**: When wrong answers are costly\n",
    "- **Hybrid Scenarios**: When you want the speed of internal KB with web fallback\n",
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "- **Threshold Tuning**: Adjust high/low thresholds based on your domain\n",
    "- **Evaluator Cost**: LLM evaluation adds latency and cost\n",
    "- **Web Search Limits**: Consider rate limits and API costs\n",
    "- **Caching**: Cache evaluation results for common queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c82a4",
   "metadata": {},
   "source": [
    "## 15. Data Flow Diagram\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        CRAG ARCHITECTURE                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "User Query\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Retrieve from        â”‚\n",
    "â”‚ Internal KB          â”‚\n",
    "â”‚ (Vector Search)      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ LLM Evaluates        â”‚\n",
    "â”‚ Relevance            â”‚\n",
    "â”‚ â†’ Confidence Score   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "    â”œâ”€â”€â”€ Confidence â‰¥ 0.7 (HIGH) â”€â”€â”€â”€â†’ Use Internal Docs\n",
    "    â”‚                                            â†“\n",
    "    â”œâ”€â”€â”€ Confidence < 0.4 (LOW) â”€â”€â”€â”€â”€â†’ Web Search Only\n",
    "    â”‚                                            â†“\n",
    "    â””â”€â”€â”€ 0.4 â‰¤ Confidence < 0.7 â”€â”€â”€â”€â”€â†’ Merge Internal + Web\n",
    "                                                â†“\n",
    "                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                    â”‚ Optional:            â”‚\n",
    "                                    â”‚ Sentence-Level       â”‚\n",
    "                                    â”‚ Filtering            â”‚\n",
    "                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                â†“\n",
    "                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                    â”‚ LLM Generation       â”‚\n",
    "                                    â”‚ with Refined Context â”‚\n",
    "                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                â†“\n",
    "                                        Final Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49e5d3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Corrective Retrieval Augmented Generation** - Yan et al., 2024\n",
    "   - arXiv:2401.15884\n",
    "   - Introduces the CRAG framework with retrieval evaluation and correction\n",
    "\n",
    "2. **Workshop Curriculum Reference #67**: \"Corrective Retrieval Augmented Generation\"\n",
    "\n",
    "3. **Related Approaches**:\n",
    "   - Self-RAG: Learning to retrieve, generate and critique through self-reflection\n",
    "   - Adaptive RAG: Dynamically selecting retrieval strategies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
