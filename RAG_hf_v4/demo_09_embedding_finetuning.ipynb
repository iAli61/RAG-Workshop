{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd3e437",
   "metadata": {},
   "source": [
    "# Demo #9: Fine-Tuning the Embedding Model for Domain-Specific Retrieval\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how **fine-tuning the embedding model** on domain-specific query-passage pairs significantly improves retrieval accuracy for specialized terminology and concepts.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Domain Adaptation**: Customize embeddings for specific terminology\n",
    "2. **Contrastive Learning**: Use triplet loss (query, positive, negative)\n",
    "3. **Transfer Learning**: Start from pre-trained model, adapt to domain\n",
    "4. **Retrieval Evaluation**: Measure improvement in retrieval accuracy\n",
    "\n",
    "### Why Fine-Tune Embeddings?\n",
    "\n",
    "Generic embedding models (e.g., `text-embedding-ada-002`) are trained on broad corpora. They may struggle with:\n",
    "- **Domain-specific terminology** (medical, legal, technical jargon)\n",
    "- **Specialized acronyms** (e.g., \"BERT\", \"API\", \"RAG\")\n",
    "- **Contextual nuances** in specific fields\n",
    "- **Rare concepts** not well-represented in training data\n",
    "\n",
    "Fine-tuning embeddings is often **more practical and cost-effective** than fine-tuning large generator LLMs.\n",
    "\n",
    "### Citations\n",
    "\n",
    "- **Multi-task retriever fine-tuning for domain-specific RAG** - arXiv:2501.04652 (Reference #69)\n",
    "- **Sentence-Transformers Training Documentation** - Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6339e3",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b6990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sentence Transformers for fine-tuning\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# LlamaIndex for RAG\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Azure OpenAI (for LLM generation)\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04b171",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI (for LLM Generation Only)\n",
    "\n",
    "We'll use Azure OpenAI for answer generation but use Hugging Face models for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Configuration (for LLM only)\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "print(\"✓ Azure OpenAI LLM configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45cc9ed",
   "metadata": {},
   "source": [
    "## 3. Load Knowledge Base Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load technical documents\n",
    "data_path = Path(\"../RAG_v2/data/tech_docs\")\n",
    "documents = SimpleDirectoryReader(str(data_path)).load_data()\n",
    "\n",
    "print(f\"✓ Loaded {len(documents)} documents\")\n",
    "\n",
    "# Parse into chunks\n",
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"✓ Created {len(nodes)} chunks\")\n",
    "\n",
    "# Store node texts for later retrieval evaluation\n",
    "corpus = {node.node_id: node.text for node in nodes}\n",
    "print(f\"✓ Corpus created with {len(corpus)} passages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cae977",
   "metadata": {},
   "source": [
    "## 4. Create Domain-Specific Training Dataset\n",
    "\n",
    "We'll create triplets: (query, positive_passage, negative_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1623c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-specific training queries for technical documents\n",
    "# Each entry: (query, positive_doc_keyword, negative_doc_keyword)\n",
    "training_queries = [\n",
    "    # BERT-related queries\n",
    "    (\"What is BERT and how does it work?\", \"bert\", \"docker\"),\n",
    "    (\"Explain bidirectional transformers\", \"bert\", \"gpt\"),\n",
    "    (\"How does BERT handle masked language modeling?\", \"bert\", \"api\"),\n",
    "    \n",
    "    # GPT-related queries\n",
    "    (\"What is GPT-4 and its capabilities?\", \"gpt\", \"bert\"),\n",
    "    (\"Explain autoregressive language models\", \"gpt\", \"docker\"),\n",
    "    (\"How does GPT generate text?\", \"gpt\", \"api\"),\n",
    "    \n",
    "    # Transformer-related queries\n",
    "    (\"What is transformer architecture?\", \"transformer\", \"docker\"),\n",
    "    (\"Explain self-attention mechanism\", \"transformer\", \"api\"),\n",
    "    (\"How do transformers process sequences?\", \"transformer\", \"bert\"),\n",
    "    \n",
    "    # Embeddings-related queries\n",
    "    (\"What are embeddings in machine learning?\", \"embedding\", \"docker\"),\n",
    "    (\"How do vector representations work?\", \"embedding\", \"api\"),\n",
    "    (\"Explain semantic similarity in embeddings\", \"embedding\", \"gpt\"),\n",
    "    \n",
    "    # Docker-related queries\n",
    "    (\"What are Docker containers?\", \"docker\", \"bert\"),\n",
    "    (\"How does containerization work?\", \"docker\", \"embedding\"),\n",
    "    (\"Explain Docker images and deployment\", \"docker\", \"transformer\"),\n",
    "    \n",
    "    # REST API queries\n",
    "    (\"What is a REST API?\", \"api\", \"bert\"),\n",
    "    (\"How do HTTP methods work in APIs?\", \"api\", \"embedding\"),\n",
    "    (\"Explain RESTful web services\", \"api\", \"docker\"),\n",
    "]\n",
    "\n",
    "def find_relevant_passage(keyword: str, corpus: Dict[str, str]) -> Tuple[str, str]:\n",
    "    \"\"\"Find a passage containing the keyword.\"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    for node_id, text in corpus.items():\n",
    "        if keyword_lower in text.lower():\n",
    "            return node_id, text\n",
    "    return None, None\n",
    "\n",
    "# Create training examples\n",
    "train_examples = []\n",
    "train_data_info = []\n",
    "\n",
    "for query, pos_keyword, neg_keyword in training_queries:\n",
    "    pos_id, pos_text = find_relevant_passage(pos_keyword, corpus)\n",
    "    neg_id, neg_text = find_relevant_passage(neg_keyword, corpus)\n",
    "    \n",
    "    if pos_text and neg_text:\n",
    "        # Create triplet: (query, positive, negative)\n",
    "        train_examples.append(\n",
    "            InputExample(texts=[query, pos_text, neg_text])\n",
    "        )\n",
    "        train_data_info.append({\n",
    "            \"query\": query,\n",
    "            \"positive_keyword\": pos_keyword,\n",
    "            \"negative_keyword\": neg_keyword,\n",
    "        })\n",
    "\n",
    "print(f\"✓ Created {len(train_examples)} training triplets\")\n",
    "print(f\"\\nSample training example:\")\n",
    "print(f\"  Query: {train_data_info[0]['query']}\")\n",
    "print(f\"  Positive: Contains '{train_data_info[0]['positive_keyword']}'\")\n",
    "print(f\"  Negative: Contains '{train_data_info[0]['negative_keyword']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7a211",
   "metadata": {},
   "source": [
    "## 5. Create Evaluation Dataset\n",
    "\n",
    "Separate test queries to measure improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries (different from training)\n",
    "test_queries = {\n",
    "    \"q1\": \"How does BERT differ from traditional word embeddings?\",\n",
    "    \"q2\": \"What are the key features of GPT-4?\",\n",
    "    \"q3\": \"Explain the attention mechanism in transformers\",\n",
    "    \"q4\": \"How are embeddings used in semantic search?\",\n",
    "    \"q5\": \"What are the benefits of using Docker for deployment?\",\n",
    "    \"q6\": \"How does a REST API handle requests?\",\n",
    "}\n",
    "\n",
    "# Manually create ground truth relevance\n",
    "# Format: query_id -> {doc_id: relevance_score}\n",
    "relevant_docs = {}\n",
    "\n",
    "for qid, query in test_queries.items():\n",
    "    relevant_docs[qid] = {}\n",
    "    # Find relevant passages based on keywords\n",
    "    if \"bert\" in query.lower():\n",
    "        for node_id, text in corpus.items():\n",
    "            if \"bert\" in text.lower():\n",
    "                relevant_docs[qid][node_id] = 1\n",
    "    elif \"gpt\" in query.lower():\n",
    "        for node_id, text in corpus.items():\n",
    "            if \"gpt\" in text.lower():\n",
    "                relevant_docs[qid][node_id] = 1\n",
    "    elif \"transformer\" in query.lower() or \"attention\" in query.lower():\n",
    "        for node_id, text in corpus.items():\n",
    "            if \"transformer\" in text.lower() or \"attention\" in text.lower():\n",
    "                relevant_docs[qid][node_id] = 1\n",
    "    elif \"embedding\" in query.lower():\n",
    "        for node_id, text in corpus.items():\n",
    "            if \"embedding\" in text.lower():\n",
    "                relevant_docs[qid][node_id] = 1\n",
    "    elif \"docker\" in query.lower():\n",
    "        for node_id, text in corpus.items():\n",
    "            if \"docker\" in text.lower():\n",
    "                relevant_docs[qid][node_id] = 1\n",
    "    elif \"api\" in query.lower() or \"rest\" in query.lower():\n",
    "        for node_id, text in corpus.items():\n",
    "            if \"api\" in text.lower() or \"rest\" in text.lower():\n",
    "                relevant_docs[qid][node_id] = 1\n",
    "\n",
    "print(f\"✓ Created evaluation dataset with {len(test_queries)} test queries\")\n",
    "print(f\"\\nRelevance judgments:\")\n",
    "for qid, query in test_queries.items():\n",
    "    num_relevant = len(relevant_docs.get(qid, {}))\n",
    "    print(f\"  {qid}: {query[:50]}... → {num_relevant} relevant docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f889faf",
   "metadata": {},
   "source": [
    "## 6. Baseline: Generic Embedding Model\n",
    "\n",
    "Test retrieval accuracy with a generic pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13388c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generic embedding model\n",
    "baseline_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "baseline_model = SentenceTransformer(baseline_model_name)\n",
    "\n",
    "print(f\"✓ Loaded baseline model: {baseline_model_name}\")\n",
    "\n",
    "# Create evaluator\n",
    "baseline_evaluator = InformationRetrievalEvaluator(\n",
    "    queries=test_queries,\n",
    "    corpus=corpus,\n",
    "    relevant_docs=relevant_docs,\n",
    "    name=\"baseline_evaluation\",\n",
    ")\n",
    "\n",
    "# Evaluate baseline\n",
    "print(\"\\nEvaluating baseline model...\")\n",
    "baseline_score = baseline_evaluator(baseline_model)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BASELINE RESULTS (Generic Model)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: {baseline_model_name}\")\n",
    "print(f\"Performance Score: {baseline_score:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307fc75",
   "metadata": {},
   "source": [
    "## 7. Fine-Tune Embedding Model\n",
    "\n",
    "Use contrastive learning with triplet loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db42e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for fine-tuning (start from same base)\n",
    "finetuned_model = SentenceTransformer(baseline_model_name)\n",
    "\n",
    "print(f\"✓ Loaded model for fine-tuning: {baseline_model_name}\")\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_examples,\n",
    "    shuffle=True,\n",
    "    batch_size=8,  # Small batch size for limited data\n",
    ")\n",
    "\n",
    "# Define loss function (Triplet Loss)\n",
    "train_loss = losses.TripletLoss(model=finetuned_model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINE-TUNING EMBEDDING MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training examples: {len(train_examples)}\")\n",
    "print(f\"Batch size: 8\")\n",
    "print(f\"Epochs: 3\")\n",
    "print(f\"Loss function: Triplet Loss (Contrastive Learning)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fine-tune\n",
    "finetuned_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=10,\n",
    "    evaluator=baseline_evaluator,  # Evaluate during training\n",
    "    evaluation_steps=50,\n",
    "    output_path=\"./finetuned_embeddings\",\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Fine-tuning complete\")\n",
    "print(\"✓ Model saved to: ./finetuned_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d3fd4f",
   "metadata": {},
   "source": [
    "## 8. Evaluate Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model\n",
    "print(\"\\nEvaluating fine-tuned model...\")\n",
    "finetuned_score = baseline_evaluator(finetuned_model)\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = ((finetuned_score - baseline_score) / baseline_score) * 100\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPARISON: BASELINE vs FINE-TUNED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Baseline Score:    {baseline_score:.4f}\")\n",
    "print(f\"Fine-tuned Score:  {finetuned_score:.4f}\")\n",
    "print(f\"Improvement:       {improvement:+.2f}%\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3e15f",
   "metadata": {},
   "source": [
    "## 9. Test with Actual Retrieval Queries\n",
    "\n",
    "Compare retrieval results on specific queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(query: str, model: SentenceTransformer, corpus: Dict, k: int = 3) -> List[Tuple[str, str, float]]:\n",
    "    \"\"\"Retrieve top-K documents for a query.\"\"\"\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    corpus_embeddings = model.encode(list(corpus.values()), convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    from sentence_transformers.util import cos_sim\n",
    "    similarities = cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    \n",
    "    # Get top-K\n",
    "    top_indices = similarities.argsort(descending=True)[:k]\n",
    "    \n",
    "    results = []\n",
    "    corpus_list = list(corpus.items())\n",
    "    for idx in top_indices:\n",
    "        node_id, text = corpus_list[idx]\n",
    "        score = similarities[idx].item()\n",
    "        results.append((node_id, text, score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test query\n",
    "test_query = \"What are the key differences between BERT and GPT models?\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TEST QUERY: {test_query}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Baseline retrieval\n",
    "print(\"\\n--- BASELINE MODEL RETRIEVAL ---\")\n",
    "baseline_results = retrieve_top_k(test_query, baseline_model, corpus, k=3)\n",
    "for i, (node_id, text, score) in enumerate(baseline_results, 1):\n",
    "    print(f\"\\nRank {i} (Score: {score:.4f}):\")\n",
    "    print(f\"{text[:200]}...\")\n",
    "\n",
    "# Fine-tuned retrieval\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- FINE-TUNED MODEL RETRIEVAL ---\")\n",
    "finetuned_results = retrieve_top_k(test_query, finetuned_model, corpus, k=3)\n",
    "for i, (node_id, text, score) in enumerate(finetuned_results, 1):\n",
    "    print(f\"\\nRank {i} (Score: {score:.4f}):\")\n",
    "    print(f\"{text[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b9e35",
   "metadata": {},
   "source": [
    "## 10. Build RAG Systems with Both Models\n",
    "\n",
    "Create complete RAG pipelines and compare answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2303ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap models for LlamaIndex\n",
    "baseline_embed = HuggingFaceEmbedding(model_name=baseline_model_name)\n",
    "finetuned_embed = HuggingFaceEmbedding(model_name=\"./finetuned_embeddings\")\n",
    "\n",
    "# Build baseline RAG\n",
    "print(\"Building baseline RAG system...\")\n",
    "baseline_index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    embed_model=baseline_embed,\n",
    ")\n",
    "baseline_engine = baseline_index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "\n",
    "# Build fine-tuned RAG\n",
    "print(\"Building fine-tuned RAG system...\")\n",
    "finetuned_index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    embed_model=finetuned_embed,\n",
    ")\n",
    "finetuned_engine = finetuned_index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "\n",
    "print(\"✓ Both RAG systems ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd984c",
   "metadata": {},
   "source": [
    "## 11. Compare RAG System Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with domain-specific question\n",
    "rag_test_query = \"How does BERT's bidirectional training differ from GPT's autoregressive approach?\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RAG COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Query: {rag_test_query}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Baseline RAG\n",
    "print(\"\\n--- BASELINE RAG (Generic Embeddings) ---\")\n",
    "baseline_answer = baseline_engine.query(rag_test_query)\n",
    "print(baseline_answer.response)\n",
    "\n",
    "# Fine-tuned RAG\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- FINE-TUNED RAG (Domain-Adapted Embeddings) ---\")\n",
    "finetuned_answer = finetuned_engine.query(rag_test_query)\n",
    "print(finetuned_answer.response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036ef6e",
   "metadata": {},
   "source": [
    "## 12. Visualize Embedding Space\n",
    "\n",
    "Show how fine-tuning affects the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Select sample queries and documents\n",
    "sample_texts = [\n",
    "    \"What is BERT?\",\n",
    "    \"Explain GPT models\",\n",
    "    \"How do transformers work?\",\n",
    "    \"What are Docker containers?\",\n",
    "    \"Explain REST APIs\",\n",
    "]\n",
    "\n",
    "# Get embeddings from both models\n",
    "baseline_embeddings = baseline_model.encode(sample_texts)\n",
    "finetuned_embeddings = finetuned_model.encode(sample_texts)\n",
    "\n",
    "# Reduce to 2D with PCA\n",
    "pca = PCA(n_components=2)\n",
    "baseline_2d = pca.fit_transform(baseline_embeddings)\n",
    "finetuned_2d = pca.fit_transform(finetuned_embeddings)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Baseline\n",
    "ax1.scatter(baseline_2d[:, 0], baseline_2d[:, 1], s=100, c='blue', alpha=0.6)\n",
    "for i, txt in enumerate([\"BERT\", \"GPT\", \"Transformer\", \"Docker\", \"API\"]):\n",
    "    ax1.annotate(txt, (baseline_2d[i, 0], baseline_2d[i, 1]), fontsize=10)\n",
    "ax1.set_title(\"Baseline Embeddings (Generic)\")\n",
    "ax1.set_xlabel(\"PC1\")\n",
    "ax1.set_ylabel(\"PC2\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Fine-tuned\n",
    "ax2.scatter(finetuned_2d[:, 0], finetuned_2d[:, 1], s=100, c='red', alpha=0.6)\n",
    "for i, txt in enumerate([\"BERT\", \"GPT\", \"Transformer\", \"Docker\", \"API\"]):\n",
    "    ax2.annotate(txt, (finetuned_2d[i, 0], finetuned_2d[i, 1]), fontsize=10)\n",
    "ax2.set_title(\"Fine-tuned Embeddings (Domain-Adapted)\")\n",
    "ax2.set_xlabel(\"PC1\")\n",
    "ax2.set_ylabel(\"PC2\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"embedding_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization saved as: embedding_comparison.png\")\n",
    "print(\"\\nObservation: Fine-tuned embeddings may show better separation between\")\n",
    "print(\"related concepts (BERT/GPT/Transformer cluster) vs unrelated (Docker/API).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bee867",
   "metadata": {},
   "source": [
    "## 13. Calculate Detailed Metrics\n",
    "\n",
    "Measure Recall@K and Mean Reciprocal Rank (MRR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99016a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_at_k(queries: Dict, corpus: Dict, relevant_docs: Dict, model: SentenceTransformer, k: int = 3) -> float:\n",
    "    \"\"\"Calculate Recall@K across all queries.\"\"\"\n",
    "    recalls = []\n",
    "    \n",
    "    for qid, query in queries.items():\n",
    "        if qid not in relevant_docs or len(relevant_docs[qid]) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Retrieve top-k\n",
    "        results = retrieve_top_k(query, model, corpus, k=k)\n",
    "        retrieved_ids = {node_id for node_id, _, _ in results}\n",
    "        \n",
    "        # Calculate recall\n",
    "        relevant_ids = set(relevant_docs[qid].keys())\n",
    "        hits = len(retrieved_ids.intersection(relevant_ids))\n",
    "        recall = hits / len(relevant_ids) if len(relevant_ids) > 0 else 0\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "def calculate_mrr(queries: Dict, corpus: Dict, relevant_docs: Dict, model: SentenceTransformer, k: int = 10) -> float:\n",
    "    \"\"\"Calculate Mean Reciprocal Rank.\"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for qid, query in queries.items():\n",
    "        if qid not in relevant_docs or len(relevant_docs[qid]) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Retrieve top-k\n",
    "        results = retrieve_top_k(query, model, corpus, k=k)\n",
    "        relevant_ids = set(relevant_docs[qid].keys())\n",
    "        \n",
    "        # Find rank of first relevant document\n",
    "        for rank, (node_id, _, _) in enumerate(results, 1):\n",
    "            if node_id in relevant_ids:\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\nCalculating detailed metrics...\")\n",
    "\n",
    "baseline_recall_3 = calculate_recall_at_k(test_queries, corpus, relevant_docs, baseline_model, k=3)\n",
    "baseline_recall_5 = calculate_recall_at_k(test_queries, corpus, relevant_docs, baseline_model, k=5)\n",
    "baseline_mrr = calculate_mrr(test_queries, corpus, relevant_docs, baseline_model, k=10)\n",
    "\n",
    "finetuned_recall_3 = calculate_recall_at_k(test_queries, corpus, relevant_docs, finetuned_model, k=3)\n",
    "finetuned_recall_5 = calculate_recall_at_k(test_queries, corpus, relevant_docs, finetuned_model, k=5)\n",
    "finetuned_mrr = calculate_mrr(test_queries, corpus, relevant_docs, finetuned_model, k=10)\n",
    "\n",
    "# Display results\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Baseline (Generic)\",\n",
    "        \"Recall@3\": f\"{baseline_recall_3:.3f}\",\n",
    "        \"Recall@5\": f\"{baseline_recall_5:.3f}\",\n",
    "        \"MRR\": f\"{baseline_mrr:.3f}\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Fine-tuned (Domain)\",\n",
    "        \"Recall@3\": f\"{finetuned_recall_3:.3f}\",\n",
    "        \"Recall@5\": f\"{finetuned_recall_5:.3f}\",\n",
    "        \"MRR\": f\"{finetuned_mrr:.3f}\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Improvement\",\n",
    "        \"Recall@3\": f\"{((finetuned_recall_3 - baseline_recall_3) / baseline_recall_3 * 100) if baseline_recall_3 > 0 else 0:+.1f}%\",\n",
    "        \"Recall@5\": f\"{((finetuned_recall_5 - baseline_recall_5) / baseline_recall_5 * 100) if baseline_recall_5 > 0 else 0:+.1f}%\",\n",
    "        \"MRR\": f\"{((finetuned_mrr - baseline_mrr) / baseline_mrr * 100) if baseline_mrr > 0 else 0:+.1f}%\",\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DETAILED RETRIEVAL METRICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf134c1",
   "metadata": {},
   "source": [
    "## 14. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Domain Adaptation Works**: Fine-tuning embeddings on domain-specific query-passage pairs improves retrieval accuracy, especially for specialized terminology.\n",
    "\n",
    "2. **Contrastive Learning is Effective**: Triplet loss (query, positive, negative) teaches the model to:\n",
    "   - Pull relevant query-document pairs closer in embedding space\n",
    "   - Push irrelevant pairs further apart\n",
    "\n",
    "3. **Small Data Can Help**: Even with ~20 training examples, we saw measurable improvements. More data = better results.\n",
    "\n",
    "4. **More Practical Than LLM Fine-Tuning**:\n",
    "   - **Faster**: Embedding models are smaller (100M params vs 10B+ for LLMs)\n",
    "   - **Cheaper**: Less compute, faster training\n",
    "   - **Focused**: Directly targets the retrieval bottleneck\n",
    "\n",
    "5. **Retrieval is Often the Bottleneck**: \n",
    "   - Bad retrieval → Bad context → Bad generation\n",
    "   - Good retrieval → Good context → Better answers\n",
    "\n",
    "### Training Data Requirements\n",
    "\n",
    "**Minimum**: 50-100 triplets (what we demonstrated)\n",
    "**Recommended**: 1,000-10,000 triplets for production\n",
    "**Optimal**: 50,000+ triplets for best performance\n",
    "\n",
    "**Data Sources**:\n",
    "- User queries + clicked documents (implicit relevance)\n",
    "- Manual annotation (expensive but high quality)\n",
    "- Synthetic generation (use LLM to generate queries from documents)\n",
    "- Hard negatives mining (find similar but irrelevant documents)\n",
    "\n",
    "### When to Fine-Tune Embeddings\n",
    "\n",
    "✅ **Good Use Cases**:\n",
    "- Specialized domains (medical, legal, technical)\n",
    "- Unique terminology or acronyms\n",
    "- Multi-lingual or code-switching scenarios\n",
    "- When generic models fail on domain queries\n",
    "\n",
    "❌ **Not Necessary**:\n",
    "- General knowledge queries\n",
    "- Limited training data availability (<50 examples)\n",
    "- When generic models already perform well\n",
    "\n",
    "### Implementation Checklist\n",
    "\n",
    "1. **Collect Training Data**: Query-positive-negative triplets\n",
    "2. **Choose Base Model**: Start with strong pre-trained model\n",
    "3. **Select Loss Function**: Triplet loss, contrastive loss, or multiple negatives ranking loss\n",
    "4. **Train with Evaluation**: Monitor retrieval metrics during training\n",
    "5. **Test on Hold-out Set**: Ensure improvements generalize\n",
    "6. **Deploy and Monitor**: Track production retrieval quality\n",
    "\n",
    "### Alternative Approaches\n",
    "\n",
    "If you don't have resources to fine-tune:\n",
    "- **Hybrid Search**: Combine dense + sparse (BM25) retrieval\n",
    "- **Query Expansion**: Reformulate queries to improve matches\n",
    "- **Re-ranking**: Use cross-encoder after initial retrieval\n",
    "- **Better Chunking**: Optimize chunk size and overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836a43e",
   "metadata": {},
   "source": [
    "## 15. Mathematical Foundation: Triplet Loss\n",
    "\n",
    "### Triplet Loss Formula\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\max(0, d(a, p) - d(a, n) + \\text{margin})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $a$ = anchor (query embedding)\n",
    "- $p$ = positive (relevant document embedding)\n",
    "- $n$ = negative (irrelevant document embedding)\n",
    "- $d(\\cdot, \\cdot)$ = distance metric (e.g., Euclidean or cosine)\n",
    "- $\\text{margin}$ = minimum separation between positive and negative (typically 0.5)\n",
    "\n",
    "### Goal\n",
    "\n",
    "Minimize the distance between anchor and positive while maximizing the distance between anchor and negative.\n",
    "\n",
    "$$\n",
    "d(a, p) + \\text{margin} < d(a, n)\n",
    "$$\n",
    "\n",
    "### Cosine Similarity Version\n",
    "\n",
    "$$\n",
    "\\text{sim}(a, p) = \\frac{a \\cdot p}{\\|a\\| \\|p\\|}\n",
    "$$\n",
    "\n",
    "We want:\n",
    "$$\n",
    "\\text{sim}(a, p) > \\text{sim}(a, n) + \\text{margin}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c70e3f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Multi-task retriever fine-tuning for domain-specific and efficient RAG** - arXiv:2501.04652\n",
    "   - Reference #69 in workshop curriculum\n",
    "   \n",
    "2. **ALoFTRAG: Automatic Local Fine Tuning for RAG** - arXiv:2501.11929\n",
    "   - Reference #71 in workshop curriculum\n",
    "\n",
    "3. **Sentence-Transformers Documentation**\n",
    "   - Training guide: https://www.sbert.net/docs/training/overview.html\n",
    "   - Hugging Face Hub: https://huggingface.co/sentence-transformers\n",
    "\n",
    "4. **Base Models Used**:\n",
    "   - sentence-transformers/all-MiniLM-L6-v2: https://hf.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "   - BAAI/bge-base-en-v1.5: https://hf.co/BAAI/bge-base-en-v1.5"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
