{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9038b091",
   "metadata": {},
   "source": [
    "# Demo #1: HyDE (Hypothetical Document Embeddings) - Query Enhancement\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo demonstrates **HyDE (Hypothetical Document Embeddings)**, an advanced query enhancement technique that dramatically improves semantic matching between queries and documents.\n",
    "\n",
    "### Core Concept\n",
    "\n",
    "Traditional RAG systems embed the user's query directly and search for similar document embeddings. However, queries are typically short questions while documents are informative passages. This asymmetry can lead to suboptimal retrieval.\n",
    "\n",
    "**HyDE solves this by:**\n",
    "1. Using an LLM to generate a hypothetical answer document from the query\n",
    "2. Embedding this hypothetical document (which has similar characteristics to real documents)\n",
    "3. Searching for documents similar to this hypothetical answer\n",
    "4. This creates an \"answer-to-answer\" similarity paradigm instead of \"question-to-answer\"\n",
    "\n",
    "### Key Benefits\n",
    "- Better semantic alignment between query and document embeddings\n",
    "- Improved retrieval precision, especially for complex queries\n",
    "- Works without requiring any fine-tuning or additional data\n",
    "\n",
    "### Citation\n",
    "- **Paper**: \"Precise Zero-Shot Dense Retrieval without Relevance Labels\" (HyDE original paper)\n",
    "  - Link: https://hf.co/papers/2212.10496\n",
    "- **Paper**: \"ARAGOG: Advanced RAG Output Grading\" - Evaluates HyDE effectiveness\n",
    "  - Link: https://hf.co/papers/2404.01037"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afba0f7",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell if packages are not already installed\n",
    "# !pip install llama-index llama-index-llms-azure-openai llama-index-embeddings-azure-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# llama-index core imports\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "# Azure OpenAI imports\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08790356",
   "metadata": {},
   "source": [
    "## 2. Configure Azure OpenAI\n",
    "\n",
    "Make sure you have a `.env` file in the project root with:\n",
    "```\n",
    "AZURE_OPENAI_API_KEY=your_key\n",
    "AZURE_OPENAI_ENDPOINT=your_endpoint\n",
    "AZURE_OPENAI_API_VERSION=2024-02-15-preview\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=your_gpt4_deployment\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT=your_embedding_deployment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b31f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Azure OpenAI LLM\n",
    "azure_llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Configure Azure OpenAI Embeddings\n",
    "azure_embed = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = azure_llm\n",
    "Settings.embed_model = azure_embed\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"âœ“ Azure OpenAI configured successfully\")\n",
    "print(f\"  LLM: {os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')}\")\n",
    "print(f\"  Embeddings: {os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378444c",
   "metadata": {},
   "source": [
    "## 3. Load and Process Documents\n",
    "\n",
    "We'll use machine learning concept documents as our knowledge base. This focused domain allows us to clearly demonstrate HyDE's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path - using existing ML concepts data\n",
    "data_path = \"../RAG_v2/data/ml_concepts\"\n",
    "\n",
    "# Check if path exists\n",
    "if not Path(data_path).exists():\n",
    "    print(f\"âš  Warning: Data path not found: {data_path}\")\n",
    "    print(\"Please adjust the path to your data directory\")\n",
    "else:\n",
    "    print(f\"âœ“ Data path found: {data_path}\")\n",
    "\n",
    "# Load documents\n",
    "reader = SimpleDirectoryReader(data_path)\n",
    "documents = reader.load_data()\n",
    "\n",
    "print(f\"\\nLoaded {len(documents)} documents:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"  {i}. {Path(doc.metadata['file_path']).name} ({len(doc.text)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955fd72",
   "metadata": {},
   "source": [
    "## 4. Chunk Documents\n",
    "\n",
    "We use sentence-based chunking with moderate chunk size (512 tokens) and minimal overlap (50 tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence splitter\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Parse documents into nodes (chunks)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"âœ“ Created {len(nodes)} chunks from {len(documents)} documents\")\n",
    "print(f\"\\nExample chunk:\")\n",
    "print(f\"  Length: {len(nodes[0].text)} characters\")\n",
    "print(f\"  Content preview: {nodes[0].text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2980f",
   "metadata": {},
   "source": [
    "## 5. Build Vector Index\n",
    "\n",
    "Create an in-memory vector store index using Azure OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55289d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store index\n",
    "index = VectorStoreIndex(nodes, embed_model=azure_embed)\n",
    "\n",
    "print(\"âœ“ Vector index created successfully\")\n",
    "print(f\"  Total nodes indexed: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f74d0b",
   "metadata": {},
   "source": [
    "## 6. Create Baseline Query Engine\n",
    "\n",
    "First, we'll create a standard query engine to establish a baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea584ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline query engine\n",
    "baseline_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    llm=azure_llm,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Baseline query engine created\")\n",
    "print(\"  Configuration:\")\n",
    "print(\"    - Retrieves top 3 most similar chunks\")\n",
    "print(\"    - Uses direct query embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c7005",
   "metadata": {},
   "source": [
    "## 7. Test Baseline Retrieval\n",
    "\n",
    "Let's test the baseline system with a technical query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b87caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test query\n",
    "test_query = \"What are the key advantages and disadvantages of using ensemble methods in machine learning?\"\n",
    "\n",
    "print(\"Test Query:\")\n",
    "print(f\"  {test_query}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Query baseline engine\n",
    "baseline_response = baseline_query_engine.query(test_query)\n",
    "\n",
    "print(\"\\nğŸ” BASELINE RETRIEVAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nGenerated Answer:\\n{baseline_response.response}\")\n",
    "\n",
    "# Display retrieved source nodes\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Retrieved Chunks (Top 3):\")\n",
    "print(\"-\"*80)\n",
    "for i, node in enumerate(baseline_response.source_nodes, 1):\n",
    "    print(f\"\\nChunk {i} (Score: {node.score:.4f}):\")\n",
    "    print(f\"Source: {Path(node.metadata.get('file_path', 'Unknown')).name}\")\n",
    "    print(f\"Content: {node.text[:300]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af5a35",
   "metadata": {},
   "source": [
    "## 8. Implement HyDE Query Enhancement\n",
    "\n",
    "Now we'll create the HyDE-enhanced query engine. The key innovation is generating a hypothetical answer document before retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HyDE query transformation\n",
    "hyde_transform = HyDEQueryTransform(\n",
    "    llm=azure_llm,\n",
    "    include_original=False,  # Only use the hypothetical document, not the original query\n",
    ")\n",
    "\n",
    "# Wrap baseline query engine with HyDE transformation\n",
    "hyde_query_engine = TransformQueryEngine(\n",
    "    baseline_query_engine,\n",
    "    query_transform=hyde_transform,\n",
    ")\n",
    "\n",
    "print(\"âœ“ HyDE query engine created\")\n",
    "print(\"  Enhancement: Query â†’ LLM generates hypothetical answer â†’ Embed hypothetical answer â†’ Retrieve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73ada8",
   "metadata": {},
   "source": [
    "## 9. Visualize HyDE Process\n",
    "\n",
    "Before running the full query, let's manually see what hypothetical document HyDE generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually run the HyDE transformation to see the hypothetical document\n",
    "from llama_index.core.schema import QueryBundle\n",
    "\n",
    "# Create query bundle\n",
    "query_bundle = QueryBundle(query_str=test_query)\n",
    "\n",
    "# Generate hypothetical document\n",
    "transformed_query = hyde_transform.run(query_bundle)\n",
    "\n",
    "print(\"ğŸ”¬ HYDE TRANSFORMATION PROCESS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal Query:\\n{test_query}\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"\\nHypothetical Document Generated by LLM:\")\n",
    "print(\"-\"*80)\n",
    "print(transformed_query.query_str)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nğŸ’¡ Key Insight:\")\n",
    "print(\"   The hypothetical document is more similar in structure to the actual documents\")\n",
    "print(\"   in our knowledge base, leading to better semantic matching.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ae58c",
   "metadata": {},
   "source": [
    "## 10. Test HyDE-Enhanced Retrieval\n",
    "\n",
    "Now let's run the same query through the HyDE-enhanced engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query HyDE-enhanced engine\n",
    "hyde_response = hyde_query_engine.query(test_query)\n",
    "\n",
    "print(\"\\nğŸ¯ HYDE-ENHANCED RETRIEVAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nGenerated Answer:\\n{hyde_response.response}\")\n",
    "\n",
    "# Display retrieved source nodes\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Retrieved Chunks (Top 3):\")\n",
    "print(\"-\"*80)\n",
    "for i, node in enumerate(hyde_response.source_nodes, 1):\n",
    "    print(f\"\\nChunk {i} (Score: {node.score:.4f}):\")\n",
    "    print(f\"Source: {Path(node.metadata.get('file_path', 'Unknown')).name}\")\n",
    "    print(f\"Content: {node.text[:300]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a9cb2",
   "metadata": {},
   "source": [
    "## 11. Side-by-Side Comparison\n",
    "\n",
    "Let's create a clear comparison between baseline and HyDE approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for i in range(min(3, len(baseline_response.source_nodes))):\n",
    "    baseline_node = baseline_response.source_nodes[i]\n",
    "    hyde_node = hyde_response.source_nodes[i]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        \"Rank\": i + 1,\n",
    "        \"Baseline Source\": Path(baseline_node.metadata.get('file_path', 'Unknown')).name,\n",
    "        \"Baseline Score\": f\"{baseline_node.score:.4f}\",\n",
    "        \"HyDE Source\": Path(hyde_node.metadata.get('file_path', 'Unknown')).name,\n",
    "        \"HyDE Score\": f\"{hyde_node.score:.4f}\",\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nğŸ“Š RETRIEVAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRetrieved Documents by Rank:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nAnswer Quality Comparison:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"\\nBaseline Answer Length: {len(baseline_response.response)} characters\")\n",
    "print(f\"HyDE Answer Length: {len(hyde_response.response)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935acdae",
   "metadata": {},
   "source": [
    "## 12. Additional Test Cases\n",
    "\n",
    "Let's test with a few more queries to demonstrate HyDE's robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional test queries\n",
    "test_queries = [\n",
    "    \"How does the kernel trick work in support vector machines?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning with examples\",\n",
    "    \"What is backpropagation and why is it important for neural networks?\",\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ”¬ ADDITIONAL TEST CASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nTest Case {i}: {query}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Get responses\n",
    "    baseline = baseline_query_engine.query(query)\n",
    "    hyde = hyde_query_engine.query(query)\n",
    "    \n",
    "    # Compare top retrieved document\n",
    "    baseline_top_source = Path(baseline.source_nodes[0].metadata.get('file_path', 'Unknown')).name\n",
    "    hyde_top_source = Path(hyde.source_nodes[0].metadata.get('file_path', 'Unknown')).name\n",
    "    \n",
    "    print(f\"  Baseline Top Source: {baseline_top_source} (Score: {baseline.source_nodes[0].score:.4f})\")\n",
    "    print(f\"  HyDE Top Source: {hyde_top_source} (Score: {hyde.source_nodes[0].score:.4f})\")\n",
    "    \n",
    "    if baseline_top_source != hyde_top_source:\n",
    "        print(\"  âš¡ HyDE retrieved a different document!\")\n",
    "    else:\n",
    "        print(\"  âœ“ Both methods retrieved the same top document\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19411aa5",
   "metadata": {},
   "source": [
    "## 13. Data Flow Visualization\n",
    "\n",
    "Let's create a visual representation of the HyDE workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“ˆ HYDE DATA FLOW DIAGRAM\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "BASELINE RAG PIPELINE:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  User Query     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Embed Query    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Vector Search  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Retrieved Chunksâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ LLM Generation  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Final Answer   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "HYDE-ENHANCED PIPELINE:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  User Query     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LLM Generates              â”‚\n",
    "â”‚  Hypothetical Answer Doc    â”‚ â† KEY INNOVATION\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Embed Hypothetical Doc     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Vector Search  â”‚ â† Answer-to-Answer Similarity\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Retrieved Chunksâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ LLM Generation  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Final Answer   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3332fb7a",
   "metadata": {},
   "source": [
    "## 14. Key Takeaways and Insights\n",
    "\n",
    "### When to Use HyDE:\n",
    "- **Complex technical queries** where users ask questions in one style but documents are written in another\n",
    "- **Domain-specific applications** where terminology matters\n",
    "- **When query-document asymmetry** is causing poor retrieval\n",
    "\n",
    "### Trade-offs:\n",
    "- **Pros:**\n",
    "  - Significantly improves semantic matching\n",
    "  - No training or fine-tuning required\n",
    "  - Works with any embedding model\n",
    "  \n",
    "- **Cons:**\n",
    "  - Adds one extra LLM call (increased latency and cost)\n",
    "  - Generated hypothetical document might be incorrect (but still semantically useful)\n",
    "  - Less effective for very simple, keyword-based queries\n",
    "\n",
    "### Performance Considerations:\n",
    "- The hypothetical document generation adds ~1-2 seconds of latency\n",
    "- This is usually worth it for the improved retrieval quality\n",
    "- Can be cached for frequently repeated queries\n",
    "\n",
    "### Real-World Applications:\n",
    "- Technical documentation search\n",
    "- Medical literature retrieval\n",
    "- Legal document search\n",
    "- Scientific paper discovery\n",
    "- Code search and software documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663043cc",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "In this demo, we've successfully implemented and compared:\n",
    "\n",
    "1. **Baseline RAG**: Direct query embedding â†’ vector search\n",
    "2. **HyDE-Enhanced RAG**: Query â†’ LLM generates hypothetical answer â†’ embed hypothetical answer â†’ vector search\n",
    "\n",
    "The key insight is that by generating a hypothetical answer document, we transform the retrieval problem from \"question-to-answer\" matching into \"answer-to-answer\" matching, which is more semantically aligned and often produces better results.\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different types of queries\n",
    "- Try HyDE with different domains and document types\n",
    "- Combine HyDE with other advanced techniques (coming in subsequent demos)\n",
    "- Measure performance metrics systematically\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Original HyDE Paper: https://hf.co/papers/2212.10496\n",
    "- ARAGOG Evaluation: https://hf.co/papers/2404.01037"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
